{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laporan Analisis Bab\n",
        "\n",
        "**Judul Buku:** Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
        "**Penulis:** Aurélien Géron\n",
        "**Edisi:** Kedua, Diperbarui untuk TensorFlow 2\n",
        "\n",
        "---\n",
        "\n",
        "## Bab 15: Processing Sequences Using RNNs and CNNs\n",
        "\n",
        "**I. Pendahuluan**\n",
        "Bab 15 berfokus pada pemrosesan data sekuensial, sebuah kemampuan krusial dalam Machine Learning yang memungkinkan model memprediksi masa depan atau menganalisis data deret waktu seperti harga saham, suhu harian, atau metrik keuangan. Penulis memperkenalkan Recurrent Neural Networks (RNNs) sebagai kelas jaringan yang dapat menangani sekuens dengan panjang arbitrer, menjadikannya sangat berguna untuk aplikasi pemrosesan bahasa alami (NLP) seperti terjemahan otomatis atau *speech-to-text*. Bab ini akan menguraikan konsep fundamental RNN, cara melatihnya menggunakan *backpropagation through time*, dan mengatasi tantangan utama RNN: gradien yang tidak stabil dan memori jangka pendek yang terbatas. Selain RNN, bab ini juga mengeksplorasi penggunaan Convolutional Neural Networks (CNNs) 1D untuk pemrosesan sekuens yang sangat panjang, seperti pada arsitektur WaveNet.\n",
        "\n",
        "**II. Recurrent Neurons and Layers**\n",
        "Berbeda dengan *feedforward neural networks* di mana aktivasi mengalir satu arah, RNN memiliki koneksi yang menunjuk ke belakang, memungkinkan neuron untuk menerima input saat ini serta outputnya sendiri dari langkah waktu sebelumnya. Ini memberikan neuron bentuk \"memori\".\n",
        "\n",
        "**Konsep-konsep Penting:**\n",
        "* **Neuron Rekuren:** Pada setiap langkah waktu `t`, neuron rekuren menerima input `x(t)` dan outputnya sendiri dari langkah waktu sebelumnya `y(t-1)`.\n",
        "* **Lapisan Rekuren:** Setiap neuron rekuren memiliki dua set bobot: satu untuk input `x(t)` (matriks bobot `Wx`) dan yang lain untuk output dari langkah waktu sebelumnya `y(t-1)` (matriks bobot `Wy`).\n",
        "* **Unrolling the Network Through Time:** Representasi jaringan rekuren yang digambarkan sepanjang sumbu waktu, dengan satu neuron rekuren direpresentasikan per langkah waktu.\n",
        "* **Memory Cells (Sel Memori):** Bagian dari jaringan saraf yang mempertahankan beberapa keadaan antar langkah waktu. Neuron rekuren tunggal atau lapisan neuron rekuren adalah sel dasar yang hanya dapat mempelajari pola pendek (biasanya sekitar 10 langkah).\n",
        "\n",
        "**Input dan Output Sekuensial:**\n",
        "* **Sequence-to-Sequence Network:** Menerima sekuens input dan menghasilkan sekuens output (misalnya, memprediksi deret waktu).\n",
        "* **Sequence-to-Vector Network:** Menerima sekuens input, mengabaikan semua output kecuali yang terakhir (misalnya, analisis sentimen ulasan film).\n",
        "* **Vector-to-Sequence Network:** Menerima input vektor yang sama berulang kali dan menghasilkan sekuens output (misalnya, pembuatan *caption* gambar).\n",
        "* **Encoder-Decoder:** Menggabungkan *sequence-to-vector network* (encoder) diikuti oleh *vector-to-sequence network* (decoder) (misalnya, terjemahan bahasa).\n",
        "\n",
        "**III. Pelatihan RNNs**\n",
        "Pelatihan RNN melibatkan \"membuka gulungannya\" (unrolling) sepanjang waktu dan kemudian menggunakan *backpropagation* reguler, sebuah strategi yang disebut *backpropagation through time* (BPTT).\n",
        "\n",
        "**Proses BPTT:**\n",
        "1.  **Forward Pass:** Terjadi melalui jaringan yang belum digulung, menghitung output sekuens.\n",
        "2.  **Evaluasi Cost Function:** Sekuens output dievaluasi menggunakan fungsi biaya `C(Y(0), Y(1), …Y(T))`.\n",
        "3.  **Backward Pass:** Gradien fungsi biaya disebarkan mundur melalui jaringan yang belum digulung.\n",
        "4.  **Parameter Update:** Parameter model diperbarui menggunakan gradien yang dihitung selama BPTT. Karena parameter yang sama (`W` dan `b`) digunakan di setiap langkah waktu, *backpropagation* akan menjumlahkannya di seluruh langkah waktu.\n",
        "\n",
        "**Prakiraan Deret Waktu (Forecasting a Time Series):**\n",
        "* **Data Deret Waktu:** Sekuens satu atau lebih nilai per langkah waktu (univariat atau multivariat).\n",
        "* **Tugas Umum:** Memprediksi nilai masa depan (*forecasting*) atau mengisi nilai yang hilang (*imputation*).\n",
        "* **Format Input:** Input umumnya direpresentasikan sebagai array 3D `[batch size, time steps, dimensionality]`.\n",
        "* **Metrik Baseline:** Penting untuk memiliki metrik baseline (misalnya, *naive forecasting* atau model linier sederhana) untuk membandingkan kinerja model RNN.\n",
        "* **Simple RNN dalam Keras:** Dapat dibangun dengan `keras.layers.SimpleRNN`. Secara *default*, lapisan ini hanya mengembalikan output terakhir.\n",
        "* **Deep RNNs:** Menumpuk beberapa lapisan sel rekuren untuk membentuk RNN yang dalam. Untuk lapisan rekuren (kecuali yang terakhir jika hanya output terakhir yang penting), `return_sequences=True` harus diatur untuk memastikan output 3D diteruskan ke lapisan rekuren berikutnya.\n",
        "* **Memprakirakan Beberapa Langkah Waktu ke Depan:**\n",
        "    * **Satu per satu:** Memprediksi nilai berikutnya, menambahkannya ke input, dan menggunakannya untuk memprediksi nilai berikutnya lagi. Akurasi cenderung menurun seiring bertambahnya langkah waktu.\n",
        "    * **Sekaligus:** Melatih RNN untuk memprediksi semua `N` nilai berikutnya secara bersamaan. Output lapisan terakhir memiliki `N` unit. Ini seringkali lebih baik dan lebih cepat daripada pendekatan satu per satu.\n",
        "    * **Sequence-to-Sequence (Output per Langkah Waktu):** Melatih model untuk memprakirakan `N` nilai berikutnya pada setiap langkah waktu. Ini meningkatkan gradien kesalahan dan menstabilkan serta mempercepat pelatihan. `keras.layers.TimeDistributed` digunakan untuk menerapkan lapisan *dense* pada setiap langkah waktu.\n",
        "\n",
        "**IV. Menangani Sekuens Panjang**\n",
        "Melatih RNN pada sekuens yang sangat panjang menghadirkan dua masalah utama: gradien yang tidak stabil dan memori jangka pendek yang sangat terbatas.\n",
        "\n",
        "**Melawan Masalah Gradien Tidak Stabil:**\n",
        "* **Teknik Umum:** Inisialisasi parameter yang baik, *optimizer* yang lebih cepat, *dropout*, dll.\n",
        "* **Fungsi Aktivasi:** Fungsi aktivasi yang tidak jenuh (misalnya ReLU) mungkin tidak banyak membantu dan bahkan dapat membuat RNN lebih tidak stabil. Fungsi aktivasi yang jenuh seperti *tanh* lebih disukai karena mencegah output meledak.\n",
        "* **Gradient Clipping:** Mencegah gradien melebihi ambang batas tertentu selama *backpropagation*.\n",
        "* **Batch Normalization (BN):** Kurang efisien pada RNN dibandingkan jaringan *feedforward*.\n",
        "* **Layer Normalization:** Menormalisasi di sepanjang dimensi fitur (bukan dimensi *batch*). Ini dapat menghitung statistik yang diperlukan dengan cepat di setiap langkah waktu, secara independen untuk setiap instans. Berperilaku sama selama pelatihan dan pengujian. Lapisan `LNSimpleRNNCell` dapat dibuat secara kustom untuk mengimplementasikan *Layer Normalization*.\n",
        "\n",
        "**Mengatasi Masalah Memori Jangka Pendek:**\n",
        "* **LSTM Cells (Long Short-Term Memory):** Diperkenalkan pada tahun 1997. LSTM adalah sel memori jangka panjang yang sangat sukses dalam menangkap pola jangka panjang dalam deret waktu, teks panjang, rekaman audio, dll. Keras menyediakan lapisan `LSTM`.\n",
        "    * **Arsitektur LSTM:** Keadaan sel terbagi menjadi dua vektor: keadaan jangka pendek (`h(t)`) dan keadaan jangka panjang (`c(t)`). Jaringan dapat belajar apa yang harus disimpan, dibuang, dan dibaca dari keadaan jangka panjang. Menggunakan *forget gate*, *input gate*, dan *output gate*.\n",
        "    * **Peephole Connections:** Varian LSTM dengan koneksi tambahan yang memungkinkan pengontrol gerbang untuk \"mengintip\" keadaan jangka panjang.\n",
        "* **GRU Cells (Gated Recurrent Unit):** Varian LSTM yang disederhanakan, seringkali berkinerja sama baiknya. Keras menyediakan lapisan `GRU`.\n",
        "    * **Penyederhanaan GRU:** Menggabungkan dua vektor keadaan menjadi satu (`h(t)`). Pengontrol gerbang tunggal (`z(t)`) mengontrol *forget gate* dan *input gate*. Tidak ada *output gate*.\n",
        "\n",
        "**Menggunakan Lapisan Konvolusional 1D untuk Memproses Sekuens:**\n",
        "* Lapisan konvolusional 1D dapat digunakan untuk mempersingkat sekuens input (sub-sampel) sebelum diteruskan ke lapisan rekuren, membantu lapisan GRU/LSTM mendeteksi pola yang lebih panjang.\n",
        "* **WaveNet:** Arsitektur yang diperkenalkan pada tahun 2016, menumpuk lapisan konvolusional 1D dengan tingkat dilasi yang berlipat ganda di setiap lapisan. Lapisan bawah mempelajari pola jangka pendek, sementara lapisan atas mempelajari pola jangka panjang. Sangat efisien untuk memproses sekuens yang sangat besar (puluhan ribu langkah waktu), seperti sampel audio.\n",
        "\n",
        "**V. Kesimpulan**\n",
        "Bab 15 mengupas tuntas tentang bagaimana RNN dan CNN digunakan untuk memproses data sekuensial. Ini mencakup dasar-dasar neuron dan lapisan rekuren, berbagai jenis arsitektur RNN untuk *forecasting* deret waktu, serta teknik-teknik canggih untuk mengatasi masalah gradien tidak stabil dan memori jangka pendek yang terbatas (seperti sel LSTM dan GRU). Selain itu, bab ini memperkenalkan bagaimana lapisan konvolusional 1D dapat diterapkan secara efektif untuk memproses sekuens yang sangat panjang, seperti pada arsitektur WaveNet yang revolusioner. Pembahasan ini memberikan landasan yang kuat untuk memahami pemrosesan bahasa alami di bab berikutnya."
      ],
      "metadata": {
        "id": "XwkguV29F3Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REPRODUCE CODE"
      ],
      "metadata": {
        "id": "K_tgLA9KF-xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting a Time Series"
      ],
      "metadata": {
        "id": "7SbgNmMtGBlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "metadata": {
        "id": "J88ouEK6GA1E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "metadata": {
        "id": "osESIUwEYFAj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Metrics"
      ],
      "metadata": {
        "id": "pzfygRiZYHel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwqvLqZWYhqi",
        "outputId": "f3e0b932-5444-4330-d2d1-d70b125db1ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf # Import tensorflow\n",
        "from tensorflow import keras # Import keras from tensorflow\n",
        "\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "y_pred = X_valid[:, -1]\n",
        "\n",
        "# Use tf.keras.metrics.MeanSquaredError by creating an instance\n",
        "# and calling its result() method after updating it.\n",
        "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
        "mse_metric.update_state(y_valid, y_pred)\n",
        "print(mse_metric.result().numpy()) # Print the result, converting the tensor to a numpy value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhxTQK_6YGVL",
        "outputId": "9c43df34-d7ec-4448-8ff4-bb4b98e24da0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.020900993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[50, 1]),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KufKjROSY9FB",
        "outputId": "5052d376-646e-43ff-d524-36a972771bd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Simple RNN"
      ],
      "metadata": {
        "id": "MPp4XCi3Y-gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR6lwGDgY_ud",
        "outputId": "b8cbce27-4d1f-4e83-bb08-a91014e86679"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep RNNs"
      ],
      "metadata": {
        "id": "qrt32e8SZBp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1)\n",
        "])"
      ],
      "metadata": {
        "id": "cHtuYAAuZDE3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "VWAS0ziQZEVU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting Several Time Steps Ahead"
      ],
      "metadata": {
        "id": "nnSmjbHTZG2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfshU_RwZHgs",
        "outputId": "22c10e67-8fa5-4d3c-8bc1-b127b5f07621"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "metadata": {
        "id": "I999YGaiZJLQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "gL_p8Zc3ZKlD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B8qWK3XZMcQ",
        "outputId": "af2f65c7-fa59-4215-824c-d287a7d2c300"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "metadata": {
        "id": "mhqJt5jtZN1x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "id": "kMcSIztlZPM8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "# Change lr=0.01 to learning_rate=0.01\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])"
      ],
      "metadata": {
        "id": "TKFHX2m_ZQoe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fighting the Unstable Gradients Problem"
      ],
      "metadata": {
        "id": "giq4pOP5ZU9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
        "                                                          activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]"
      ],
      "metadata": {
        "id": "8AC-a4X1ZZ9s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9jKgT_QZbvr",
        "outputId": "6e80d501-1cc5-4358-b5b4-47b880db4728"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'ln_simple_rnn_cell_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tackling the Short-Term Memory Problem"
      ],
      "metadata": {
        "id": "HKZm7OQ8Zdgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "id": "Ztc1SlIxZeMi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]),\n",
        "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "id": "1uI0rnAbZfX3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the time series generation function\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# Generate data\n",
        "n_steps = 50\n",
        "# Generate a bit longer series for target, ensuring we have enough steps\n",
        "# for targets after the Conv1D output sequence.\n",
        "# Conv1D output length = 24. Last Conv1D output index = 23.\n",
        "# Corresponds to input index 2*23 = 46.\n",
        "# Window is 46 to 49. Target starts at 50 and needs 10 steps (50 to 59).\n",
        "# So, original series needs length n_steps + kernel_size + (target_steps - 1)\n",
        "# 50 + 4 + (10 - 1) = 50 + 4 + 9 = 63\n",
        "# Let's generate slightly more to be safe.\n",
        "series_length = n_steps + 10 + 10 # n_steps + target_length + some buffer\n",
        "series = generate_time_series(10000, series_length)\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "\n",
        "# Define the Conv1D layer parameters\n",
        "kernel_size = 4\n",
        "strides = 2\n",
        "conv1d_output_steps = (n_steps - kernel_size) // strides + 1 # Calculate Conv1D output length\n",
        "\n",
        "# Prepare target data to match the Conv1D output sequence length\n",
        "# Y will have shape (batch_size, conv1d_output_steps, 10)\n",
        "Y = np.empty((10000, conv1d_output_steps, 10))\n",
        "for i in range(conv1d_output_steps):\n",
        "    # The i-th output of Conv1D (with stride 2, kernel 4) corresponds to\n",
        "    # input slice from 2*i to 2*i + 3.\n",
        "    # We want to predict the next 10 steps *after* this slice.\n",
        "    # These steps are from index (2*i) + 4 to (2*i) + 4 + 10 - 1.\n",
        "    start_target_index = (2 * i) + kernel_size\n",
        "    end_target_index = start_target_index + 10\n",
        "    Y[:, i, :] = series[:, start_target_index:end_target_index, 0]\n",
        "\n",
        "\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]\n",
        "\n",
        "# Define the custom metric\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    # Ensure Y_true and Y_pred have the same rank for slicing\n",
        "    # Y_true[:, -1] takes the last time step of the true targets (shape: (batch_size, 10))\n",
        "    # Y_pred[:, -1] takes the last time step of the predicted outputs (shape: (batch_size, 10))\n",
        "    # Use tf.math.square and tf.reduce_mean for calculating MSE on the last time step\n",
        "    return tf.reduce_mean(tf.math.square(Y_true[:, -1] - Y_pred[:, -1]))\n",
        "\n",
        "# Define the model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=kernel_size, strides=strides, padding=\"valid\",\n",
        "                         input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRwYdT8NZggk",
        "outputId": "4d25feff-a048-41f1-920d-8e6441299f0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - last_time_step_mse: 0.0915 - loss: 0.0971 - val_last_time_step_mse: 0.0347 - val_loss: 0.0435\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - last_time_step_mse: 0.0331 - loss: 0.0397 - val_last_time_step_mse: 0.0227 - val_loss: 0.0313\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - last_time_step_mse: 0.0219 - loss: 0.0302 - val_last_time_step_mse: 0.0170 - val_loss: 0.0273\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0162 - loss: 0.0265 - val_last_time_step_mse: 0.0135 - val_loss: 0.0246\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0138 - loss: 0.0246 - val_last_time_step_mse: 0.0120 - val_loss: 0.0232\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0122 - loss: 0.0234 - val_last_time_step_mse: 0.0113 - val_loss: 0.0223\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0114 - loss: 0.0223 - val_last_time_step_mse: 0.0106 - val_loss: 0.0217\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0108 - loss: 0.0215 - val_last_time_step_mse: 0.0101 - val_loss: 0.0211\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - last_time_step_mse: 0.0102 - loss: 0.0212 - val_last_time_step_mse: 0.0102 - val_loss: 0.0208\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0099 - loss: 0.0209 - val_last_time_step_mse: 0.0090 - val_loss: 0.0202\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0094 - loss: 0.0202 - val_last_time_step_mse: 0.0091 - val_loss: 0.0199\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0091 - loss: 0.0200 - val_last_time_step_mse: 0.0090 - val_loss: 0.0198\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - last_time_step_mse: 0.0092 - loss: 0.0200 - val_last_time_step_mse: 0.0085 - val_loss: 0.0195\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - last_time_step_mse: 0.0089 - loss: 0.0195 - val_last_time_step_mse: 0.0090 - val_loss: 0.0194\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0091 - loss: 0.0194 - val_last_time_step_mse: 0.0088 - val_loss: 0.0192\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0087 - loss: 0.0193 - val_last_time_step_mse: 0.0085 - val_loss: 0.0189\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - last_time_step_mse: 0.0087 - loss: 0.0193 - val_last_time_step_mse: 0.0081 - val_loss: 0.0188\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - last_time_step_mse: 0.0084 - loss: 0.0189 - val_last_time_step_mse: 0.0085 - val_loss: 0.0188\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - last_time_step_mse: 0.0082 - loss: 0.0188 - val_last_time_step_mse: 0.0082 - val_loss: 0.0184\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - last_time_step_mse: 0.0082 - loss: 0.0184 - val_last_time_step_mse: 0.0080 - val_loss: 0.0184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wavenet"
      ],
      "metadata": {
        "id": "QkC5W8dfaLm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the time series generation function\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# Generate data\n",
        "n_steps = 50\n",
        "# Need enough steps in the original series to provide targets for each of the\n",
        "# n_steps (50) input steps, predicting 10 steps ahead.\n",
        "# The target for input step i is series[i+1:i+11].\n",
        "# For the last input step (49), the target is series[50:60].\n",
        "# So, the original series needs at least 50 + 10 = 60 steps.\n",
        "series_length = n_steps + 10\n",
        "series = generate_time_series(10000, series_length)\n",
        "\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "\n",
        "# Prepare target data for the WaveNet model with causal padding.\n",
        "# The model outputs a sequence of length n_steps (50).\n",
        "# For each input step i (from 0 to 49), the model predicts the next 10 steps.\n",
        "# So, the target Y will have shape (batch_size, n_steps, 10).\n",
        "# Y[batch, i, :] should be the series values from series[batch, i+1] to series[batch, i+10].\n",
        "Y = np.empty((10000, n_steps, 10), dtype=np.float32)\n",
        "for step in range(n_steps):\n",
        "    # The target for input at time `step` is the sequence from `step + 1` to `step + 10`.\n",
        "    Y[:, step, :] = series[:, step + 1 : step + 1 + 10, 0]\n",
        "\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]\n",
        "\n",
        "\n",
        "# Define the custom metric\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    # Ensure Y_true and Y_pred have the same rank for slicing\n",
        "    # Y_true[:, -1] takes the last time step of the true targets (shape: (batch_size, 10))\n",
        "    # Y_pred[:, -1] takes the last time step of the predicted outputs (shape: (batch_size, 10))\n",
        "    # Use tf.math.square and tf.reduce_mean for calculating MSE on the last time step\n",
        "    return tf.reduce_mean(tf.math.square(Y_true[:, -1] - Y_pred[:, -1]))\n",
        "\n",
        "# Define the model (this part remains the same)\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj697IqaaM0_",
        "outputId": "654baa01-8fb5-47b9-e020-9fe0796fb782"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - last_time_step_mse: 0.0899 - loss: 0.0976 - val_last_time_step_mse: 0.0226 - val_loss: 0.0357\n",
            "Epoch 2/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - last_time_step_mse: 0.0202 - loss: 0.0326 - val_last_time_step_mse: 0.0171 - val_loss: 0.0294\n",
            "Epoch 3/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0163 - loss: 0.0284 - val_last_time_step_mse: 0.0148 - val_loss: 0.0272\n",
            "Epoch 4/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0143 - loss: 0.0263 - val_last_time_step_mse: 0.0131 - val_loss: 0.0259\n",
            "Epoch 5/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0134 - loss: 0.0251 - val_last_time_step_mse: 0.0125 - val_loss: 0.0247\n",
            "Epoch 6/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0124 - loss: 0.0241 - val_last_time_step_mse: 0.0121 - val_loss: 0.0242\n",
            "Epoch 7/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - last_time_step_mse: 0.0116 - loss: 0.0233 - val_last_time_step_mse: 0.0113 - val_loss: 0.0236\n",
            "Epoch 8/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - last_time_step_mse: 0.0112 - loss: 0.0229 - val_last_time_step_mse: 0.0109 - val_loss: 0.0230\n",
            "Epoch 9/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0110 - loss: 0.0224 - val_last_time_step_mse: 0.0108 - val_loss: 0.0231\n",
            "Epoch 10/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0106 - loss: 0.0223 - val_last_time_step_mse: 0.0103 - val_loss: 0.0225\n",
            "Epoch 11/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0105 - loss: 0.0217 - val_last_time_step_mse: 0.0101 - val_loss: 0.0219\n",
            "Epoch 12/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0100 - loss: 0.0215 - val_last_time_step_mse: 0.0096 - val_loss: 0.0215\n",
            "Epoch 13/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0095 - loss: 0.0208 - val_last_time_step_mse: 0.0095 - val_loss: 0.0215\n",
            "Epoch 14/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0093 - loss: 0.0207 - val_last_time_step_mse: 0.0096 - val_loss: 0.0212\n",
            "Epoch 15/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0091 - loss: 0.0205 - val_last_time_step_mse: 0.0085 - val_loss: 0.0205\n",
            "Epoch 16/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0089 - loss: 0.0202 - val_last_time_step_mse: 0.0085 - val_loss: 0.0201\n",
            "Epoch 17/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - last_time_step_mse: 0.0084 - loss: 0.0195 - val_last_time_step_mse: 0.0082 - val_loss: 0.0198\n",
            "Epoch 18/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0082 - loss: 0.0193 - val_last_time_step_mse: 0.0082 - val_loss: 0.0197\n",
            "Epoch 19/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - last_time_step_mse: 0.0077 - loss: 0.0190 - val_last_time_step_mse: 0.0092 - val_loss: 0.0203\n",
            "Epoch 20/20\n",
            "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - last_time_step_mse: 0.0077 - loss: 0.0188 - val_last_time_step_mse: 0.0075 - val_loss: 0.0193\n"
          ]
        }
      ]
    }
  ]
}