{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4F1CwyXjDc6B9yQKHBfv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch13_Loading_and_Preprocessing_Data_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laporan Analisis Bab 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "## Pendahuluan\n",
        "Bab 13 berfokus pada penanganan dataset besar yang tidak muat di memori (RAM) dalam konteks Deep Learning. Ini adalah tantangan umum karena model Deep Learning modern seringkali dilatih dengan volume data yang sangat besar. Bab ini memperkenalkan TensorFlow Data API sebagai solusi efisien untuk memuat dan memproses data, serta membahas format TFRecord untuk penyimpanan data yang optimal. Selain itu, bab ini mengeksplorasi teknik-teknik pra-pemrosesan fitur, termasuk penanganan fitur kategorikal dan teks, serta memperkenalkan lapisan pra-pemrosesan Keras dan alat TF Transform.\n",
        "\n",
        "## Ringkasan Isi Bab\n",
        "\n",
        "Bab 13 menguraikan beberapa konsep kunci terkait Pemuatan dan Pra-pemrosesan Data dengan TensorFlow:\n",
        "\n",
        "1.  **The Data API (API Data)**:\n",
        "    * **Konsep Dasar**: Seluruh Data API berpusat pada konsep `dataset`, yang merepresentasikan urutan item data. `tf.data.Dataset.from_tensor_slices()` dapat membuat dataset dari *tensor* di RAM.\n",
        "    * **Chaining Transformations (Menggabungkan Transformasi)**: Setelah membuat dataset, berbagai transformasi dapat diterapkan secara berantai menggunakan metode dataset seperti `repeat()`, `batch()`, `map()`, `apply()`, dan `filter()`. Setiap metode mengembalikan dataset baru, memungkinkan alur pemrosesan yang fleksibel. Fungsi yang diberikan ke `map()` harus dapat dikonversi menjadi TF Function.\n",
        "    * **Shuffling the Data (Mengocok Data)**: `shuffle()` digunakan untuk mengocok instance, yang penting agar instance pelatihan independen dan terdistribusi secara identik (IID). Perlu ditentukan ukuran *buffer* yang cukup besar. Untuk dataset yang sangat besar, dapat mengocok data sumber atau menginterleave baris dari banyak file acak.\n",
        "    * **Interleaving lines from multiple files (Menginterleave baris dari banyak file)**: `tf.data.Dataset.list_files()` membuat dataset dari *filepath*. Metode `interleave()` dapat membaca dari beberapa file secara bersamaan dan menggabungkan baris-barisnya, seringkali dengan `tf.data.TextLineDataset` dan `skip()` untuk melewati *header*. Ini mendukung paralelisasi dengan `num_parallel_calls`.\n",
        "    * **Preprocessing the Data (Pra-pemrosesan Data)**: Dapat dilakukan dengan metode `map()` menggunakan fungsi pra-pemrosesan kustom. Fungsi ini harus menggunakan operasi TensorFlow dan dapat menskalakan fitur.\n",
        "    * **Putting Everything Together (Menggabungkan Semuanya)**: Contoh fungsi `csv_reader_dataset` menunjukkan bagaimana menggabungkan `list_files()`, `interleave()`, `map()`, `shuffle()`, `repeat()`, `batch()`, dan `prefetch()` untuk membuat *input pipeline* yang efisien.\n",
        "    * **Prefetching (Pra-pengambilan)**: `prefetch(1)` di akhir *pipeline* membuat dataset selalu satu *batch* di depan. Ini memungkinkan CPU dan GPU bekerja secara paralel, secara dramatis meningkatkan kinerja pelatihan. Untuk dataset yang muat di memori, `cache()` dapat mempercepat pelatihan dengan menyimpan konten dataset di RAM setelah pra-pemrosesan.\n",
        "    * **Using the Dataset with tf.keras (Menggunakan Dataset dengan tf.keras)**: Dataset yang dibuat dengan Data API dapat langsung diumpankan ke metode `fit()`, `evaluate()`, dan `predict()` dari model Keras. Ini juga memungkinkan iterasi langsung pada dataset dalam *custom training loops*.\n",
        "\n",
        "2.  **The TFRecord Format (Format TFRecord)**:\n",
        "    * **Definisi**: Format biner pilihan TensorFlow untuk menyimpan data dalam jumlah besar dan membacanya secara efisien. Berisi urutan catatan biner dengan ukuran bervariasi.\n",
        "    * **Membuat File TFRecord**: Menggunakan `tf.io.TFRecordWriter`.\n",
        "    * **Membaca File TFRecord**: Menggunakan `tf.data.TFRecordDataset`. Dapat membaca banyak file secara paralel dengan `num_parallel_reads`.\n",
        "    * **Compressed TFRecord Files (File TFRecord Terkompresi)**: Mendukung kompresi (misalnya, GZIP) untuk mengurangi ukuran file, berguna saat memuat melalui jaringan.\n",
        "    * **Introduction to Protocol Buffers (Pengantar Protocol Buffers - Protobufs)**:\n",
        "        * Format biner portabel, dapat diperluas, dan efisien yang dikembangkan oleh Google. Didefinisikan menggunakan bahasa sederhana (`.proto`).\n",
        "        * Data dalam file TFRecord umumnya berisi *serialized protocol buffers*, terutama *Example* protobuf.\n",
        "        * Contoh dasar penggunaan kelas akses protobuf Python (misalnya, `Person` protobuf, `SerializeToString()`, `ParseFromString()`).\n",
        "    * **TensorFlow Protobufs**:\n",
        "        * `Example` protobuf: Merepresentasikan satu instance dalam dataset, berisi daftar fitur bernama (byte strings, float, atau integer). Fitur-fitur ini disimpan dalam objek `Features`.\n",
        "        * `tf.train.Example`: Kelas untuk membuat *Example* protobuf.\n",
        "        * **Loading and Parsing Examples (Memuat dan Mem-parsing Contoh)**: Menggunakan `tf.io.parse_single_example()` (untuk satu catatan) atau `tf.io.parse_example()` (untuk *batch*) dengan deskripsi fitur (`tf.io.FixedLenFeature` atau `tf.io.VarLenFeature`).\n",
        "        * **Menyimpan Data Kompleks**: `BytesList` dapat menyimpan data biner apa pun (misalnya, gambar JPEG atau *tensor* yang diserialisasi).\n",
        "        * `SequenceExample` protobuf: Dirancang untuk kasus penggunaan daftar-daftar (misalnya, dokumen teks dengan kalimat dan kata-kata). Diparse dengan `tf.io.parse_single_sequence_example()` atau `tf.io.parse_sequence_example()`.\n",
        "\n",
        "3.  **Preprocessing the Input Features (Pra-pemrosesan Fitur Input)**:\n",
        "    * **Tujuan**: Mengubah semua fitur menjadi numerik, menormalisasikannya, dan mengodekan fitur kategorikal atau teks.\n",
        "    * **Implementasi Pra-pemrosesan**: Dapat dilakukan dengan metode `map()` dataset atau langsung dalam model sebagai lapisan pra-pemrosesan.\n",
        "    * **Standardization Layer (Lapisan Standardisasi)**: Implementasi kustom dapat dibuat sebagai subclass `keras.layers.Layer` dengan metode `adapt()` (untuk menghitung rata-rata/std dari sampel data) dan `call()` (untuk menerapkan transformasi). Keras kemungkinan akan menyediakan `keras.layers.Normalization`.\n",
        "    * **Encoding Categorical Features Using One-Hot Vectors (Mengodekan Fitur Kategorikal Menggunakan One-Hot Vectors)**:\n",
        "        * Menggunakan tabel pencarian (`tf.lookup.StaticVocabularyTable`) untuk memetakan kategori ke ID integer, termasuk *out-of-vocabulary* (OOV) *buckets* untuk kategori yang tidak dikenal.\n",
        "        * Kemudian `tf.one_hot()` mengodekan ID menjadi *one-hot vector*.\n",
        "        * Cocok untuk kategori < 10.\n",
        "    * **Encoding Categorical Features Using Embeddings (Mengodekan Fitur Kategorikal Menggunakan Embeddings)**:\n",
        "        * *Embedding* adalah *dense vector* yang dapat dilatih dan merepresentasikan kategori.\n",
        "        * `keras.layers.Embedding` layer menangani matriks *embedding* (dapat dilatih).\n",
        "        * *Embeddings* akan membaik selama pelatihan, dan kategori serupa akan memiliki *embedding* yang lebih dekat (*representation learning*).\n",
        "        * *Word Embeddings*: Contoh paling umum, seringkali dapat digunakan kembali dari model *pretrained* (misalnya, dari TensorFlow Hub).\n",
        "        * Cocok untuk kategori > 50.\n",
        "    * **Keras Preprocessing Layers (Lapisan Pra-pemrosesan Keras)**:\n",
        "        * Tim TensorFlow sedang mengembangkan serangkaian lapisan pra-pemrosesan standar Keras (misalnya, `keras.layers.Normalization`, `keras.layers.TextVectorization`, `keras.layers.Discretization`).\n",
        "        * Metode `adapt()` akan mengekstrak kosakata/statistik dari sampel data.\n",
        "        * `PreprocessingStage` dapat menggabungkan beberapa lapisan pra-pemrosesan.\n",
        "        * `TextVectorization` dapat menghasilkan *word-count vectors* (*bag of words*) atau TF-IDF.\n",
        "        * Lapisan pra-pemrosesan ini harus digunakan di awal model dan akan dibekukan selama pelatihan.\n",
        "\n",
        "4.  **TF Transform (tf.Transform)**:\n",
        "    * Bagian dari TensorFlow Extended (TFX), platform *end-to-end* untuk produksi model TensorFlow.\n",
        "    * **Tujuan**: Menulis fungsi pra-pemrosesan hanya sekali (di Python menggunakan fungsi TF Transform). Fungsi ini dapat dijalankan dalam mode *batch* pada seluruh dataset pelatihan sebelum pelatihan (untuk kecepatan).\n",
        "    * **Deployment**: TF Transform akan menghasilkan TF Function yang setara yang dapat dimasukkan ke dalam model yang sudah dilatih dan diterapkan di produksi, sehingga menghindari *training/serving skew* (perbedaan pra-pemrosesan antara pelatihan dan produksi). Ini mengurangi pemeliharaan kode pra-pemrosesan di berbagai platform.\n",
        "\n",
        "5.  **The TensorFlow Datasets (TFDS) Project (Proyek TFDS)**:\n",
        "    * **Tujuan**: Memudahkan pengunduhan dan pemuatan dataset umum (dari kecil seperti MNIST hingga besar seperti ImageNet).\n",
        "    * **Penggunaan**: `tfds.load(name=\"mnist\", as_supervised=True)` akan mengunduh data dan mengembalikannya sebagai `tf.data.Dataset` dalam format terawasi (fitur, label).\n",
        "    * Dataset TFDS dapat langsung digunakan dengan model `tf.keras` setelah transformasi yang diperlukan (misalnya, `shuffle()`, `batch()`, `prefetch()`).\n",
        "\n",
        "## Analisis dan Relevansi untuk Mahasiswa\n",
        "\n",
        "Bab 13 adalah fondasi kritis bagi mahasiswa yang akan berhadapan dengan dataset besar dan kompleks dalam proyek Deep Learning mereka.\n",
        "\n",
        "* **Pentingnya *Input Pipeline***: Mahasiswa akan memahami bahwa *input pipeline* yang efisien sama pentingnya dengan arsitektur model dalam mencapai kinerja pelatihan yang optimal, terutama untuk dataset yang tidak muat di memori.\n",
        "* **Penguasaan Data API**: Detail tentang `tf.data.Dataset` dan berbagai transformasinya adalah pengetahuan inti. Ini membekali mahasiswa dengan kemampuan untuk membangun alur data yang fleksibel, paralel, dan berkinerja tinggi.\n",
        "* **Penyimpanan Data Optimal**: Pengenalan TFRecord dan Protocol Buffers mengajarkan mahasiswa format penyimpanan data yang direkomendasikan TensorFlow untuk efisiensi skala besar, penting untuk proyek yang melampaui CSV sederhana.\n",
        "* **Pra-pemrosesan yang Robust**: Bab ini menyediakan berbagai strategi untuk menangani fitur numerik, kategorikal, dan teks. Mahasiswa akan belajar cara mengodekan fitur, menskalakan data, dan memilih antara pra-pemrosesan *offline* atau *on-the-fly*.\n",
        "* **Mencegah *Training/Serving Skew***: Konsep *training/serving skew* dan solusi yang ditawarkan oleh lapisan pra-pemrosesan Keras dan TF Transform adalah wawasan tingkat lanjut yang krusial untuk penerapan model di dunia nyata. Ini mendorong praktik rekayasa ML yang baik.\n",
        "* **Sumber Dataset yang Mudah**: TFDS menyederhanakan proses mendapatkan dan menggunakan dataset publik, memungkinkan mahasiswa untuk lebih fokus pada eksperimen model daripada akuisisi data.\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab 13 memberikan panduan komprehensif untuk mengelola, memuat, dan mempra-memproses data secara efisien di TensorFlow. Mahasiswa yang menguasai bab ini akan sangat siap untuk menghadapi tantangan data skala besar dan membangun *input pipeline* yang solid untuk model Deep Learning mereka. Kemampuan ini merupakan keterampilan praktis yang tidak terpisahkan dari pengembangan sistem Machine Learning yang efektif dan dapat diterapkan di produksi."
      ],
      "metadata": {
        "id": "DH3OD3plHZhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "# Mengatur seed untuk reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- Persiapan Data California Housing (untuk contoh utama) ---\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"--- Mempersiapkan Data California Housing ---\")\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "# Skala data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Membuat direktori 'datasets/housing'\n",
        "housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "os.makedirs(housing_dir, exist_ok=True)\n",
        "\n",
        "def save_to_csv(filepath, X, y, header=None, mode=\"w\"):\n",
        "    with open(filepath, mode) as f:\n",
        "        if header is not None:\n",
        "            f.write(header + \"\\n\")\n",
        "        for row, target in zip(X, y):\n",
        "            f.write(\",\".join([str(x) for x in row]) + \",\" + str(target) + \"\\n\")\n",
        "\n",
        "# Simpan data MENTAH ke CSV, penskalaan akan dilakukan di fungsi preprocess\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "train_data_path = os.path.join(housing_dir, \"my_train.csv\")\n",
        "valid_data_path = os.path.join(housing_dir, \"my_valid.csv\")\n",
        "test_data_path = os.path.join(housing_dir, \"my_test.csv\")\n",
        "save_to_csv(train_data_path, X_train_full, y_train_full, header=header)\n",
        "save_to_csv(valid_data_path, X_valid, y_valid, header=header)\n",
        "save_to_csv(test_data_path, X_test, y_test, header=header)\n",
        "\n",
        "# Memecah file training\n",
        "train_filepaths = []\n",
        "for i in range(10):\n",
        "    filepath = os.path.join(housing_dir, f\"my_train_{i:02d}.csv\")\n",
        "    save_to_csv(filepath, X_train_full[i::10], y_train_full[i::10], header=header)\n",
        "    train_filepaths.append(filepath)\n",
        "\n",
        "valid_filepaths = [valid_data_path]\n",
        "test_filepaths = [test_data_path]\n",
        "print(\"Data California Housing telah disimpan ke file CSV.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 1: The Data API ---\n",
        "print(\"\\n--- Bagian 1: The Data API ---\")\n",
        "X_mean = X_train_full.mean(axis=0, keepdims=True)\n",
        "X_std = X_train_full.std(axis=0, keepdims=True)\n",
        "n_inputs = X_train_full.shape[1]\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.0] * n_inputs + [tf.constant(0.0, dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = fields[-1]\n",
        "    scaled_x = (x - X_mean) / X_std\n",
        "    return tf.squeeze(scaled_x), y\n",
        "\n",
        "def csv_reader_dataset(filepaths, repeat=1, shuffle_buffer_size=10000, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths, seed=42)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=5, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if shuffle_buffer_size > 0:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size, seed=42)\n",
        "    dataset = dataset.repeat(repeat)\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filepaths, batch_size=batch_size, repeat=None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths, batch_size=batch_size)\n",
        "test_set = csv_reader_dataset(test_filepaths, batch_size=batch_size)\n",
        "\n",
        "model_keras_dataset = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_keras_dataset.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "steps_per_epoch = len(X_train_full) // batch_size\n",
        "history_dataset = model_keras_dataset.fit(train_set, epochs=2,\n",
        "                                          steps_per_epoch=steps_per_epoch,\n",
        "                                          validation_data=valid_set)\n",
        "\n",
        "\n",
        "# --- BAGIAN 2: The TFRecord Format ---\n",
        "print(\"\\n--- Bagian 2: The TFRecord Format ---\")\n",
        "# (Bagian ini tidak diubah dan diasumsikan sudah benar)\n",
        "from tensorflow.train import BytesList, FloatList, Int64List, Feature, Features, Example\n",
        "\n",
        "# ... (Kode TFRecord dari sebelumnya) ...\n",
        "\n",
        "\n",
        "# --- BAGIAN 3: Preprocessing the Input Features ---\n",
        "print(\"\\n--- Bagian 3: Preprocessing the Input Features ---\")\n",
        "\n",
        "# 3.1 Custom Standardization Layer\n",
        "class Standardization(keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "    def call(self, inputs):\n",
        "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
        "\n",
        "# 3.2 Encoding Categorical Features Using One-Hot Vectors\n",
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=indices)\n",
        "num_oov_buckets = 2\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "\n",
        "# ##################################################################\n",
        "# #############   PERBAIKAN UTAMA ADA DI BAGIAN INI   ##############\n",
        "# ##################################################################\n",
        "# 3.3 Encoding Categorical Features Using Embeddings\n",
        "print(\"\\n3.3 Encoding Categorical Features Using Embeddings:\")\n",
        "embedding_dim = 2\n",
        "\n",
        "# Definisikan input untuk model fungsional\n",
        "regular_inputs = keras.layers.Input(shape=[8])\n",
        "categories_input = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "# PERBAIKAN: Bungkus operasi TensorFlow 'table.lookup' di dalam Lambda layer\n",
        "# agar dapat bekerja dengan Keras Input layer (symbolic tensor).\n",
        "cat_indices_model = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories_input)\n",
        "\n",
        "cat_embed = keras.layers.Embedding(\n",
        "    input_dim=len(vocab) + num_oov_buckets,\n",
        "    output_dim=embedding_dim\n",
        ")(cat_indices_model)\n",
        "\n",
        "# Gabungkan kembali input numerik dan embedding\n",
        "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
        "\n",
        "outputs_model_embedding = keras.layers.Dense(1)(encoded_inputs)\n",
        "model_with_embedding = keras.models.Model(inputs=[regular_inputs, categories_input],\n",
        "                                          outputs=[outputs_model_embedding])\n",
        "\n",
        "print(\"Model dengan Embedding Layer untuk fitur kategorikal berhasil dibuat.\")\n",
        "model_with_embedding.summary()\n",
        "# ##################################################################\n",
        "# ##################################################################\n",
        "\n",
        "\n",
        "# --- BAGIAN 4: TF Transform (Konseptual) ---\n",
        "print(\"\\n--- Bagian 4: TF Transform (Konseptual) ---\")\n",
        "try:\n",
        "    import tensorflow_transform as tft\n",
        "    print(\"Paket tensorflow-transform ditemukan.\")\n",
        "except ImportError:\n",
        "    print(\"Paket tensorflow-transform tidak ditemukan. Melewati bagian 4.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 5: The TensorFlow Datasets (TFDS) Project ---\n",
        "print(\"\\n--- Bagian 5: The TensorFlow Datasets (TFDS) Project ---\")\n",
        "try:\n",
        "    import tensorflow_datasets as tfds\n",
        "\n",
        "    print(\"\\nMemuat dataset MNIST dari TFDS...\")\n",
        "    dataset_mnist_tfds, info_mnist_tfds = tfds.load(name=\"mnist\", as_supervised=True, with_info=True)\n",
        "    mnist_train_tfds, mnist_test_tfds = dataset_mnist_tfds[\"train\"], dataset_mnist_tfds[\"test\"]\n",
        "    print(\"Dataset MNIST dari TFDS dimuat.\")\n",
        "\n",
        "    model_tfds = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "        keras.layers.Lambda(lambda x: tf.cast(x, tf.float32) / 255.0),\n",
        "        keras.layers.Dense(100, activation=\"relu\"),\n",
        "        keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    model_tfds.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print(\"\\nMelatih model Keras dengan TFDS dataset (singkat):\")\n",
        "    model_tfds.fit(mnist_train_tfds.shuffle(10000, seed=42).batch(32).prefetch(tf.data.AUTOTUNE),\n",
        "                   epochs=2,\n",
        "                   validation_data=mnist_test_tfds.batch(32).prefetch(tf.data.AUTOTUNE))\n",
        "    print(\"Pelatihan dengan TFDS dataset selesai.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\nPaket tensorflow-datasets tidak ditemukan. Melewati bagian 5.\")\n",
        "\n",
        "print(\"\\n--- Skrip selesai dieksekusi dengan perbaikan. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "gXItiG4lI_jA",
        "outputId": "8750fe6d-08c3-407c-89f5-29fae1d40d4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mempersiapkan Data California Housing ---\n",
            "Data California Housing telah disimpan ke file CSV.\n",
            "\n",
            "--- Bagian 1: The Data API ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2.6802 - val_loss: 0.8408\n",
            "Epoch 2/2\n",
            "\u001b[1m 48/483\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7770"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.7877 - val_loss: 0.6900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bagian 2: The TFRecord Format ---\n",
            "\n",
            "--- Bagian 3: Preprocessing the Input Features ---\n",
            "\n",
            "3.3 Encoding Categorical Features Using Embeddings:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None,), dtype=string, sparse=False, name=keras_tensor_36>',)\n  • kwargs={'mask': 'None'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3320136848>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# PERBAIKAN: Bungkus operasi TensorFlow 'table.lookup' di dalam Lambda layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# agar dapat bekerja dengan Keras Input layer (symbolic tensor).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mcat_indices_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m cat_embed = keras.layers.Embedding(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0;34m\"We could not automatically infer the shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;34m\"the Lambda's output. Please specify the `output_shape` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None,), dtype=string, sparse=False, name=keras_tensor_36>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ]
    }
  ]
}