{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbm8vNiL0PwNveb4tNHB7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch5_Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PENJELASAN AWAL"
      ],
      "metadata": {
        "id": "V0e6p4qVaxBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 5 dari buku \"Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\" memperkenalkan Support Vector Machines (SVMs), sebuah model Machine Learning yang kuat dan serbaguna. Bab ini menjelaskan konsep inti SVMs, bagaimana menggunakannya untuk klasifikasi dan regresi, serta bagaimana mekanisme internalnya bekerja, termasuk \"kernel trick\" yang terkenal."
      ],
      "metadata": {
        "id": "xlvHEthaa0YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TUJUAN"
      ],
      "metadata": {
        "id": "EcUE_uk_a1hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tujuan utama bab ini adalah untuk memperkenalkan pembaca pada model SVM, menjelaskan prinsip dasarnya, serta menunjukkan implementasinya untuk tugas klasifikasi (linier dan non-linier) dan regresi. Pembaca diharapkan memahami bagaimana SVM beroperasi, metrik-metrik yang relevan, dan bagaimana mengelola hyperparameternya."
      ],
      "metadata": {
        "id": "6CdfiE6Xa21F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RINGKASAN UTAMA"
      ],
      "metadata": {
        "id": "4H3wO0r3a9R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter ini membahas topik-topik kunci terkait SVM:\n"
      ],
      "metadata": {
        "id": "3KFb3ygCa_oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Linear SVM Classification (Klasifikasi SVM Linier)"
      ],
      "metadata": {
        "id": "mBhossUWbDWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Konsep Dasar: Ide fundamental di balik SVM adalah menemukan \"jalan\" (atau margin) terlebar yang mungkin antara kelas-kelas dalam data. Ini disebut large margin classification.\n",
        "  - Hard Margin Classification: Ini adalah pendekatan yang ketat di mana semua instansi harus berada di luar \"jalan\" dan di sisi yang benar.\n",
        "    - Kelemahan: Hanya berfungsi jika data dapat dipisahkan secara linier dan sangat sensitif terhadap outlier.\n",
        "  - Soft Margin Classification: Pendekatan yang lebih fleksibel, mencari keseimbangan antara menjaga \"jalan\" selebar mungkin dan membatasi pelanggaran margin (instansi yang berada di dalam \"jalan\" atau di sisi yang salah).\n",
        "    - Hyperparameter C: Mengontrol trade-off ini. Nilai C yang rendah menghasilkan margin yang lebih lebar tetapi lebih banyak pelanggaran margin, sementara nilai C yang tinggi menghasilkan lebih sedikit pelanggaran margin tetapi margin yang lebih sempit. Jika model overfitting, C dapat dikurangi untuk regularisasi.\n",
        "- Sensitivitas terhadap Skala Fitur: SVM sangat sensitif terhadap skala fitur. Penting untuk menskalakan data (misalnya, menggunakan StandardScaler dari Scikit-Learn) sebelum melatih SVM.\n",
        "- Implementasi dengan Scikit-Learn:\n",
        "  - LinearSVC: Kelas yang direkomendasikan untuk klasifikasi SVM linier. Ia mengimplementasikan algoritma yang dioptimalkan untuk SVM linier.\n",
        "  - SVC(kernel=\"linear\"): Alternatif untuk LinearSVC, tetapi umumnya lebih lambat untuk dataset linier besar.\n",
        "  - SGDClassifier(loss=\"hinge\"): Dapat digunakan untuk melatih pengklasifikasi SVM linier menggunakan Stochastic Gradient Descent, berguna untuk tugas klasifikasi online atau dataset besar yang tidak muat di memori.\n",
        "  - SVM tidak mengeluarkan probabilitas kelas secara langsung, hanya skor keputusan.\n"
      ],
      "metadata": {
        "id": "D_HRE_5gbI-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Iris dataset (from Chapter 4)\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y_iris = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
        "\n",
        "# 1. Linear SVM Classification\n",
        "# Soft Margin Classification with LinearSVC\n",
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)) # C=1, loss=\"hinge\"\n",
        "])\n",
        "svm_clf.fit(X_iris, y_iris)\n",
        "\n",
        "print(\"Prediction for [5.5, 1.7] (Iris virginica):\", svm_clf.predict([[5.5, 1.7]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeJ9DMtMblxz",
        "outputId": "3bb50c39-93d4-486d-99d6-38cc3d4a6998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for [5.5, 1.7] (Iris virginica): [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can try SVC with linear kernel as an alternative\n",
        "svc_linear_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc_linear\", SVC(kernel=\"linear\", C=1, random_state=42))\n",
        "])\n",
        "svc_linear_clf.fit(X_iris, y_iris)\n",
        "print(\"SVC(kernel='linear') prediction for [5.5, 1.7]:\", svc_linear_clf.predict([[5.5, 1.7]]))\n",
        "\n",
        "# You can also try SGDClassifier for linear SVM\n",
        "from sklearn.linear_model import SGDClassifier # Import SGDClassifier\n",
        "sgd_linear_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"sgd_clf\", SGDClassifier(loss=\"hinge\", alpha=1/(len(X_iris)*1), random_state=42)) # alpha corresponds to 1/(m*C)\n",
        "])\n",
        "sgd_linear_clf.fit(X_iris, y_iris)\n",
        "print(\"SGDClassifier prediction for [5.5, 1.7]:\", sgd_linear_clf.predict([[5.5, 1.7]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti-fQmesb1Mg",
        "outputId": "af568f0a-316c-4ec1-b6cb-97f652c9d806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC(kernel='linear') prediction for [5.5, 1.7]: [1.]\n",
            "SGDClassifier prediction for [5.5, 1.7]: [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Nonlinear SVM Classification (Klasifikasi SVM Non-Linier)"
      ],
      "metadata": {
        "id": "7eAPZ0qIcBtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Menangani Data Non-Linier:\n",
        "  - Menambahkan Fitur Polinomial: Salah satu pendekatan adalah menambahkan fitur-fitur polinomial (seperti yang dilakukan di Chapter 4) untuk mengubah dataset agar dapat dipisahkan secara linier di ruang fitur yang lebih tinggi. PolynomialFeatures dari Scikit-Learn dapat digunakan untuk ini.\n",
        "  - Kernel Trick: Metode matematis \"kernel trick\" memungkinkan SVM untuk mencapai hasil yang sama seolah-olah fitur polinomial telah ditambahkan, tanpa benar-benar menambahkannya. Ini menghindari ledakan kombinatorial jumlah fitur.\n",
        "    - SVC Class: Kelas SVC dari Scikit-Learn mengimplementasikan kernel trick.\n",
        "    - Polynomial Kernel: Digunakan untuk data polinomial. Hyperparameter degree mengontrol derajat polinomial, dan coef0 mengontrol seberapa besar model dipengaruhi oleh polinomial derajat tinggi versus derajat rendah.\n",
        "- Similarity Features (Fitur Kemiripan):\n",
        "  - Pendekatan lain adalah menambahkan fitur yang dihitung menggunakan fungsi kemiripan (misalnya, Gaussian Radial Basis Function - RBF) yang mengukur seberapa mirip setiap instansi dengan landmark tertentu.\n",
        "  - Gaussian RBF Kernel: Kelas SVC juga mendukung kernel RBF. Hyperparameter gamma (γ) mengontrol bentuk kurva RBF. Nilai γ yang besar membuat kurva lebih sempit (rentang pengaruh instansi lebih kecil), menghasilkan batas keputusan yang lebih tidak teratur dan cenderung overfitting. Nilai γ yang kecil menghasilkan kurva yang lebih lebar (rentang pengaruh instansi lebih besar), batas keputusan yang lebih halus, dan cenderung underfitting. Jadi, γ bertindak seperti hyperparameter regularisasi.\n",
        "- Memilih Kernel:\n",
        "  - Sebagai aturan praktis, selalu coba linear kernel terlebih dahulu (LinearSVC lebih cepat daripada SVC(kernel=\"linear\")).\n",
        "  - Jika training set tidak terlalu besar, coba juga Gaussian RBF kernel karena ia bekerja dengan baik di sebagian besar kasus.\n",
        "  - Eksperimen dengan kernel lain menggunakan cross-validation dan grid search jika waktu dan daya komputasi memungkinkan.\n",
        "\n"
      ],
      "metadata": {
        "id": "hCz1ElsOcD7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate moons dataset (from Chapter 4)\n",
        "X_moons, y_moons = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "# 2. Nonlinear SVM Classification\n",
        "# Polynomial Features with LinearSVC\n",
        "polynomial_svm_clf = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
        "])\n",
        "polynomial_svm_clf.fit(X_moons, y_moons)\n",
        "\n",
        "print(\"\\nPolynomial Features with LinearSVC prediction for [[-1, 1]] (example):\", polynomial_svm_clf.predict([[-1, 1]]))\n",
        "\n",
        "# Polynomial Kernel with SVC\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5, random_state=42))\n",
        "])\n",
        "poly_kernel_svm_clf.fit(X_moons, y_moons)\n",
        "\n",
        "print(\"Polynomial Kernel SVC prediction for [[-1, 1]] (example):\", poly_kernel_svm_clf.predict([[-1, 1]]))\n",
        "\n",
        "# Gaussian RBF Kernel with SVC\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001, random_state=42)) # Example: high gamma, low C\n",
        "])\n",
        "rbf_kernel_svm_clf.fit(X_moons, y_moons)\n",
        "\n",
        "print(\"Gaussian RBF Kernel SVC prediction for [[-1, 1]] (example):\", rbf_kernel_svm_clf.predict([[-1, 1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eWDGZOqckWU",
        "outputId": "7969bf86-1ac8-49ff-dc54-8640a06d526f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Polynomial Features with LinearSVC prediction for [[-1, 1]] (example): [0]\n",
            "Polynomial Kernel SVC prediction for [[-1, 1]] (example): [0]\n",
            "Gaussian RBF Kernel SVC prediction for [[-1, 1]] (example): [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Computational Complexity (Kompleksitas Komputasi)"
      ],
      "metadata": {
        "id": "LBuHVyxPcy7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LinearSVC: Berbasis pustaka liblinear, kompleksitas pelatihan sekitar O(m×n). Skala hampir linier dengan jumlah instansi pelatihan dan jumlah fitur. Lebih lambat jika memerlukan presisi sangat tinggi (dikontrol oleh hyperparameter tol / ϵ).\n",
        "- SVC: Berbasis pustaka libsvm, mendukung kernel trick. Kompleksitas pelatihan biasanya antara O(m\n",
        "2\n",
        " ×n) dan O(m\n",
        "3\n",
        " ×n). Sangat lambat untuk jumlah instansi pelatihan yang besar (ratusan ribu). Cocok untuk training set yang kompleks, berukuran kecil hingga sedang. Skala baik dengan jumlah fitur, terutama dengan fitur sparse."
      ],
      "metadata": {
        "id": "z8PcvTQpc3dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabel berikut merangkum karakteristik utama dari beberapa kelas Support Vector Machine (SVM) yang umum digunakan dalam Scikit-Learn untuk tugas klasifikasi. Pemilihan kelas yang tepat bergantung pada ukuran dataset, jumlah fitur, dan apakah masalahnya bersifat linier atau non-linier.\n",
        "\n",
        "| Class          | Time complexity                 | Out-of-core support | Scaling required | Kernel trick |\n",
        "| :------------- | :------------------------------ | :------------------ | :--------------- | :----------- |\n",
        "| `LinearSVC`    | $O(m \\times n)$                 | No                  | Yes              | No           |\n",
        "| `SGDClassifier` | $O(m \\times n)$                 | Yes                 | Yes              | No           |\n",
        "| `SVC`          | $O(m^2 \\times n)$ to $O(m^3 \\times n)$ | No                  | Yes              | Yes          |\n",
        "\n",
        "**Penjelasan Kolom:**\n",
        "\n",
        "* **Class**: Nama kelas pengklasifikasi SVM di Scikit-Learn.\n",
        "    * [cite_start]`LinearSVC`: Menerapkan pengklasifikasi Support Vector Classification linier.\n",
        "    * [cite_start]`SGDClassifier`: Menerapkan pengklasifikasi linier menggunakan Stochastic Gradient Descent (SGD).\n",
        "    * [cite_start]`SVC`: Menerapkan pengklasifikasi Support Vector Classification yang mendukung kernel non-linier.\n",
        "\n",
        "* **Time complexity**: Menunjukkan bagaimana waktu pelatihan meningkat seiring dengan jumlah instansi ($m$) dan jumlah fitur ($n$).\n",
        "    * [cite_start]$O(m \\times n)$: Kompleksitas komputasi yang linier terhadap jumlah instansi dan jumlah fitur, membuatnya efisien untuk dataset besar dengan banyak fitur.\n",
        "    * [cite_start]$O(m^2 \\times n)$ to $O(m^3 \\times n)$: Kompleksitas komputasi yang meningkat secara kuadratik atau kubik terhadap jumlah instansi, menjadikannya sangat lambat untuk dataset yang sangat besar.\n",
        "\n",
        "* **Out-of-core support**: Menunjukkan apakah algoritma dapat menangani dataset yang tidak muat seluruhnya di memori (yaitu, dapat belajar secara inkremental dari *mini-batch* data).\n",
        "    * [cite_start]`Yes`: Algoritma dapat memproses data secara inkremental.\n",
        "    * [cite_start]`No`: Algoritma memerlukan seluruh dataset untuk dimuat ke dalam memori.\n",
        "\n",
        "* **Scaling required**: Menunjukkan apakah penskalaan fitur (misalnya, Standardisasi atau Normalisasi) direkomendasikan atau diperlukan untuk kinerja optimal.\n",
        "    * [cite_start]`Yes`: Penskalaan fitur sangat disarankan atau diperlukan karena algoritma sensitif terhadap skala atribut.\n",
        "\n",
        "* **Kernel trick**: Menunjukkan apakah kelas mendukung penggunaan *kernel trick* untuk menangani masalah klasifikasi non-linier dengan memetakan data secara implisit ke ruang berdimensi lebih tinggi.\n",
        "    * [cite_start]`Yes`: Kelas dapat menggunakan berbagai fungsi kernel non-linier.\n",
        "    * [cite_start]`No`: Kelas hanya mendukung batas keputusan linier di ruang fitur asli.\n",
        "\n",
        "**Catatan Tambahan:**\n",
        "\n",
        "* [cite_start]**`LinearSVC`** adalah pilihan yang baik untuk klasifikasi linier skala besar.\n",
        "* [cite_start]**`SGDClassifier`** dengan `loss=\"hinge\"` setara dengan SVM linier dan mendukung pembelajaran *out-of-core*, sehingga cocok untuk dataset yang sangat besar.\n",
        "* [cite_start]**`SVC`** adalah pilihan serbaguna untuk masalah non-linier berukuran kecil hingga menengah karena kemampuannya menggunakan *kernel trick*.\n",
        "\n"
      ],
      "metadata": {
        "id": "ugyJmoV3c8TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.SVM Regression (Regresi SVM)"
      ],
      "metadata": {
        "id": "eBOG6B4dj6tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Konsep: Membalikkan tujuan klasifikasi. Regresi SVM mencoba memasukkan sebanyak mungkin instansi ke dalam \"jalan\" sambil membatasi pelanggaran margin (instansi di luar \"jalan\"). Lebar \"jalan\" dikontrol oleh hyperparameter ϵ.\n",
        "- ϵ-Insensitive: Menambahkan lebih banyak instansi pelatihan di dalam margin tidak memengaruhi prediksi model, sehingga model disebut ϵ-insensitive.\n",
        "- Implementasi dengan Scikit-Learn:\n",
        "  - LinearSVR: Untuk Regresi SVM linier, memiliki kompleksitas O(m×n) dan skala linier dengan ukuran training set.\n",
        "  - SVR: Untuk Regresi SVM non-linier (mendukung kernel trick), tetapi sangat lambat untuk training set yang besar.\n"
      ],
      "metadata": {
        "id": "8XWpm15Fj820"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. SVM Regression\n",
        "# Generate some linear data (from Chapter 4)\n",
        "X_reg_svm = 2 * np.random.rand(100, 1)\n",
        "y_reg_svm = 4 + 3 * X_reg_svm + np.random.randn(100, 1)\n",
        "\n",
        "# Scale the data first for SVR (important for most regularized models)\n",
        "scaler_reg_svm = StandardScaler()\n",
        "X_reg_svm_scaled = scaler_reg_svm.fit_transform(X_reg_svm)\n",
        "y_reg_svm_scaled = scaler_reg_svm.fit_transform(y_reg_svm) # Scale target as well if needed, but not strictly required for Huber loss\n",
        "\n",
        "from sklearn.svm import LinearSVR, SVR\n",
        "\n",
        "# Linear SVR\n",
        "svm_reg = LinearSVR(epsilon=1.5, random_state=42) # epsilon controls margin width\n",
        "svm_reg.fit(X_reg_svm_scaled, y_reg_svm_scaled.ravel()) # ravel() to flatten y\n",
        "print(\"\\nLinear SVR prediction for scaled [[1.5]]:\", svm_reg.predict([[1.5]]))\n",
        "\n",
        "# Non-linear SVR with polynomial kernel\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X_reg_svm_scaled, y_reg_svm_scaled.ravel())\n",
        "print(\"Polynomial SVR prediction for scaled [[1.5]]:\", svm_poly_reg.predict([[1.5]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwLMXiukRnd",
        "outputId": "dca27b94-e0f8-41f7-e01b-a3d2e26a8def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear SVR prediction for scaled [[1.5]]: [1.34053499]\n",
            "Polynomial SVR prediction for scaled [[1.5]]: [0.29404287]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Under the Hood (Di Balik Layar)"
      ],
      "metadata": {
        "id": "wUcfR2N8kU3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagian ini menjelaskan detail matematis cara kerja SVM, yang dapat dilewati oleh pembaca yang baru memulai ML."
      ],
      "metadata": {
        "id": "oQJhAgdAkc0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Fungsi Keputusan dan Prediksi**: Pengklasifikasi SVM linier memprediksi kelas instansi baru $\\mathbf{x}$ dengan menghitung fungsi keputusan $\\mathbf{w}^T \\mathbf{x} + b$. [cite_start]Jika hasilnya positif, kelas prediksi adalah kelas positif (1), jika tidak, kelas negatif (0).\n",
        "    * [cite_start]$\\mathbf{w}$: vektor bobot fitur.\n",
        "    * [cite_start]$b$: *bias term*.\n",
        "* [cite_start]**Batas keputusan** adalah himpunan titik di mana fungsi keputusan sama dengan 0.\n",
        "* [cite_start]**Tujuan pelatihan** adalah menemukan $\\mathbf{w}$ dan $b$ yang memaksimalkan *margin* (yaitu, meminimalkan $||\\mathbf{w}||$, yang setara dengan meminimalkan $\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}$) sambil menghindari atau membatasi pelanggaran *margin*.\n",
        "\n",
        "* **Tujuan Pelatihan (Hard Margin)**:\n",
        "    * Meminimalkan $\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}$ tunduk pada kendala $t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1$ untuk semua instansi $i$. [cite_start]($t^{(i)}$ adalah -1 untuk kelas negatif dan 1 untuk kelas positif).\n",
        "\n",
        "* **Tujuan Pelatihan (Soft Margin)**:\n",
        "    * [cite_start]Meminimalkan $\\frac{1}{2}\\mathbf{w}^T \\mathbf{w} + C \\sum_{i=1}^{m} \\zeta^{(i)}$ tunduk pada kendala $t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)}$ dan $\\zeta^{(i)} \\ge 0$ untuk semua instansi $i$.\n",
        "    * [cite_start]$\\zeta^{(i)}$ adalah *slack variable* yang mengukur seberapa banyak instansi ke-$i$ diizinkan melanggar *margin*.\n",
        "    * [cite_start]$C$ adalah *hyperparameter* yang mengatur *trade-off* antara tujuan *margin* besar dan pelanggaran *margin* kecil.\n",
        "\n",
        "* [cite_start]**Quadratic Programming (QP)**: Masalah *hard margin* dan *soft margin* adalah masalah optimasi kuadratik *convex* dengan kendala linier, yang dikenal sebagai masalah *Quadratic Programming* (QP).\n",
        "\n",
        "* **The Dual Problem (Masalah Dual)**:\n",
        "    * Setiap masalah optimasi yang dibatasi (masalah *primal*) memiliki masalah *dual* yang terkait erat. [cite_start]Untuk SVM, masalah *dual* memiliki solusi yang sama dengan masalah *primal*.\n",
        "    * [cite_start]Masalah *dual* seringkali lebih cepat dipecahkan ketika jumlah instansi pelatihan lebih kecil dari jumlah fitur.\n",
        "    * [cite_start]Yang lebih penting, masalah *dual* memungkinkan *kernel trick*.\n",
        "\n",
        "* **Kernelized SVMs**:\n",
        "    * [cite_start]*Kernel trick* memungkinkan untuk menghitung *dot product* $\\phi(\\mathbf{a})^T \\phi(\\mathbf{b})$ di ruang fitur berdimensi tinggi (mungkin tak terbatas) hanya berdasarkan vektor asli $\\mathbf{a}$ dan $\\mathbf{b}$, tanpa perlu secara eksplisit mengubah vektor tersebut ke ruang fitur yang lebih tinggi.\n",
        "    * [cite_start]Contoh *kernel* umum: Linear, Polynomial, Gaussian RBF, Sigmoid.\n",
        "    * [cite_start]**Teorema Mercer**: Menjamin bahwa jika fungsi $K(\\mathbf{a}, \\mathbf{b})$ memenuhi kondisi tertentu, maka ada fungsi pemetaan $\\phi$ sehingga $K(\\mathbf{a}, \\mathbf{b}) = \\phi(\\mathbf{a})^T \\phi(\\mathbf{b})$.\n",
        "    * [cite_start]Prediksi SVM *kernelized* hanya melibatkan *dot product* antara vektor input baru dengan *support vectors*, bukan semua instansi pelatihan.\n",
        "\n",
        "* **Online SVMs**:\n",
        "    * [cite_start]Untuk SVM linier, *online learning* dapat diimplementasikan menggunakan *Gradient Descent* untuk meminimalkan fungsi biaya yang berasal dari masalah *primal*.\n",
        "    * [cite_start]**Hinge Loss**: Fungsi kerugian umum yang digunakan dalam SVM."
      ],
      "metadata": {
        "id": "ClBCpbyOkfWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Online SVMs (SVM Online)"
      ],
      "metadata": {
        "id": "rO25GBQnoT5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Untuk SVM linier, online learning dapat diimplementasikan menggunakan Gradient Descent untuk meminimalkan fungsi biaya yang berasal dari masalah primal.\n",
        "- Hinge Loss: Fungsi kerugian umum yang digunakan dalam SVM adalah hinge loss function, didefinisikan sebagai max(0,1−t). Ini adalah 0 ketika t≥1, dan derivatifnya adalah -1 jika t<1 dan 0 jika t>1. Ini tidak dapat didiferensiasi pada t=1."
      ],
      "metadata": {
        "id": "iVtZOp-LoZ5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konsep Kunci yang Dipelajari di \"Under the Hood\""
      ],
      "metadata": {
        "id": "LkjQf5UrofxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fungsi Keputusan: Peran w dan b dalam menentukan batas klasifikasi.\n",
        "- Tujuan Optimasi (Primal): Memaksimalkan margin (meminimalkan norma bobot) dengan atau tanpa slack variables.\n",
        "- Pemrograman Kuadratik (QP): Klasifikasi SVM sebagai masalah optimasi QP.\n",
        "- Masalah Dual: Alternatif formulasi masalah optimasi yang memungkinkan \"kernel trick\" dan seringkali lebih efisien.\n",
        "- Kernel Trick: Transformasi implisit ke ruang berdimensi tinggi untuk menangani non-linieritas tanpa komputasi eksplisit.\n",
        "- Hinge Loss: Fungsi kerugian spesifik untuk SVM.\n",
        "- Support Vectors: Instansi pelatihan yang memengaruhi model, dengan Lagrange multipliers α i >0.\n"
      ],
      "metadata": {
        "id": "o8Lwn18XojBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KESIMPULAN"
      ],
      "metadata": {
        "id": "D8KlW9yqozXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagian \"Under the Hood\" ini memberikan wawasan mendalam tentang dasar matematis yang kompleks namun elegan di balik SVM. Dengan memahami masalah optimasi primal dan dual, serta bagaimana \"kernel trick\" bekerja, pembaca memperoleh pemahaman yang lebih kuat tentang mengapa SVM sangat efektif untuk berbagai masalah klasifikasi dan regresi. Meskipun detail matematisnya mungkin menantang, pemahaman konsep-konsep ini sangat berharga untuk diagnosis masalah dan penyetelan model tingkat lanjut.\n"
      ],
      "metadata": {
        "id": "6EseaPOIo00I"
      }
    }
  ]
}