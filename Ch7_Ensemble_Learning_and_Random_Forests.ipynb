{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3nIMahmp8d9XLVAQd+Vpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch7_Ensemble_Learning_and_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentu, berikut adalah laporan analisis Bab 7 dalam format teks `.ipynb` dengan sitasi sesuai pedoman yang Anda berikan:\n",
        "\n",
        "\n",
        "# Laporan Analisis Bab 7: Ensemble Learning and Random Forests\n",
        "\n",
        "## Pendahuluan\n",
        "[cite_start]Bab 7 memperkenalkan konsep Ensemble Learning, sebuah teknik di mana prediksi dari sekelompok prediktor digabungkan untuk menghasilkan prediksi yang lebih baik daripada prediktor individual[cite: 236]. [cite_start]Bab ini menyoroti bagaimana Ensemble Methods sering digunakan di akhir proyek Machine Learning untuk meningkatkan kinerja model[cite: 236].\n",
        "\n",
        "## Ringkasan Isi Bab\n",
        "\n",
        "Bab 7 menguraikan beberapa konsep kunci terkait Ensemble Learning:\n",
        "\n",
        "1.  **Voting Classifiers**:\n",
        "    * [cite_start]Ide dasarnya adalah menggabungkan prediksi dari beberapa klasifier yang beragam (misalnya, Logistic Regression, SVM, Random Forest) dan memprediksi kelas yang mendapatkan suara terbanyak[cite: 237].\n",
        "    * [cite_start]**Hard Voting Classifier**: Klasifier ini hanya menghitung suara (prediksi kelas) dari setiap klasifier dalam ensemble dan memilih kelas yang paling banyak dipilih[cite: 237].\n",
        "    * [cite_start]**Soft Voting Classifier**: Klasifier ini menghitung rata-rata probabilitas kelas yang diestimasi oleh setiap klasifier dan memilih kelas dengan probabilitas rata-rata tertinggi[cite: 239]. [cite_start]Metode ini seringkali memiliki kinerja lebih tinggi karena memberikan bobot lebih pada suara yang sangat percaya diri[cite: 239].\n",
        "    * [cite_start]Ensemble Methods bekerja paling baik ketika prediktor-prediktor seindependen mungkin satu sama lain, yaitu membuat kesalahan yang tidak berkorelasi[cite: 238].\n",
        "\n",
        "2.  **Bagging dan Pasting**:\n",
        "    * [cite_start]Kedua metode ini melibatkan penggunaan algoritma pelatihan yang sama untuk setiap prediktor, tetapi melatihnya pada subset acak yang berbeda dari dataset pelatihan[cite: 240].\n",
        "    * [cite_start]**Bagging** (Bootstrap Aggregating): Pengambilan sampel dilakukan *dengan pengembalian* (bootstrap)[cite: 240]. [cite_start]Ini memperkenalkan lebih banyak keragaman pada subset pelatihan, sehingga menghasilkan bias yang sedikit lebih tinggi tetapi varians ensemble yang lebih rendah[cite: 240].\n",
        "    * [cite_start]**Pasting**: Pengambilan sampel dilakukan *tanpa pengembalian*[cite: 240].\n",
        "    * [cite_start]Prediktor dapat dilatih dan prediksi dapat dibuat secara paralel, membuat metode ini sangat skalabel[cite: 241].\n",
        "    * [cite_start]Secara keseluruhan, bagging seringkali menghasilkan model yang lebih baik[cite: 242].\n",
        "\n",
        "3.  **Out-of-Bag Evaluation (OOB)**:\n",
        "    * [cite_start]Dengan bagging, beberapa instance mungkin diambil sampelnya beberapa kali, sementara yang lain mungkin tidak diambil sama sekali untuk prediktor tertentu[cite: 242].\n",
        "    * [cite_start]Sekitar 37% dari instance pelatihan tidak diambil sampelnya untuk setiap prediktor; ini disebut instance *out-of-bag*[cite: 242].\n",
        "    * [cite_start]Prediktor dapat dievaluasi pada instance OOB ini, sehingga tidak memerlukan *validation set* terpisah[cite: 242].\n",
        "    * [cite_start]Scikit-Learn menyediakan `oob_score_` untuk mengakses skor evaluasi OOB secara otomatis setelah pelatihan[cite: 243].\n",
        "\n",
        "4.  **Random Patches dan Random Subspaces**:\n",
        "    * [cite_start]Kelas `BaggingClassifier` mendukung pengambilan sampel fitur juga, dikontrol oleh `max_features` dan `bootstrap_features`[cite: 243].\n",
        "    * [cite_start]**Random Patches**: Mengambil sampel instance pelatihan dan fitur[cite: 243].\n",
        "    * [cite_start]**Random Subspaces**: Mempertahankan semua instance pelatihan tetapi mengambil sampel fitur[cite: 244].\n",
        "    * [cite_start]Pengambilan sampel fitur meningkatkan keragaman prediktor, menukar bias yang sedikit lebih tinggi dengan varians yang lebih rendah[cite: 244].\n",
        "\n",
        "5.  **Random Forests**:\n",
        "    * [cite_start]Merupakan kumpulan Decision Trees, umumnya dilatih melalui metode bagging, biasanya dengan `max_samples` diatur ke ukuran dataset pelatihan[cite: 244].\n",
        "    * [cite_start]Kelas `RandomForestClassifier` di Scikit-Learn lebih mudah digunakan dan dioptimalkan untuk Decision Trees daripada `BaggingClassifier`[cite: 244, 245].\n",
        "    * [cite_start]Algoritma Random Forest memperkenalkan keacakan ekstra saat menumbuhkan pohon: alih-alih mencari fitur terbaik, ia mencari fitur terbaik di antara subset fitur acak[cite: 244]. [cite_start]Ini umumnya menghasilkan model yang lebih baik secara keseluruhan[cite: 244].\n",
        "\n",
        "6.  **Extra-Trees (Extremely Randomized Trees)**:\n",
        "    * [cite_start]Mirip dengan Random Forests, tetapi memperkenalkan lebih banyak keacakan dengan juga menggunakan *random thresholds* untuk setiap fitur, daripada mencari *threshold* terbaik[cite: 245].\n",
        "    * [cite_start]Ini menukar bias yang lebih tinggi dengan varians yang lebih rendah, dan juga membuat pelatihan lebih cepat daripada Random Forests biasa[cite: 245].\n",
        "\n",
        "7.  **Feature Importance**:\n",
        "    * [cite_start]Random Forests memudahkan pengukuran kepentingan relatif setiap fitur[cite: 245].\n",
        "    * [cite_start]Scikit-Learn mengukur kepentingan fitur dengan melihat seberapa banyak node pohon yang menggunakan fitur tersebut mengurangi *impurity* secara rata-rata (di seluruh pohon dalam hutan)[cite: 245].\n",
        "    * [cite_start]Skor ini dapat diakses melalui variabel `feature_importances_` setelah pelatihan[cite: 245].\n",
        "\n",
        "8.  **Boosting**:\n",
        "    * [cite_start]Sebuah Ensemble Method yang menggabungkan beberapa *weak learners* menjadi *strong learner*[cite: 199].\n",
        "    * [cite_start]Ide umum sebagian besar metode *boosting* adalah melatih prediktor secara berurutan, dengan setiap prediktor mencoba mengoreksi kesalahan prediktor sebelumnya[cite: 199].\n",
        "    * [cite_start]**AdaBoost** (Adaptive Boosting): Prediktor baru lebih memperhatikan instance pelatihan yang kurang *underfit* oleh pendahulunya, dengan meningkatkan bobot relatif instance yang salah diklasifikasi[cite: 200].\n",
        "    * [cite_start]**Gradient Boosting**: Setiap prediktor baru mencoba mencocokkan *residual errors* yang dibuat oleh prediktor sebelumnya[cite: 203]. [cite_start]Contohnya adalah Gradient Tree Boosting (GBRT) menggunakan Decision Trees sebagai *base predictors*[cite: 203].\n",
        "    * [cite_start]Teknik *shrinkage* (menurunkan `learning_rate`) dapat meningkatkan generalisasi[cite: 205].\n",
        "    * [cite_start]*Early stopping* juga dapat digunakan untuk menemukan jumlah pohon yang optimal[cite: 206].\n",
        "    * [cite_start]XGBoost adalah implementasi Gradient Boosting yang sangat cepat, skalabel, dan portabel[cite: 208].\n",
        "\n",
        "9.  **Stacking (Stacked Generalization)**:\n",
        "    * [cite_start]Menggabungkan prediksi dari semua prediktor dalam ensemble dengan melatih model terpisah (*blender* atau *meta learner*) untuk melakukan agregasi ini[cite: 208].\n",
        "    * [cite_start]Untuk melatih *blender*, dataset pelatihan dibagi menjadi subset[cite: 209]. [cite_start]Subset pertama melatih prediktor layer pertama, dan prediksi prediktor ini pada subset kedua digunakan sebagai fitur input untuk melatih *blender*[cite: 209, 210].\n",
        "    * [cite_start]Scikit-Learn tidak mendukung stacking secara langsung, tetapi dapat diimplementasikan secara manual atau menggunakan pustaka pihak ketiga seperti DESlib[cite: 211].\n",
        "\n",
        "## Analisis dan Relevansi untuk Mahasiswa\n",
        "\n",
        "Bab 7 memberikan fondasi yang sangat penting dalam memahami Ensemble Learning, sebuah pilar dalam Machine Learning yang sangat efektif untuk meningkatkan kinerja model.\n",
        "\n",
        "* **Peningkatan Kinerja Model**: Konsep inti bahwa menggabungkan prediktor dapat menghasilkan model yang lebih kuat adalah wawasan fundamental. Mahasiswa belajar bahwa \"kebijaksanaan kerumunan\" berlaku dalam Machine Learning.\n",
        "* **Diversitas adalah Kunci**: Bab ini menekankan pentingnya diversitas prediktor dalam ensemble. Baik melalui penggunaan algoritma yang berbeda (Voting) maupun subset data yang berbeda (Bagging/Pasting), diversitas ini mengurangi korelasi kesalahan dan meningkatkan ketahanan model.\n",
        "* **Strategi Overfitting/Underfitting**: Metode seperti Bagging (mengurangi varians) dan Boosting (mengurangi bias) memberikan mahasiswa alat strategis untuk mengatasi masalah *overfitting* dan *underfitting* yang umum.\n",
        "* **Pemahaman Algoritma Lanjutan**: Penjelasan mendalam tentang AdaBoost dan Gradient Boosting, termasuk konsep *residual errors* dan *shrinkage*, membantu mahasiswa memahami bagaimana algoritma kompleks dibangun dan dioptimalkan.\n",
        "* **Praktik Terbaik dan Performa**: Diskusi tentang OOB evaluation, Feature Importance, dan pertimbangan *trade-off* (misalnya, bias/varians, kecepatan/akurasi) mengajarkan praktik terbaik yang relevan dalam proyek ML nyata.\n",
        "* **Skalabilitas**: Penekanan pada kemampuan paralelisasi Bagging/Pasting vs. sekuensial Boosting memberikan pemahaman tentang efisiensi komputasi untuk dataset besar.\n",
        "* **Dasar untuk Kompetisi ML**: Banyak solusi pemenang dalam kompetisi ML melibatkan Ensemble Methods, sehingga pemahaman ini sangat relevan bagi mahasiswa yang tertarik pada bidang ini.\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab 7 dengan efektif menguraikan prinsip-prinsip, metode, dan aplikasi Ensemble Learning. Mahasiswa akan mendapatkan wawasan yang komprehensif tentang bagaimana dan mengapa Ensemble Methods bekerja, melengkapi pemahaman mereka tentang model tunggal dari bab sebelumnya. Bab ini merupakan jembatan penting menuju teknik-teknik Machine Learning yang lebih canggih dan praktis.\n"
      ],
      "metadata": {
        "id": "HMW6btYMmiBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REPRODUCE CODE"
      ],
      "metadata": {
        "id": "uSlKWFbRoNSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.svm import SVC # Import SVC untuk VotingClassifier\n",
        "import joblib # Untuk menyimpan/memuat model (disebut di buku)\n",
        "\n",
        "# Menghasilkan dataset moons untuk contoh klasifikasi\n",
        "# Menggunakan random_state untuk hasil yang reproducible\n",
        "X, y = make_moons(n_samples=10000, noise=0.15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# --- BAGIAN 1: VOTING CLASSIFIERS ---\n",
        "print(\"--- Voting Classifiers ---\")\n",
        "\n",
        "# Inisialisasi base classifiers\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(random_state=42)\n",
        "# SVC dengan probability=True diperlukan untuk soft voting\n",
        "svm_clf = SVC(random_state=42, probability=True)\n",
        "\n",
        "# Hard Voting Classifier\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_clf_hard.fit(X_train, y_train) # Melatih voting classifier\n",
        "\n",
        "print(\"Akurasi Hard Voting Classifier:\")\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf_hard):\n",
        "    # Melatih setiap klasifier secara individual untuk mendapatkan akurasinya\n",
        "    # Jika sudah dilatih sebelumnya di voting_clf, ini akan melatih ulang\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Soft Voting Classifier\n",
        "# Membutuhkan probability=True pada SVC yang sudah diinisialisasi\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf_soft.fit(X_train, y_train) # Melatih soft voting classifier\n",
        "\n",
        "print(\"\\nAkurasi Soft Voting Classifier:\")\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf_soft):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NR3O-3cnMX2",
        "outputId": "5af2f85b-6922-44a9-ee16-ed808d8a7409"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Voting Classifiers ---\n",
            "Akurasi Hard Voting Classifier:\n",
            "LogisticRegression: 0.8892\n",
            "RandomForestClassifier: 0.9908\n",
            "SVC: 0.9920\n",
            "VotingClassifier: 0.9904\n",
            "\n",
            "Akurasi Soft Voting Classifier:\n",
            "LogisticRegression: 0.8892\n",
            "RandomForestClassifier: 0.9908\n",
            "SVC: 0.9920\n",
            "VotingClassifier: 0.9908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 2: BAGGING DAN PASTING ---\n",
        "print(\"\\n--- Bagging dan Pasting ---\")\n",
        "\n",
        "# Bagging (default bootstrap=True)\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "print(f\"Akurasi BaggingClassifier: {accuracy_score(y_test, y_pred_bag):.4f}\")\n",
        "\n",
        "# Pasting (bootstrap=False)\n",
        "paste_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=False, n_jobs=-1, random_state=42\n",
        ")\n",
        "paste_clf.fit(X_train, y_train)\n",
        "y_pred_paste = paste_clf.predict(X_test)\n",
        "print(f\"Akurasi PastingClassifier: {accuracy_score(y_test, y_pred_paste):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4pSSjnFnf4O",
        "outputId": "1159391d-3253-4ab2-c4fb-3cfe938dd634"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bagging dan Pasting ---\n",
            "Akurasi BaggingClassifier: 0.9784\n",
            "Akurasi PastingClassifier: 0.9788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 3: OUT-OF-BAG EVALUATION ---\n",
        "print(\"\\n--- Out-of-Bag Evaluation ---\")\n",
        "\n",
        "bag_clf_oob = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=42\n",
        ")\n",
        "bag_clf_oob.fit(X_train, y_train)\n",
        "# oob_score_ tersedia setelah fit\n",
        "print(f\"OOB Score: {bag_clf_oob.oob_score_:.4f}\")\n",
        "y_pred_oob_check = bag_clf_oob.predict(X_test)\n",
        "print(f\"Akurasi Test (Check OOB): {accuracy_score(y_test, y_pred_oob_check):.4f}\")\n",
        "\n",
        "# Optional: Inspect oob_decision_function_ if base_estimator supports predict_proba\n",
        "# print(bag_clf_oob.oob_decision_function_[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31yiayusnn9P",
        "outputId": "bd156f6a-f53c-42e2-cb48-ddbc2fe12f28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Out-of-Bag Evaluation ---\n",
            "OOB Score: 0.9905\n",
            "Akurasi Test (Check OOB): 0.9892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 4: RANDOM FORESTS ---\n",
        "print(\"\\n--- Random Forests ---\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rnd_clf_rf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "rnd_clf_rf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf_rf.predict(X_test)\n",
        "print(f\"Akurasi RandomForestClassifier: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "\n",
        "# Mengukur Feature Importance (contoh dengan dataset Iris)\n",
        "from sklearn.datasets import load_iris # Import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf_iris = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
        "rnd_clf_iris.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "print(\"\\nFeature Importances (Iris Dataset):\")\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf_iris.feature_importances_):\n",
        "    print(f\"  {name}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL4TuvoBnvZB",
        "outputId": "dd067eeb-1149-4167-9e8a-377ac5ecf345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Random Forests ---\n",
            "Akurasi RandomForestClassifier: 0.9868\n",
            "\n",
            "Feature Importances (Iris Dataset):\n",
            "  sepal length (cm): 0.1125\n",
            "  sepal width (cm): 0.0231\n",
            "  petal length (cm): 0.4410\n",
            "  petal width (cm): 0.4234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 5: BOOSTING (ADABOOST & GRADIENT BOOSTING) ---\n",
        "print(\"\\n--- Boosting ---\")\n",
        "\n",
        "# AdaBoost Classifier (menggunakan Decision Stump sebagai base estimator)\n",
        "# Decision Stump adalah Decision Tree dengan max_depth=1\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1, random_state=42), n_estimators=200,\n",
        "    algorithm=\"SAMME\", learning_rate=0.5, random_state=42\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)\n",
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "print(f\"Akurasi AdaBoostClassifier: {accuracy_score(y_test, y_pred_ada):.4f}\")\n",
        "\n",
        "# GradientBoostingRegressor (contoh regresi)\n",
        "# Menghasilkan data regresi untuk GBRT\n",
        "np.random.seed(42) # Untuk reproducibility\n",
        "m = 100\n",
        "X_reg = 6 * np.random.rand(m, 1) - 3\n",
        "y_reg = 0.5 * X_reg**2 + X_reg + 2 + np.random.randn(m, 1)\n",
        "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_reg, y_reg, random_state=42)\n",
        "\n",
        "# Contoh GBRT sederhana\n",
        "gbrt_simple = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt_simple.fit(X_train_reg, y_train_reg.ravel()) # .ravel() karena y_reg adalah 2D array\n",
        "\n",
        "print(f\"\\nMSE GradientBoostingRegressor (3 estimators): {mean_squared_error(y_val_reg, gbrt_simple.predict(X_val_reg)):.4f}\")\n",
        "\n",
        "# Ilustrasi Early Stopping untuk Gradient Boosting\n",
        "gbrt_es = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt_es.n_estimators = n_estimators # Menambah estimator secara inkremental\n",
        "    gbrt_es.fit(X_train_reg, y_train_reg.ravel())\n",
        "    y_pred_es = gbrt_es.predict(X_val_reg)\n",
        "    val_error = mean_squared_error(y_val_reg, y_pred_es)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5: # Hentikan jika error tidak membaik selama 5 iterasi\n",
        "            break\n",
        "\n",
        "print(f\"Jumlah estimator optimal dengan early stopping: {gbrt_es.n_estimators}\")\n",
        "print(f\"MSE minimal dengan early stopping: {min_val_error:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs0bH958n6oO",
        "outputId": "8438de3a-6891-4406-aae4-3b4a9e1dd7f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Boosting ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi AdaBoostClassifier: 0.9920\n",
            "\n",
            "MSE GradientBoostingRegressor (3 estimators): 1.0133\n",
            "Jumlah estimator optimal dengan early stopping: 58\n",
            "MSE minimal dengan early stopping: 0.7983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAGIAN 6: STACKING (KONSEPTUAL) ---\n",
        "print(\"\\n--- Stacking (Konseptual) ---\")\n",
        "\n",
        "# Karena Scikit-Learn tidak memiliki kelas Stacking langsung di versi yang mungkin diacu buku,\n",
        "# ini adalah implementasi konseptual sesuai penjelasan.\n",
        "\n",
        "# Asumsikan kita menggunakan log_clf, rnd_clf, dan svm_clf sebagai base learners\n",
        "# Split data pelatihan menjadi dua subset: satu untuk melatih base learners, satu untuk melatih blender\n",
        "X_train_base, X_train_blender, y_train_base, y_train_blender = train_test_split(\n",
        "    X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Melatih base learners pada X_train_base\n",
        "log_clf.fit(X_train_base, y_train_base)\n",
        "rnd_clf.fit(X_train_base, y_train_base)\n",
        "svm_clf.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Base learners membuat prediksi pada X_train_blender (sebagai input untuk blender)\n",
        "# Menggunakan predict_proba untuk soft voting style\n",
        "X_blender_input = np.c_[\n",
        "    log_clf.predict_proba(X_train_blender)[:, 1],\n",
        "    rnd_clf.predict_proba(X_train_blender)[:, 1],\n",
        "    svm_clf.predict_proba(X_train_blender)[:, 1]\n",
        "]\n",
        "\n",
        "# Melatih blender (misalnya Logistic Regression) pada prediksi base learners\n",
        "blender = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "blender.fit(X_blender_input, y_train_blender)\n",
        "\n",
        "# Mengevaluasi ensemble Stacking pada test set\n",
        "# Base learners membuat prediksi pada X_test\n",
        "X_test_blender_input = np.c_[\n",
        "    log_clf.predict_proba(X_test)[:, 1],\n",
        "    rnd_clf.predict_proba(X_test)[:, 1],\n",
        "    svm_clf.predict_proba(X_test)[:, 1]\n",
        "]\n",
        "y_pred_stacking = blender.predict(X_test_blender_input)\n",
        "\n",
        "print(f\"Akurasi Stacking (Konseptual): {accuracy_score(y_test, y_pred_stacking):.4f}\")\n",
        "\n",
        "# Note: Untuk implementasi stacking yang lebih modern di Scikit-Learn (versi 0.22+),\n",
        "# ada kelas StackingClassifier atau StackingRegressor.\n",
        "# Contoh:\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# stacking_clf = StackingClassifier(\n",
        "#     estimators=[('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)],\n",
        "#     final_estimator=LogisticRegression(),\n",
        "#     cv=5\n",
        "# )\n",
        "# stacking_clf.fit(X_train, y_train)\n",
        "# print(f\"Akurasi StackingClassifier (built-in): {accuracy_score(y_test, stacking_clf.predict(X_test)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgDDBFIoEHz",
        "outputId": "034ccc4a-7312-44ba-ae0f-b37920134dc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stacking (Konseptual) ---\n",
            "Akurasi Stacking (Konseptual): 0.9932\n"
          ]
        }
      ]
    }
  ]
}