{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6uPxsnNByA4Bg09sZZHsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch12_Custom_Models_and_Training_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laporan Analisis Bab 12: Custom Models and Training with TensorFlow\n",
        "\n",
        "## Pendahuluan\n",
        "Bab 12 mendalami TensorFlow API level rendah setelah sebelumnya fokus pada `tf.keras`. Meskipun `tf.keras` sudah sangat powerful dan mencakup sebagian besar kasus penggunaan, ada situasi di mana kontrol yang lebih granular diperlukan. [cite_start]Bab ini membahas bagaimana menulis fungsi *loss* kustom, metrik, lapisan, model, *initializer*, *regularizer*, hingga mengendalikan *training loop* secara manual, serta bagaimana mengoptimalkan kode kustom menggunakan fitur *automatic graph generation* TensorFlow (TF Functions)[cite: 422].\n",
        "\n",
        "## Ringkasan Isi Bab\n",
        "\n",
        "Bab 12 menguraikan beberapa konsep kunci terkait Model Kustom dan Pelatihan dengan TensorFlow:\n",
        "\n",
        "1.  **A Quick Tour of TensorFlow (Sekilas TensorFlow)**:\n",
        "    * [cite_start]**Definisi**: Pustaka komputasi numerik yang kuat, sangat cocok untuk *Machine Learning* skala besar[cite: 423]. [cite_start]Dikembangkan oleh tim Google Brain dan mendukung banyak layanan Google[cite: 423].\n",
        "    * **Fitur Inti**:\n",
        "        * [cite_start]Mirip NumPy, tetapi dengan dukungan GPU[cite: 423].\n",
        "        * [cite_start]Mendukung *distributed computing* (di banyak perangkat dan server)[cite: 423].\n",
        "        * [cite_start]Memiliki kompiler *just-in-time* (JIT) yang mengoptimalkan komputasi (kecepatan dan penggunaan memori) dengan mengekstrak dan mengeksekusi *computation graph* dari fungsi Python[cite: 423].\n",
        "        * [cite_start]*Computation graph* dapat diekspor ke format portabel[cite: 423].\n",
        "        * [cite_start]Mengimplementasikan *autodiff* (diferensiasi otomatis) dan menyediakan *optimizer* yang sangat baik (misalnya, RMSProp, Nadam)[cite: 423].\n",
        "    * [cite_start]**Arsitektur TensorFlow**: Sebagian besar kode menggunakan API level tinggi (`tf.keras`, `tf.data`), tetapi API Python level rendah memungkinkan penanganan *tensor* secara langsung[cite: 424]. [cite_start]Operasi TensorFlow (ops) diimplementasikan dalam kode C++ yang sangat efisien, dengan *kernel* khusus untuk CPU, GPU, atau TPU[cite: 424].\n",
        "    * [cite_start]**Ecosystem TensorFlow**: Meliputi TensorBoard (visualisasi), TensorFlow Extended (TFX) untuk produksi ML, TensorFlow Hub (model *pretrained*), dan *model garden*[cite: 425].\n",
        "\n",
        "2.  **Using TensorFlow like NumPy (Menggunakan TensorFlow seperti NumPy)**:\n",
        "    * [cite_start]**Tensors dan Operasi**: *Tensor* sangat mirip dengan NumPy `ndarray` (array multidimensi atau skalar)[cite: 426]. Dibuat dengan `tf.constant()`. Memiliki `shape` dan `dtype`. Mendukung *indexing* seperti NumPy dan berbagai operasi *tensor* (misalnya, `tf.add()`, `tf.square()`, `tf.matmul()`).\n",
        "    * **Tensors dan NumPy**: Berinteraksi dengan baik. [cite_start]Dapat membuat *tensor* dari array NumPy, dan sebaliknya (`.numpy()` atau `np.array()`)[cite: 428]. [cite_start]Dapat menerapkan operasi TensorFlow ke array NumPy, dan operasi NumPy ke *tensor*[cite: 428]. [cite_start]TensorFlow default ke presisi 32-bit, NumPy 64-bit[cite: 428].\n",
        "    * [cite_start]**Type Conversions (Konversi Tipe)**: TensorFlow tidak melakukan konversi tipe secara otomatis untuk menghindari masalah kinerja[cite: 428]. [cite_start]Akan memunculkan *exception* jika tipe tidak kompatibel[cite: 429]. [cite_start]Gunakan `tf.cast()` untuk konversi eksplisit[cite: 429].\n",
        "    * [cite_start]**Variables (Variabel)**: `tf.Tensor` bersifat *immutable* (tidak dapat diubah)[cite: 429]. [cite_start]`tf.Variable` diperlukan untuk parameter model yang berubah selama pelatihan[cite: 429]. [cite_start]Bertindak seperti `tf.Tensor` tetapi dapat dimodifikasi di tempat menggunakan `assign()` (atau `assign_add()`, `assign_sub()`)[cite: 429].\n",
        "\n",
        "3.  **Other Data Structures (Struktur Data Lain)**:\n",
        "    * [cite_start]**Sparse tensors (`tf.SparseTensor`)**: Representasi efisien untuk *tensor* yang sebagian besar nol[cite: 430].\n",
        "    * [cite_start]**Tensor arrays (`tf.TensorArray`)**: Daftar *tensor* dengan ukuran tetap atau dinamis[cite: 430].\n",
        "    * [cite_start]**Ragged tensors (`tf.RaggedTensor`)**: Representasi daftar *tensor* yang statis, di mana *slice* (irisan) dimensi dapat memiliki panjang yang berbeda[cite: 430].\n",
        "    * [cite_start]**String tensors (`tf.string`)**: Merepresentasikan *byte strings* atau *Unicode strings*[cite: 430]. [cite_start]Operasi penanganan *string* di `tf.strings`[cite: 430].\n",
        "    * [cite_start]**Sets**: Direpresentasikan sebagai *tensor* biasa (atau *sparse tensor*) dari integer atau string[cite: 431].\n",
        "    * **Queues (`tf.queue`)**: Menyimpan *tensor* di banyak langkah. [cite_start]Ada berbagai jenis seperti FIFOQueue, PriorityQueue, RandomShuffleQueue, PaddingFIFOQueue[cite: 431]. [cite_start]Saat ini kurang relevan karena adanya API `tf.data`[cite: 431].\n",
        "\n",
        "4.  **Customizing Models and Training Algorithms (Mengustomisasi Model dan Algoritma Pelatihan)**:\n",
        "    * [cite_start]**Custom Loss Functions (Fungsi Loss Kustom)**: Dibuat sebagai fungsi Python yang menerima `y_true` dan `y_pred`, dan menggunakan operasi TensorFlow untuk menghitung *loss* per instance[cite: 431]. [cite_start]Contoh: *Huber Loss*[cite: 431]. [cite_start]Penting untuk mengembalikan *tensor* *loss* per instance[cite: 431].\n",
        "    * [cite_start]**Saving and Loading Models That Contain Custom Components**: Saat menyimpan model dengan fungsi *loss* kustom (yang bukan subclass `keras.losses.Loss`), nama fungsi akan disimpan, dan harus disediakan melalui `custom_objects` saat memuat model[cite: 432]. [cite_start]Untuk *hyperparameter* yang perlu disimpan, buat subclass dari `keras.losses.Loss` dan implementasikan metode `get_config()`[cite: 433].\n",
        "    * **Custom Activation Functions, Initializers, Regularizers, and Constraints**: Sebagian besar fungsionalitas Keras dapat dikustomisasi dengan fungsi Python sederhana yang memiliki input dan output yang sesuai. Untuk *hyperparameter* yang perlu disimpan, buat subclass dari `keras.regularizers.Regularizer`, `keras.constraints.Constraint`, `keras.initializers.Initializer`, atau `keras.layers.Layer`.\n",
        "    * **Custom Metrics (Metrik Kustom)**: Berbeda dengan *loss* (harus *differentiable*), *metric* digunakan untuk evaluasi dan tidak harus *differentiable*. [cite_start]Fungsi *metric* sederhana mirip fungsi *loss*[cite: 435]. Untuk *streaming metric* (yang diperbarui secara bertahap di setiap *batch* dan menghitung nilai kumulatif, seperti presisi), subclass `keras.metrics.Metric` dan implementasikan `__init__()`, `update_state()`, `result()`, dan `get_config()`.\n",
        "    * **Custom Layers (Lapisan Kustom)**:\n",
        "        * [cite_start]Untuk lapisan tanpa bobot (misalnya, `Flatten`, `ReLU`), gunakan `keras.layers.Lambda` dengan fungsi Python[cite: 438].\n",
        "        * Untuk lapisan *stateful* (dengan bobot), subclass `keras.layers.Layer`. Implementasikan `__init__()` (untuk *hyperparameter*), `build()` (untuk membuat variabel layer dengan `add_weight()`), `call()` (untuk melakukan komputasi layer), dan `compute_output_shape()` (mengembalikan bentuk output).\n",
        "        * [cite_start]Untuk lapisan dengan banyak input/output atau perilaku yang berbeda selama pelatihan/pengujian, sesuaikan metode `call()` dan `compute_output_shape()` untuk menangani *tuple* input/output dan argumen `training=None`[cite: 440].\n",
        "    * **Custom Models (Model Kustom)**:\n",
        "        * [cite_start]Subclass `keras.Model`, buat lapisan dan variabel di konstruktor, dan implementasikan metode `call()` untuk mendefinisikan *forward pass* model[cite: 441]. [cite_start]Ini memungkinkan arsitektur yang kompleks seperti *ResidualBlock*, loop, dan *skip connections*[cite: 442].\n",
        "        * Model kustom dapat digunakan seperti lapisan dalam model lain. Jika ingin model dapat disimpan dengan `model.save()`, perlu mengimplementasikan `get_config()`.\n",
        "    * **Losses and Metrics Based on Model Internals (Loss dan Metrik Berdasarkan Internal Model)**:\n",
        "        * [cite_start]Untuk *loss* yang bergantung pada internal model (misalnya, bobot atau aktivasi *hidden layer*), hitung *loss* di metode `call()` model dan tambahkan ke model dengan `self.add_loss()`[cite: 444].\n",
        "        * [cite_start]Untuk metrik yang bergantung pada internal model, buat objek metrik (misalnya, `keras.metrics.Mean`) di konstruktor, panggil di `call()` dengan nilai internal yang diinginkan, dan tambahkan ke model dengan `self.add_metric()`[cite: 445].\n",
        "\n",
        "5.  **Computing Gradients Using Autodiff (Menghitung Gradien Menggunakan Autodiff)**:\n",
        "    * [cite_start]**`tf.GradientTape()`**: Digunakan untuk merekam operasi yang melibatkan `tf.Variable`[cite: 446].\n",
        "    * [cite_start]**`tape.gradient(output, variables)`**: Menghitung gradien dari *output* terhadap *variable* yang diamati[cite: 446]. [cite_start]Tape otomatis terhapus setelah `gradient()` dipanggil (kecuali `persistent=True`)[cite: 447].\n",
        "    * Dapat memaksa *tape* untuk \"mengamati\" *tensor* selain *variable* dengan `tape.watch()`.\n",
        "    * [cite_start]`tf.stop_gradient()`: Menghentikan gradien dari *backpropagate* melalui bagian tertentu dari jaringan[cite: 448].\n",
        "    * [cite_start]`@tf.custom_gradient`: Dekorator untuk menyediakan fungsi gradien kustom yang lebih stabil secara numerik[cite: 449].\n",
        "\n",
        "6.  **Custom Training Loops (Loop Pelatihan Kustom)**:\n",
        "    * [cite_start]**Kapan Digunakan**: Jika metode `fit()` tidak cukup fleksibel (misalnya, ingin menggunakan *optimizer* berbeda untuk bagian jaringan yang berbeda, atau untuk kontrol penuh atas proses pelatihan/debugging)[cite: 450].\n",
        "    * **Implementasi**: Melibatkan loop bersarang untuk *epochs* dan *batches*. [cite_start]Di setiap langkah: ambil *batch*, lakukan *forward pass*, hitung *loss* (termasuk *regularization loss* dari `model.losses`), hitung gradien dengan `tf.GradientTape().gradient()`, dan terapkan gradien dengan `optimizer.apply_gradients()`[cite: 451].\n",
        "    * [cite_start]**Penting**: Perlu menangani `training=True` untuk lapisan yang berperilaku berbeda selama pelatihan (misalnya, `BatchNormalization`, `Dropout`)[cite: 452].\n",
        "\n",
        "7.  **TensorFlow Functions and Graphs (Fungsi dan Graf TensorFlow)**:\n",
        "    * [cite_start]**`@tf.function`**: Dekorator untuk mengubah fungsi Python menjadi Fungsi TensorFlow (TF Function)[cite: 453].\n",
        "    * [cite_start]**Manfaat**: Fungsi TensorFlow dianalisis untuk menghasilkan *computation graph* yang dioptimalkan, seringkali berjalan lebih cepat daripada fungsi Python asli[cite: 453].\n",
        "    * [cite_start]**Polimorfisme**: TF Function menghasilkan *concrete function* baru (dengan grafik khusus) untuk setiap kombinasi unik bentuk dan tipe data input (*input signature*)[cite: 454].\n",
        "    * **AutoGraph dan Tracing**: TensorFlow menganalisis kode sumber Python (*AutoGraph*) untuk menangkap pernyataan kontrol alur (loop, if) dan menggantinya dengan operasi TensorFlow yang sesuai (misalnya, `tf.while_loop()`, `tf.cond()`). [cite_start]Kemudian, saat dipanggil dengan *symbolic tensor* (tanpa nilai sebenarnya), grafik komputasi dihasilkan (*tracing*)[cite: 455].\n",
        "    * **TF Function Rules**: Aturan penting untuk menulis TF Function yang benar: hanya gunakan operasi TensorFlow (atau bungkus kode non-TensorFlow dengan `tf.py_function()`), pastikan *side effect* terjadi sesuai keinginan (hanya saat *tracing*), variabel TensorFlow harus dibuat pada panggilan pertama (atau di luar fungsi) dan dimodifikasi dengan `assign()`, kode sumber harus tersedia, dan loop harus berulang di atas `tf.range()` untuk dijadikan loop dinamis dalam grafik.\n",
        "    * **`tf.keras` dan TF Functions**: Secara default, komponen kustom di `tf.keras` secara otomatis diubah menjadi TF Function. Dapat dinonaktifkan dengan `dynamic=True` atau `run_eagerly=True`.\n",
        "\n",
        "## Analisis dan Relevansi untuk Mahasiswa\n",
        "\n",
        "Bab 12 adalah lompatan signifikan bagi mahasiswa yang ingin bergerak melampaui penggunaan API level tinggi dan mendapatkan kontrol lebih dalam atas TensorFlow dan Deep Learning.\n",
        "\n",
        "* **Pemahaman Fundamental TensorFlow**: Bab ini memperkuat pemahaman tentang *tensor*, *variable*, dan operasi dasar TensorFlow, yang merupakan fondasi dari semua yang terjadi \"di bawah kap\" Keras.\n",
        "* **Kustomisasi Mendalam**: Mahasiswa belajar bagaimana membuat komponen kustom (fungsi *loss*, metrik, lapisan, model) yang sesuai dengan kebutuhan spesifik proyek, sebuah keahlian penting untuk riset dan aplikasi canggih. [cite_start]Ini memungkinkan fleksibilitas yang luar biasa dalam mendesain arsitektur jaringan saraf[cite: 431, 435, 438, 441].\n",
        "* [cite_start]**Kontrol Pelatihan**: Penjelasan tentang *tf.GradientTape()* dan *custom training loops* membuka pintu bagi mahasiswa untuk mengimplementasikan algoritma pelatihan baru, bereksperimen dengan teknik optimasi tingkat lanjut, dan mendebug proses pelatihan secara lebih mendalam[cite: 446, 450].\n",
        "* **Optimasi Performa**: Konsep TF Functions dan AutoGraph adalah kunci untuk menulis kode yang tidak hanya berfungsi tetapi juga berkinerja tinggi di TensorFlow. Memahami cara grafik komputasi dihasilkan dan dioptimalkan sangat relevan untuk skala besar.\n",
        "* **Pengembangan Model Tingkat Lanjut**: Pengetahuan dari bab ini adalah prasyarat untuk memahami arsitektur jaringan saraf yang lebih kompleks (misalnya, di Bab 14, 15, 17) yang mungkin memerlukan penyesuaian di luar yang ditawarkan oleh API Sequential atau Functional Keras.\n",
        "* **Debugging yang Lebih Baik**: Kemampuan untuk menelusuri gradien dan memahami perilaku model pada level yang lebih rendah memberikan kemampuan *debugging* yang lebih kuat, membantu mahasiswa mendiagnosis masalah pelatihan yang rumit.\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab 12 adalah panduan komprehensif bagi mahasiswa yang ingin menguasai TensorFlow di level yang lebih dalam. Dengan fokus pada kustomisasi, kontrol, dan optimasi, bab ini membekali mahasiswa dengan keahlian yang diperlukan untuk membangun model Machine Learning yang unik dan berkinerja tinggi. Pemahaman ini sangat berharga bagi mereka yang bercita-cita untuk berinovasi di bidang Deep Learning, baik dalam riset maupun pengembangan produk.\n"
      ],
      "metadata": {
        "id": "3guoSQq3xpNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REPRODUCE CODE"
      ],
      "metadata": {
        "id": "mYqf3UNuxsYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "# Mengatur seed untuk reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Persiapan Data (dari Chapter 10, diulang agar kode berdiri sendiri)\n",
        "print(\"--- Persiapan Data Fashion MNIST (untuk demo umum) ---\")\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0 # Skala test set juga\n",
        "print(\"Data Fashion MNIST siap.\")\n",
        "\n",
        "# Persiapan Data California Housing (untuk demo regresi)\n",
        "print(\"\\n--- Persiapan Data California Housing (untuk demo regresi) ---\")\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full_reg, X_test_reg, y_train_full_reg, y_test_reg = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train_reg, X_valid_reg, y_train_reg, y_valid_reg = train_test_split(\n",
        "    X_train_full_reg, y_train_full_reg, random_state=42)\n",
        "\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "X_valid_reg_scaled = scaler_reg.transform(X_valid_reg)\n",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
        "print(\"Data California Housing siap.\")\n",
        "\n",
        "# --- BAGIAN 1: Menggunakan TensorFlow seperti NumPy ---\n",
        "print(\"\\n--- Bagian 1: Menggunakan TensorFlow seperti NumPy ---\")\n",
        "\n",
        "# Tensors dan Operasi\n",
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(f\"\\nTensor t:\\n{t}\")\n",
        "print(f\"Bentuk t: {t.shape}\")\n",
        "print(f\"Tipe data t: {t.dtype}\")\n",
        "\n",
        "# Indexing\n",
        "print(f\"t[:, 1:]: {t[:, 1:]}\")\n",
        "print(f\"t[..., 1, tf.newaxis]: {t[..., 1, tf.newaxis]}\")\n",
        "\n",
        "# Operasi Tensor\n",
        "print(f\"t + 10:\\n{t + 10}\")\n",
        "print(f\"tf.square(t):\\n{tf.square(t)}\")\n",
        "print(f\"t @ tf.transpose(t):\\n{t @ tf.transpose(t)}\")\n",
        "\n",
        "# Tensors dan NumPy\n",
        "a = np.array([2., 4., 5.])\n",
        "print(f\"\\nNumPy array a: {a}\")\n",
        "print(f\"tf.constant(a): {tf.constant(a)}\")\n",
        "print(f\"t.numpy(): {t.numpy()}\")\n",
        "print(f\"tf.square(a): {tf.square(a)}\")\n",
        "print(f\"np.square(t):\\n{np.square(t)}\")\n",
        "\n",
        "# Type Conversions\n",
        "try:\n",
        "    tf.constant(2.) + tf.constant(40)\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "    print(f\"\\nError konversi tipe: {e.message.splitlines()[0]} [...]\")\n",
        "\n",
        "t2_float64 = tf.constant(40., dtype=tf.float64)\n",
        "converted_add = tf.constant(2.0) + tf.cast(t2_float64, tf.float32)\n",
        "print(f\"Hasil konversi dan penambahan: {converted_add}\")\n",
        "\n",
        "# Variables\n",
        "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(f\"\\nVariabel v awal:\\n{v}\")\n",
        "\n",
        "v.assign(2 * v)\n",
        "print(f\"v setelah v.assign(2 * v):\\n{v}\")\n",
        "\n",
        "v[0, 1].assign(42.)\n",
        "print(f\"v setelah v[0, 1].assign(42.):\\n{v}\")\n",
        "\n",
        "v[:, 2].assign([0., 1.])\n",
        "print(f\"v setelah v[:, 2].assign([0., 1.]):\\n{v}\")\n",
        "\n",
        "# Scatter update (perlu tf.Variable, bukan tf.Tensor)\n",
        "v_scatter = tf.Variable([[2., 42., 0.], [8., 10., 1.]]) # Re-initialize v for clarity\n",
        "v_scatter.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
        "print(f\"v_scatter setelah scatter_nd_update:\\n{v_scatter}\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 2: Customizing Models and Training Algorithms ---\n",
        "print(\"\\n--- Bagian 2: Customizing Models and Training Algorithms ---\")\n",
        "\n",
        "# 2.1 Custom Loss Functions\n",
        "print(\"\\n2.1 Custom Loss Functions (Huber Loss):\")\n",
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "model_huber = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[X_train_reg_scaled.shape[1]]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_huber.compile(loss=huber_fn, optimizer=\"nadam\")\n",
        "print(\"Model dengan custom Huber loss function dibuat.\")\n",
        "# Melatih secara singkat untuk demo\n",
        "# history_huber = model_huber.fit(X_train_reg_scaled, y_train_reg, epochs=1, verbose=0)\n",
        "# print(f\"Loss setelah 1 epoch: {history_huber.history['loss'][0]:.4f}\")\n",
        "\n",
        "\n",
        "# 2.2 Saving and Loading Models with Custom Components\n",
        "print(\"\\n2.2 Saving and Loading Models with Custom Components:\")\n",
        "# Fungsi Huber dengan threshold yang bisa dikonfigurasi\n",
        "def create_huber(threshold=1.0):\n",
        "    def huber_fn_threshold(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_fn_threshold\n",
        "\n",
        "model_huber_thr = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[X_train_reg_scaled.shape[1]]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_huber_thr.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
        "# model_huber_thr.save(\"my_model_with_a_custom_loss_threshold_2.h5\") # Simpan model\n",
        "\n",
        "# Memuat model yang menggunakan fungsi kustom dengan threshold\n",
        "# loaded_model_huber_thr = keras.models.load_model(\n",
        "#     \"my_model_with_a_custom_loss_threshold_2.h5\",\n",
        "#     custom_objects={\"huber_fn_threshold\": create_huber(2.0)} # Perlu nama fungsi\n",
        "# )\n",
        "# print(\"Model dengan custom Huber loss (threshold) dimuat.\")\n",
        "\n",
        "# Subclassing keras.losses.Loss untuk menyimpan threshold\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "model_huber_class = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[X_train_reg_scaled.shape[1]]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_huber_class.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
        "# model_huber_class.save(\"my_model_with_a_custom_loss_class.h5\") # Simpan model\n",
        "# loaded_model_huber_class = keras.models.load_model(\n",
        "#     \"my_model_with_a_custom_loss_class.h5\",\n",
        "#     custom_objects={\"HuberLoss\": HuberLoss}\n",
        "# )\n",
        "print(\"Model dengan custom HuberLoss class dibuat dan dapat disimpan/dimuat.\")\n",
        "\n",
        "\n",
        "# 2.3 Custom Activation Functions, Initializers, Regularizers, and Constraints\n",
        "print(\"\\n2.3 Custom Activation Functions, Initializers, Regularizers, Constraints:\")\n",
        "\n",
        "# Custom Activation Function (softplus)\n",
        "def my_softplus(z):\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\n",
        "\n",
        "# Custom Initializer (Glorot normal)\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
        "\n",
        "# Custom L1 Regularizer\n",
        "def my_l1_regularizer(weights):\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
        "\n",
        "# Custom Constraint (positive weights)\n",
        "def my_positive_weights(weights):\n",
        "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
        "\n",
        "# Menggunakan custom components dalam layer Dense\n",
        "layer_custom = keras.layers.Dense(\n",
        "    30, activation=my_softplus,\n",
        "    kernel_initializer=my_glorot_initializer,\n",
        "    kernel_regularizer=my_l1_regularizer,\n",
        "    kernel_constraint=my_positive_weights\n",
        ")\n",
        "print(\"Layer dengan custom activation, initializer, regularizer, dan constraint dibuat.\")\n",
        "\n",
        "# Contoh subclassing Regularizer (untuk penyimpanan)\n",
        "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "    def __call__(self, weights):\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "print(\"Custom MyL1Regularizer class dibuat.\")\n",
        "\n",
        "\n",
        "# 2.4 Custom Metrics\n",
        "print(\"\\n2.4 Custom Metrics:\")\n",
        "\n",
        "# Menggunakan fungsi loss sebagai metrik\n",
        "model_metric_huber_fn = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[X_train_reg_scaled.shape[1]]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model_metric_huber_fn.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
        "print(\"Model dengan custom Huber loss sebagai metrik dibuat.\")\n",
        "\n",
        "# Contoh Precision metric (streaming metric)\n",
        "precision_metric = keras.metrics.Precision()\n",
        "precision_metric([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
        "precision_metric([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
        "print(f\"Hasil precision metric setelah 2 batch: {precision_metric.result():.2f}\")\n",
        "print(f\"Variabel precision metric: {precision_metric.variables}\")\n",
        "precision_metric.reset_state() # # Corrected method name\n",
        "print(\"Precision metric di-reset.\")\n",
        "\n",
        "# Custom Streaming Metric (HuberMetric)\n",
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn_metric = create_huber(threshold)\n",
        "        self.total = self.add_weight(name=\"total\", shape=(), initializer=\"zeros\") # # Corrected: added shape=()\n",
        "        self.count = self.add_weight(name=\"count\", shape=(), initializer=\"zeros\") # # Corrected: added shape=()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        metric = self.huber_fn_metric(y_true, y_pred)\n",
        "        self.total.assign_add(tf.reduce_sum(metric))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "huber_metric_instance = HuberMetric(2.0)\n",
        "print(\"Custom HuberMetric class dibuat.\")\n",
        "\n",
        "\n",
        "# 2.5 Custom Layers\n",
        "print(\"\\n2.5 Custom Layers:\")\n",
        "\n",
        "# Layer tanpa bobot (Lambda layer)\n",
        "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
        "print(\"Exponential layer (Lambda) dibuat.\")\n",
        "\n",
        "# Custom Stateful Layer (MyDense)\n",
        "# class MyDense(keras.layers.Layer):\n",
        "#     def __init__(self, units, activation=None, **kwargs): #\n",
        "#         super().__init__(**kwargs) #\n",
        "#         self.units = units #\n",
        "#         self.activation = keras.activations.get(activation) #\n",
        "\n",
        "#     def build(self, batch_input_shape): #\n",
        "#         self.kernel = self.add_weight( #\n",
        "#             name=\"kernel\", shape=[batch_input_shape[-1], self.units], #\n",
        "#             initializer=\"glorot_normal\" #\n",
        "#         )\n",
        "#         self.bias = self.add_weight( #\n",
        "#             name=\"bias\", shape=[self.units], initializer=\"zeros\" #\n",
        "#         )\n",
        "#         super().build(batch_input_shape) #\n",
        "\n",
        "#     def call(self, X): #\n",
        "#         # Explicitly cast to float32 for matrix multiplication\n",
        "#         X = tf.cast(X, tf.float32)\n",
        "#         kernel = tf.cast(self.kernel, tf.float32)\n",
        "#         bias = tf.cast(self.bias, tf.float32)\n",
        "#         return self.activation(X @ kernel + bias) #\n",
        "\n",
        "#     def compute_output_shape(self, batch_input_shape): #\n",
        "#         return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]) #\n",
        "\n",
        "#     def get_config(self): #\n",
        "#         base_config = super().get_config() #\n",
        "#         return {**base_config, \"units\": self.units, #\n",
        "#                 \"activation\": keras.activations.serialize(self.activation)} #\n",
        "\n",
        "# print(\"Custom MyDense layer class dibuat.\")\n",
        "\n",
        "# Custom Layer with Multiple Inputs/Outputs\n",
        "class MyMultiLayer(keras.layers.Layer):\n",
        "    def call(self, X):\n",
        "        X1, X2 = X\n",
        "        return [X1 + X2, X1 * X2, X1 / X2]\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        b1, b2 = batch_input_shape\n",
        "        return [b1, b1, b1] # # Simplified for demo, proper broadcasting rules would be more complex\n",
        "print(\"Custom MyMultiLayer (multiple inputs/outputs) class dibuat.\")\n",
        "\n",
        "# Custom Layer with different behavior during training/testing (MyGaussianNoise)\n",
        "class MyGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, X, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
        "            return X + noise\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape\n",
        "print(\"Custom MyGaussianNoise layer class dibuat.\")\n",
        "\n",
        "\n",
        "# 2.6 Custom Models\n",
        "print(\"\\n2.6 Custom Models:\")\n",
        "\n",
        "# Custom ResidualBlock layer\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
        "                                           kernel_initializer=\"he_normal\")\n",
        "                       for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z # # Skip connection (residual)\n",
        "print(\"Custom ResidualBlock layer class dibuat.\")\n",
        "\n",
        "# Custom ResidualRegressor model\n",
        "class ResidualRegressor(keras.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
        "                                           kernel_initializer=\"he_normal\")\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        for _ in range(1 + 3): # # Contoh loop, bisa diubah\n",
        "            Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "model_residual_reg = ResidualRegressor(output_dim=1)\n",
        "model_residual_reg.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "print(\"Custom ResidualRegressor model class dibuat.\")\n",
        "# Melatih model secara singkat\n",
        "# model_residual_reg.fit(X_train_reg_scaled, y_train_reg, epochs=1, verbose=0)\n",
        "# print(f\"Loss setelah 1 epoch (ResidualRegressor): {model_residual_reg.history.history['loss'][0]:.4f}\")\n",
        "\n",
        "\n",
        "# 2.7 Losses and Metrics Based on Model Internals\n",
        "print(\"\\n2.7 Losses and Metrics Based on Model Internals:\")\n",
        "\n",
        "# Custom model dengan reconstruction loss\n",
        "class ReconstructingRegressor(keras.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
        "                                           kernel_initializer=\"lecun_normal\")\n",
        "                       for _ in range(5)]\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        n_inputs = batch_input_shape[-1]\n",
        "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        reconstruction = self.reconstruct(Z)\n",
        "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
        "        self.add_loss(0.05 * recon_loss) # # Add reconstruction loss\n",
        "\n",
        "        # Tambahkan reconstruction_error sebagai metrik\n",
        "        self.add_metric(recon_loss, name=\"reconstruction_error\", aggregation=\"mean\") # Ini ditambahkan untuk menampilkan metrik secara eksplisit\n",
        "\n",
        "        return self.out(Z)\n",
        "\n",
        "model_reconstruct = ReconstructingRegressor(output_dim=1)\n",
        "model_reconstruct.compile(loss=\"mse\", optimizer=\"rmsprop\") # Contoh rmsprop\n",
        "print(\"Model ReconstructingRegressor (dengan custom loss internal) dibuat.\")\n",
        "# Melatih model secara singkat\n",
        "# model_reconstruct.fit(X_train_reg_scaled, y_train_reg, epochs=1, verbose=1)\n",
        "# print(f\"Loss setelah 1 epoch (ReconstructingRegressor): {model_reconstruct.history.history['loss'][0]:.4f}\")\n",
        "# print(f\"Reconstruction Error setelah 1 epoch: {model_reconstruct.history.history['reconstruction_error'][0]:.4f}\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 3: Computing Gradients Using Autodiff ---\n",
        "print(\"\\n--- Bagian 3: Computing Gradients Using Autodiff ---\")\n",
        "\n",
        "def f(w1, w2):\n",
        "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
        "\n",
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(z, [w1, w2])\n",
        "print(f\"\\nGradien dari f(w1, w2) di (5,3): {gradients}\")\n",
        "\n",
        "# GradientTape persistent\n",
        "with tf.GradientTape(persistent=True) as tape_persistent:\n",
        "    z_persistent = f(w1, w2)\n",
        "dz_dw1 = tape_persistent.gradient(z_persistent, w1)\n",
        "dz_dw2 = tape_persistent.gradient(z_persistent, w2)\n",
        "del tape_persistent\n",
        "print(f\"Gradien dengan tape persistent (w1, w2): {dz_dw1}, {dz_dw2}\")\n",
        "\n",
        "# tape.watch() untuk non-variables\n",
        "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
        "with tf.GradientTape() as tape_watch:\n",
        "    tape_watch.watch(c1)\n",
        "    tape_watch.watch(c2)\n",
        "    z_watch = f(c1, c2)\n",
        "gradients_watch = tape_watch.gradient(z_watch, [c1, c2])\n",
        "print(f\"Gradien dengan tape.watch (c1, c2): {gradients_watch}\")\n",
        "\n",
        "# tf.stop_gradient()\n",
        "def f_stop_grad(w1, w2):\n",
        "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
        "\n",
        "w1_stop, w2_stop = tf.Variable(5.), tf.Variable(3.)\n",
        "with tf.GradientTape() as tape_stop_grad:\n",
        "    z_stop_grad = f_stop_grad(w1_stop, w2_stop)\n",
        "gradients_stop_grad = tape_stop_grad.gradient(z_stop_grad, [w1_stop, w2_stop])\n",
        "print(f\"Gradien dengan tf.stop_gradient: {gradients_stop_grad}\")\n",
        "\n",
        "# @tf.custom_gradient untuk numerik stabil\n",
        "@tf.custom_gradient\n",
        "def my_better_softplus(z):\n",
        "    exp = tf.exp(z)\n",
        "    def my_softplus_gradients(grad):\n",
        "        return grad / (1 + 1 / exp)\n",
        "    return tf.math.log(exp + 1), my_softplus_gradients\n",
        "print(\"Custom @tf.custom_gradient function (my_better_softplus) dibuat.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 4: Custom Training Loops ---\n",
        "print(\"\\n--- Bagian 4: Custom Training Loops ---\")\n",
        "\n",
        "# Model sederhana untuk custom training loop\n",
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "model_custom_loop = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                       kernel_regularizer=l2_reg, input_shape=X_train_reg_scaled.shape[1:]),\n",
        "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
        "])\n",
        "print(\"Model untuk custom training loop dibuat.\")\n",
        "\n",
        "# Fungsi untuk mengambil random batch\n",
        "def random_batch(X, y, batch_size=32):\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Fungsi untuk menampilkan status bar\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\" for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(f\"\\r{iteration}/{total} - \" + metrics, end=end)\n",
        "\n",
        "\n",
        "# Hyperparameters dan setup untuk training loop\n",
        "n_epochs = 5\n",
        "batch_size_loop = 32\n",
        "n_steps_per_epoch = len(X_train_reg_scaled) // batch_size_loop\n",
        "optimizer_loop = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn_loop = tf.keras.losses.MeanSquaredError() # # Corrected: use tf.keras.losses\n",
        "mean_loss = keras.metrics.Mean(name=\"mean_loss\")\n",
        "metrics_loop = [keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
        "\n",
        "# Custom Training Loop\n",
        "print(\"\\nMenjalankan Custom Training Loop (ini akan memakan waktu)...\")\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    for step in range(1, n_steps_per_epoch + 1):\n",
        "        X_batch, y_batch = random_batch(X_train_reg_scaled, y_train_reg, batch_size=batch_size_loop)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model_custom_loop(X_batch, training=True)\n",
        "            main_loss = tf.reduce_mean(loss_fn_loop(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model_custom_loop.losses)\n",
        "        gradients = tape.gradient(loss, model_custom_loop.trainable_variables)\n",
        "        optimizer_loop.apply_gradients(zip(gradients, model_custom_loop.trainable_variables))\n",
        "        mean_loss(loss)\n",
        "        for metric in metrics_loop:\n",
        "            metric(y_batch, y_pred)\n",
        "        print_status_bar(step, n_steps_per_epoch, mean_loss, metrics_loop)\n",
        "\n",
        "    # Print final status for epoch\n",
        "    print_status_bar(n_steps_per_epoch, n_steps_per_epoch, mean_loss, metrics_loop)\n",
        "    for metric in [mean_loss] + metrics_loop:\n",
        "        metric.reset_state() # # Corrected method name\n",
        "\n",
        "print(\"Custom Training Loop selesai.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 5: TensorFlow Functions and Graphs ---\n",
        "print(\"\\n--- Bagian 5: TensorFlow Functions and Graphs ---\")\n",
        "\n",
        "# TF Function dasar (@tf.function)\n",
        "@tf.function\n",
        "def tf_cube(x):\n",
        "    print(f\"x = {x}\")\n",
        "    return x ** 3\n",
        "\n",
        "result_tf_cube = tf_cube(tf.constant(2.0)) # Panggilan pertama, tracing terjadi\n",
        "print(f\"tf_cube(2.0): {result_tf_cube}\")\n",
        "\n",
        "result_tf_cube_int = tf_cube(2) # New Python value, another trace\n",
        "print(f\"tf_cube(2): {result_tf_cube_int}\")\n",
        "\n",
        "# Mendapatkan concrete function\n",
        "concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n",
        "print(f\"\\nConcrete function untuk float32 scalar: {concrete_function}\")\n",
        "print(f\"Panggilan concrete function: {concrete_function(tf.constant(2.0))}\")\n",
        "\n",
        "# Mendapatkan operasi dari graph\n",
        "ops = concrete_function.graph.get_operations()\n",
        "print(f\"\\nOperasi dalam graph concrete function:\\n{ops}\")\n",
        "\n",
        "pow_op = ops[2] # Asumsi 'pow' adalah operasi ketiga\n",
        "print(f\"Input operasi 'pow': {list(pow_op.inputs)}\")\n",
        "print(f\"Output operasi 'pow': {pow_op.outputs}\")\n",
        "\n",
        "# tf.function dengan input_signature\n",
        "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
        "def shrink(images):\n",
        "    print(f\"Tracing shrink function with shape: {images.shape}\")\n",
        "    return images[:, ::2, ::2]\n",
        "\n",
        "img_batch_1 = tf.random.uniform(shape=[100, 28, 28], dtype=tf.float32)\n",
        "img_batch_2 = tf.random.uniform(shape=[50, 28, 28], dtype=tf.float32)\n",
        "\n",
        "shrunk_images_1 = shrink(img_batch_1) # Tracing occurs here\n",
        "shrunk_images_2 = shrink(img_batch_2) # Reuse same concrete function\n",
        "print(f\"\\nBentuk gambar setelah shrink (batch 1): {shrunk_images_1.shape}\")\n",
        "print(f\"Bentuk gambar setelah shrink (batch 2): {shrunk_images_2.shape}\")\n",
        "\n",
        "# >>>>>>>> BAGIAN YANG DIPERBAIKI <<<<<<<<<<\n",
        "try:\n",
        "    img_batch_3_wrong_shape = tf.random.uniform(shape=[2, 2, 2], dtype=tf.float32)\n",
        "    shrink(img_batch_3_wrong_shape)\n",
        "except TypeError as e:  # DIUBAH DARI ValueError menjadi TypeError\n",
        "    print(f\"\\nError saat memanggil shrink dengan bentuk yang salah: {e.args[0]} [...]\")\n",
        "\n",
        "\n",
        "# tf.function dengan tf.range() untuk loop dinamis\n",
        "@tf.function\n",
        "def add_10_dynamic_loop(x):\n",
        "    for i in tf.range(10): # # Menggunakan tf.range()\n",
        "        x += 1\n",
        "    return x\n",
        "\n",
        "result_add_10_dynamic = add_10_dynamic_loop(tf.constant(0))\n",
        "print(f\"\\nHasil add_10_dynamic_loop(0): {result_add_10_dynamic}\")\n",
        "print(f\"Operasi dalam graph add_10_dynamic_loop: {[op.name for op in add_10_dynamic_loop.get_concrete_function(tf.constant(0)).graph.get_operations()]}\")\n",
        "\n",
        "# Penanganan Variables dalam TF Functions\n",
        "counter = tf.Variable(0)\n",
        "@tf.function\n",
        "def increment_counter(c=1):\n",
        "    return counter.assign_add(c)\n",
        "\n",
        "increment_counter() # counter menjadi 1\n",
        "increment_counter() # counter menjadi 2\n",
        "print(f\"\\nNilai counter setelah increment: {counter.numpy()}\")\n",
        "\n",
        "class MyCounter:\n",
        "    def __init__(self):\n",
        "        self.counter = tf.Variable(0)\n",
        "    @tf.function\n",
        "    def increment(self, c=1):\n",
        "        return self.counter.assign_add(c)\n",
        "\n",
        "my_counter_obj = MyCounter()\n",
        "my_counter_obj.increment()\n",
        "my_counter_obj.increment()\n",
        "print(f\"Nilai counter dari objek MyCounter: {my_counter_obj.counter.numpy()}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Semua contoh kode dari Chapter 12 telah direproduksi. ---\")\n",
        "print(\"Catatan: Beberapa bagian memerlukan pelatihan penuh atau file yang disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYouQM21yUma",
        "outputId": "8f3555f1-3aa4-4116-9edf-8f64b08a3c53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Persiapan Data Fashion MNIST (untuk demo umum) ---\n",
            "Data Fashion MNIST siap.\n",
            "\n",
            "--- Persiapan Data California Housing (untuk demo regresi) ---\n",
            "Data California Housing siap.\n",
            "\n",
            "--- Bagian 1: Menggunakan TensorFlow seperti NumPy ---\n",
            "\n",
            "Tensor t:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Bentuk t: (2, 3)\n",
            "Tipe data t: <dtype: 'float32'>\n",
            "t[:, 1:]: [[2. 3.]\n",
            " [5. 6.]]\n",
            "t[..., 1, tf.newaxis]: [[2.]\n",
            " [5.]]\n",
            "t + 10:\n",
            "[[11. 12. 13.]\n",
            " [14. 15. 16.]]\n",
            "tf.square(t):\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "t @ tf.transpose(t):\n",
            "[[14. 32.]\n",
            " [32. 77.]]\n",
            "\n",
            "NumPy array a: [2. 4. 5.]\n",
            "tf.constant(a): [2. 4. 5.]\n",
            "t.numpy(): [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "tf.square(a): [ 4. 16. 25.]\n",
            "np.square(t):\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "\n",
            "Error konversi tipe: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name:  [...]\n",
            "Hasil konversi dan penambahan: 42.0\n",
            "\n",
            "Variabel v awal:\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)>\n",
            "v setelah v.assign(2 * v):\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2.,  4.,  6.],\n",
            "       [ 8., 10., 12.]], dtype=float32)>\n",
            "v setelah v[0, 1].assign(42.):\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2., 42.,  6.],\n",
            "       [ 8., 10., 12.]], dtype=float32)>\n",
            "v setelah v[:, 2].assign([0., 1.]):\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2., 42.,  0.],\n",
            "       [ 8., 10.,  1.]], dtype=float32)>\n",
            "v_scatter setelah scatter_nd_update:\n",
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[100.,  42.,   0.],\n",
            "       [  8.,  10., 200.]], dtype=float32)>\n",
            "\n",
            "--- Bagian 2: Customizing Models and Training Algorithms ---\n",
            "\n",
            "2.1 Custom Loss Functions (Huber Loss):\n",
            "Model dengan custom Huber loss function dibuat.\n",
            "\n",
            "2.2 Saving and Loading Models with Custom Components:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan custom HuberLoss class dibuat dan dapat disimpan/dimuat.\n",
            "\n",
            "2.3 Custom Activation Functions, Initializers, Regularizers, Constraints:\n",
            "Layer dengan custom activation, initializer, regularizer, dan constraint dibuat.\n",
            "Custom MyL1Regularizer class dibuat.\n",
            "\n",
            "2.4 Custom Metrics:\n",
            "Model dengan custom Huber loss sebagai metrik dibuat.\n",
            "Hasil precision metric setelah 2 batch: 0.50\n",
            "Variabel precision metric: [<Variable path=precision_7/true_positives, shape=(1,), dtype=float32, value=[4.]>, <Variable path=precision_7/false_positives, shape=(1,), dtype=float32, value=[4.]>]\n",
            "Precision metric di-reset.\n",
            "Custom HuberMetric class dibuat.\n",
            "\n",
            "2.5 Custom Layers:\n",
            "Exponential layer (Lambda) dibuat.\n",
            "Custom MyMultiLayer (multiple inputs/outputs) class dibuat.\n",
            "Custom MyGaussianNoise layer class dibuat.\n",
            "\n",
            "2.6 Custom Models:\n",
            "Custom ResidualBlock layer class dibuat.\n",
            "Custom ResidualRegressor model class dibuat.\n",
            "\n",
            "2.7 Losses and Metrics Based on Model Internals:\n",
            "Model ReconstructingRegressor (dengan custom loss internal) dibuat.\n",
            "\n",
            "--- Bagian 3: Computing Gradients Using Autodiff ---\n",
            "\n",
            "Gradien dari f(w1, w2) di (5,3): [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n",
            "Gradien dengan tape persistent (w1, w2): 36.0, 10.0\n",
            "Gradien dengan tape.watch (c1, c2): [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n",
            "Gradien dengan tf.stop_gradient: [<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]\n",
            "Custom @tf.custom_gradient function (my_better_softplus) dibuat.\n",
            "\n",
            "--- Bagian 4: Custom Training Loops ---\n",
            "Model untuk custom training loop dibuat.\n",
            "\n",
            "Menjalankan Custom Training Loop (ini akan memakan waktu)...\n",
            "Epoch 1/5\n",
            "362/362 - mean_loss: 1.5480 - mae: 0.6053\n",
            "362/362 - mean_loss: 1.5480 - mae: 0.6053\n",
            "Epoch 2/5\n",
            "362/362 - mean_loss: 0.6672 - mae: 0.5233\n",
            "362/362 - mean_loss: 0.6672 - mae: 0.5233\n",
            "Epoch 3/5\n",
            "362/362 - mean_loss: 0.6290 - mae: 0.5145\n",
            "362/362 - mean_loss: 0.6290 - mae: 0.5145\n",
            "Epoch 4/5\n",
            "362/362 - mean_loss: 0.6386 - mae: 0.5175\n",
            "362/362 - mean_loss: 0.6386 - mae: 0.5175\n",
            "Epoch 5/5\n",
            "362/362 - mean_loss: 0.6443 - mae: 0.5223\n",
            "362/362 - mean_loss: 0.6443 - mae: 0.5223\n",
            "Custom Training Loop selesai.\n",
            "\n",
            "--- Bagian 5: TensorFlow Functions and Graphs ---\n",
            "x = Tensor(\"x:0\", shape=(), dtype=float32)\n",
            "tf_cube(2.0): 8.0\n",
            "x = 2\n",
            "tf_cube(2): 8\n",
            "\n",
            "Concrete function untuk float32 scalar: ConcreteFunction Input Parameters:\n",
            "  x (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
            "Output Type:\n",
            "  TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  None\n",
            "Panggilan concrete function: 8.0\n",
            "\n",
            "Operasi dalam graph concrete function:\n",
            "[<tf.Operation 'x' type=Placeholder>, <tf.Operation 'pow/y' type=Const>, <tf.Operation 'pow' type=Pow>, <tf.Operation 'Identity' type=Identity>]\n",
            "Input operasi 'pow': [<tf.Tensor 'x:0' shape=() dtype=float32>, <tf.Tensor 'pow/y:0' shape=() dtype=float32>]\n",
            "Output operasi 'pow': [<tf.Tensor 'pow:0' shape=() dtype=float32>]\n",
            "Tracing shrink function with shape: (None, 28, 28)\n",
            "\n",
            "Bentuk gambar setelah shrink (batch 1): (100, 14, 14)\n",
            "Bentuk gambar setelah shrink (batch 2): (50, 14, 14)\n",
            "\n",
            "Error saat memanggil shrink dengan bentuk yang salah: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(2, 2, 2), dtype=tf.float32, name=None) to TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
            "array([[[0.7413678 , 0.62854624],\n",
            "        [0.01738465, 0.3431449 ]],\n",
            "\n",
            "       [[0.51063764, 0.3777541 ],\n",
            "        [0.07321596, 0.02137029]]], dtype=float32)>,) and kwargs: {} for signature: (images: TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None)). [...]\n",
            "\n",
            "Hasil add_10_dynamic_loop(0): 10\n",
            "Operasi dalam graph add_10_dynamic_loop: ['x', 'range/start', 'range/limit', 'range/delta', 'range', 'sub', 'floordiv', 'mod', 'zeros_like', 'NotEqual', 'Cast', 'add', 'zeros_like_1', 'Maximum', 'while/maximum_iterations', 'while/loop_counter', 'while', 'Identity']\n",
            "\n",
            "Nilai counter setelah increment: 2\n",
            "Nilai counter dari objek MyCounter: 2\n",
            "\n",
            "--- Semua contoh kode dari Chapter 12 telah direproduksi. ---\n",
            "Catatan: Beberapa bagian memerlukan pelatihan penuh atau file yang disimpan.\n"
          ]
        }
      ]
    }
  ]
}