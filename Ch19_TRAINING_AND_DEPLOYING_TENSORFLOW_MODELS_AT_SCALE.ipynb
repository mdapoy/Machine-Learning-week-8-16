{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMEpqMhJ12E7M01xOSkY0M3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch19_TRAINING_AND_DEPLOYING_TENSORFLOW_MODELS_AT_SCALE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAPORAN ANALISIS BUKU\n",
        "**Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)**\n",
        "\n",
        "**BAB 19: TRAINING AND DEPLOYING TENSORFLOW MODELS AT SCALE**\n",
        "\n",
        "## 1. Pendahuluan\n",
        "Bab 19 membahas langkah-langkah penting setelah model *Machine Learning* dilatih: penyebaran (*deployment*) dan pelatihan berskala besar. Model yang telah dilatih perlu ditempatkan di lingkungan produksi untuk digunakan secara langsung. Seringkali, ini melibatkan penyediaan model sebagai layanan web yang dapat diakses melalui API (seperti REST atau gRPC). [cite_start]Pembaruan model secara berkala dengan data baru, pengelolaan versi model, transisi yang mulus antar versi, dan kemampuan untuk melakukan eksperimen A/B adalah aspek krusial dari *deployment* ini. [cite_start]Jika produk berhasil, layanan prediksi harus mampu menskalakan untuk mendukung volume *queries per second* (QPS) yang tinggi.\n",
        "\n",
        "Pelatihan model berskala besar juga menjadi tantangan, terutama dengan volume data yang masif dan model yang intensif komputasi. Waktu pelatihan yang lama dapat menghambat eksperimen ide baru dan adaptasi cepat terhadap perubahan data. [cite_start]Solusinya mencakup penggunaan *hardware accelerator* seperti GPU atau TPU, dan mendistribusikan komputasi ke banyak mesin, masing-masing dilengkapi dengan banyak *accelerator*. [cite_start]TensorFlow menyediakan *Distribution Strategies API* untuk mempermudah hal ini.\n",
        "\n",
        "## 2. Menyediakan Model TensorFlow (Serving a TensorFlow Model)\n",
        "Setelah model TensorFlow dilatih, penggunaannya dalam kode Python cukup mudah (misalnya, memanggil metode `predict()` pada model `tf.keras`). [cite_start]Namun, seiring berkembangnya infrastruktur, lebih baik menyediakan model sebagai layanan terpisah yang hanya bertugas membuat prediksi.\n",
        "\n",
        "* **Keuntungan Menggunakan Layanan Model**:\n",
        "    * **Dekopling**: Model terpisah dari infrastruktur lain.\n",
        "    * **Pembaruan Mudah**: Memungkinkan transisi dan pembaruan versi model yang mudah.\n",
        "    * **Skalabilitas**: Layanan dapat diskalakan secara independen untuk menangani beban tinggi.\n",
        "    * **Eksperimen A/B**: Memungkinkan pengujian beberapa versi model secara paralel.\n",
        "    * **Konsistensi**: Memastikan semua komponen perangkat lunak bergantung pada versi model yang sama.\n",
        "    * [cite_start]**Pengujian dan Pengembangan yang Lebih Sederhana**.\n",
        "\n",
        "### 2.1. Menggunakan TensorFlow Serving (TF Serving)\n",
        "[cite_start]TF Serving adalah *model server* yang sangat efisien dan teruji yang ditulis dalam C++.\n",
        "\n",
        "* **Fitur Utama**:\n",
        "    * Mampu menangani beban tinggi.\n",
        "    * Melayani beberapa versi model secara bersamaan.\n",
        "    * Memantau repositori model untuk secara otomatis menyebarkan versi terbaru.\n",
        "    * [cite_start]Mendukung API gRPC (port 8500 secara default) dan REST (port 8501 secara default).\n",
        "    * [cite_start]Secara otomatis melakukan *batching* permintaan untuk meningkatkan kinerja pada GPU.\n",
        "\n",
        "#### 2.1.1. Mengekspor SavedModels\n",
        "[cite_start]Model TensorFlow diekspor ke format *SavedModel* menggunakan `tf.saved_model.save()`. [cite_start]Ini menyimpan grafik komputasi model dan bobotnya.\n",
        "\n",
        "* **Pentingnya Pra-pemrosesan dalam Model**: Sebaiknya sertakan semua lapisan pra-pemrosesan dalam model yang diekspor agar dapat menerima data dalam bentuk mentahnya di produksi. [cite_start]Ini menyederhanakan pembaruan dan mengurangi risiko ketidakcocokan antara model dan langkah pra-pemrosesan yang diperlukan.\n",
        "* [cite_start]**Keterbatasan SavedModel**: Hanya dapat digunakan dengan model yang secara eksklusif berbasis operasi TensorFlow, tidak termasuk kode Python (`tf.py_function()`) atau model `tf.keras` dinamis.\n",
        "* [cite_start]**Struktur Direktori SavedModel**: Terdiri dari direktori versi (misalnya, `0001`), yang berisi `saved_model.pb` (grafik komputasi sebagai *protocol buffer* serial), dan subdirektori `variables` (nilai variabel model).\n",
        "* [cite_start]**Inspeksi SavedModel**: Alat baris perintah `saved_model_cli` dapat digunakan untuk memeriksa *SavedModel* dan detail tanda tangannya (input/output).\n",
        "\n",
        "#### 2.1.2. Menginstal TensorFlow Serving\n",
        "[cite_start]Instalasi TF Serving dapat dilakukan melalui citra Docker, manajer paket sistem, atau dari sumber. [cite_start]Docker sangat direkomendasikan karena kemudahan instalasi, isolasi sistem, dan kinerja tinggi.\n",
        "\n",
        "#### 2.1.3. Mengirim Kueri TF Serving melalui REST API\n",
        "Permintaan ke TF Serving melalui REST API harus dalam format JSON, termasuk nama tanda tangan fungsi dan data input. [cite_start]Meskipun sederhana dan banyak didukung klien, ini kurang efisien untuk data besar karena format berbasis teks dan verbositasnya.\n",
        "\n",
        "#### 2.1.4. Mengirim Kueri TF Serving melalui gRPC API\n",
        "gRPC API lebih efisien karena menggunakan *protocol buffer* serial sebagai input dan output, yang merupakan format biner yang ringkas. [cite_start]Ini jauh lebih baik untuk transfer data dalam jumlah besar.\n",
        "\n",
        "#### 2.1.5. Menyebarkan Versi Model Baru\n",
        "TF Serving secara berkala memeriksa versi model baru. Ketika versi baru terdeteksi, ia menangani transisi secara mulus: permintaan yang tertunda dijawab dengan versi lama, sementara permintaan baru dilayani oleh versi baru. Setelah semua permintaan tertunda selesai, versi lama dibongkar. [cite_start]Ini memastikan transisi yang lancar.\n",
        "\n",
        "* [cite_start]**Model Warmup**: TF Serving dapat dikonfigurasi untuk mengeksekusi model pada *example instances* sebelum melayani permintaan baru, memastikan semuanya dimuat dengan benar dan menghindari waktu respons yang lama untuk permintaan pertama.\n",
        "* [cite_start]**Skalabilitas**: Untuk menangani QPS yang tinggi, TF Serving dapat disebarkan di beberapa server dengan *load balancing*. [cite_start]Kubernetes adalah alat orkestrasi kontainer yang dapat digunakan untuk mengelola banyak kontainer TF Serving di banyak server.\n",
        "\n",
        "## 3. Membuat Layanan Prediksi di Google Cloud AI Platform (GCP AI Platform)\n",
        "GCP AI Platform adalah layanan *cloud* yang mengelola *deployment* model TensorFlow, menawarkan fitur seperti skalabilitas otomatis, pemantauan, dan penggunaan TPU.\n",
        "\n",
        "* **Langkah-langkah Penyiapan**:\n",
        "    1.  [cite_start]Masuk ke konsol GCP dan terima persyaratan layanan.\n",
        "    2.  [cite_start]Verifikasi atau aktifkan akun penagihan.\n",
        "    3.  [cite_start]Buat atau pilih proyek GCP.\n",
        "    4.  [cite_start]Buat *bucket* di Google Cloud Storage (GCS) untuk menyimpan *SavedModels* dan data.\n",
        "    5.  [cite_start]Unggah folder *SavedModel* ke *bucket* GCS.\n",
        "    6.  [cite_start]Konfigurasi AI Platform: buat model baru dan versinya, tentukan jalur model GCS, versi *runtime* TensorFlow, tipe mesin, dan penskalaan (otomatis direkomendasikan).\n",
        "* [cite_start]**Skalabilitas Otomatis**: AI Platform akan meluncurkan lebih banyak kontainer TF Serving saat QPS meningkat, dan menghentikannya saat QPS menurun, mengaitkan biaya langsung dengan penggunaan.\n",
        "* [cite_start]**Autentikasi**: Autentikasi klien untuk layanan prediksi harus menggunakan *service account* dengan hak akses yang sangat terbatas (misalnya, peran *ML Engine Developer*) untuk keamanan. [cite_start]Kunci pribadi *service account* harus dijaga kerahasiaannya.\n",
        "\n",
        "## 4. Menyebarkan Model ke Perangkat Seluler atau Tertanam (Mobile or Embedded Device)\n",
        "Untuk menyebarkan model ke perangkat seluler atau tertanam, model harus ringan dan efisien untuk menghindari masalah kinerja. [cite_start]Pustaka TFLite menyediakan alat untuk tujuan ini.\n",
        "\n",
        "* **Tujuan TFLite**:\n",
        "    * Mengurangi ukuran model.\n",
        "    * Mengurangi jumlah komputasi per prediksi (mengurangi latensi, konsumsi baterai, dan panas).\n",
        "    * Mengadaptasi model ke batasan perangkat spesifik.\n",
        "* **Konversi Model**: *Converter* TFLite dapat mengompres *SavedModel* ke format yang lebih ringan berdasarkan *FlatBuffers*. [cite_start]Ini mengurangi waktu *loading* dan jejak memori karena *FlatBuffers* dapat dimuat langsung ke RAM.\n",
        "* [cite_start]**Optimasi Model**: *Converter* juga mengoptimalkan model dengan memangkas operasi yang tidak perlu dan mengoptimalkan serta menggabungkan komputasi (misalnya, lapisan *Batch Normalization* dilipat ke dalam operasi lapisan sebelumnya).\n",
        "* [cite_start]**Kuantisasi**: Mengurangi ukuran model lebih lanjut dengan mengkuantisasi bobot model ke bilangan bulat 8-bit *fixed-point*, menghasilkan pengurangan ukuran empat kali lipat dibandingkan *float* 32-bit.\n",
        "    * **Post-training Quantization**: Mengkuantisasi bobot setelah pelatihan, menggunakan teknik kuantisasi simetris dasar. [cite_start]Namun, saat *runtime*, bobot dikonversi kembali ke *float*.\n",
        "    * **Kuantisasi Penuh**: Mengkuantisasi bobot dan aktivasi sehingga komputasi dapat dilakukan sepenuhnya dengan bilangan bulat, mengurangi latensi dan konsumsi daya. [cite_start]Ini memerlukan langkah kalibrasi untuk menemukan nilai absolut maksimum aktivasi.\n",
        "    * [cite_start]**Quantization-aware Training**: Menambahkan operasi kuantisasi palsu ke model selama pelatihan agar model belajar mengabaikan *noise* kuantisasi, membuat bobot akhir lebih kuat terhadap kuantisasi.\n",
        "\n",
        "## 5. TensorFlow di Browser\n",
        "[cite_start]Model dapat dijalankan langsung di *browser* pengguna menggunakan pustaka TensorFlow.js JavaScript.\n",
        "\n",
        "* **Manfaat**:\n",
        "    * **Konektivitas Terputus**: Berguna saat konektivitas pengguna terputus-putus atau lambat.\n",
        "    * **Latensi Rendah**: Prediksi secepat mungkin karena tidak perlu kueri server.\n",
        "    * [cite_start]**Privasi Pengguna**: Prediksi dilakukan di sisi klien, menjaga data pribadi pengguna tetap lokal.\n",
        "* [cite_start]**tensorflowjs_converter**: Alat untuk mengonversi *SavedModel* atau file model Keras ke format TensorFlow.js Layers, yang dioptimalkan untuk pengunduhan web.\n",
        "\n",
        "## 6. Menggunakan GPU untuk Mempercepat Komputasi\n",
        "[cite_start]Pelatihan jaringan saraf besar pada satu CPU bisa memakan waktu berhari-hari atau berminggu-minggu. GPU dapat secara signifikan mempercepat pelatihan.\n",
        "\n",
        "* [cite_start]**Mendapatkan GPU Sendiri**: Memerlukan pembelian kartu GPU Nvidia dengan CUDA Compute Capability 3.5+ dan instalasi *driver* Nvidia, CUDA, serta cuDNN (pustaka yang dipercepat GPU untuk DNNs).\n",
        "* **Menggunakan Mesin Virtual yang Dilengkapi GPU**: Platform *cloud* besar menawarkan VM GPU yang sudah dikonfigurasi sebelumnya dengan semua *driver* dan pustaka yang diperlukan (termasuk TensorFlow). Google Cloud AI Platform menawarkan VM *Deep Learning* dan *Colaboratory* (Colab) untuk akses GPU gratis.\n",
        "* **Mengelola RAM GPU**: TensorFlow secara default mengambil semua RAM GPU yang tersedia pada komputasi pertama untuk membatasi fragmentasi.\n",
        "    * **Pembagian GPU antar Proses**: Gunakan variabel lingkungan `CUDA_VISIBLE_DEVICES` untuk menetapkan GPU tertentu ke proses individual.\n",
        "    * [cite_start]**Membatasi Penggunaan RAM GPU**: Konfigurasi TensorFlow untuk mengambil hanya sejumlah RAM GPU tertentu menggunakan `tf.config.experimental.set_virtual_device_configuration`.\n",
        "    * **Pertumbuhan Memori Sesuai Permintaan**: TensorFlow dapat mengambil memori hanya saat dibutuhkan menggunakan `tf.config.experimental.set_memory_growth`.\n",
        "    * [cite_start]**Virtual GPU**: Membagi satu GPU fisik menjadi dua atau lebih GPU virtual.\n",
        "* **Menempatkan Operasi dan Variabel pada Perangkat**: `tf.keras` dan `tf.data` biasanya menempatkan operasi dan variabel secara optimal. [cite_start]Namun, penempatan manual dapat dilakukan menggunakan konteks `tf.device()` untuk kontrol lebih lanjut (misalnya, pra-pemrosesan di CPU, komputasi NN di GPU).\n",
        "* **Eksekusi Paralel Lintas Beberapa Perangkat**: Ketika TensorFlow menjalankan `TF Function`, ia menganalisis grafiknya untuk mengeksekusi operasi secara paralel pada perangkat yang berbeda. Operasi pada CPU dijalankan di *thread pool* `inter-op`, dan operasi yang memiliki kernel *multithreaded* menggunakan *thread pool* `intra-op`. Operasi GPU dijalankan secara berurutan, tetapi kernel GPU yang *multithreaded* memanfaatkan banyak *thread* GPU secara paralel.\n",
        "\n",
        "## 7. Melatih Model Lintas Beberapa Perangkat (Training Models Across Multiple Devices)\n",
        "Ada dua pendekatan utama untuk melatih satu model di beberapa perangkat:\n",
        "\n",
        "### 7.1. Model Parallelism\n",
        "* **Konsep**: Membagi model menjadi bagian-bagian terpisah dan menjalankan setiap bagian pada perangkat yang berbeda.\n",
        "* **Keterbatasan**: Sulit diterapkan secara efisien untuk jaringan yang *fully connected* karena tingginya komunikasi lintas perangkat. Lebih cocok untuk arsitektur dengan koneksi parsial seperti CNN.\n",
        "\n",
        "### 7.2. Data Parallelism\n",
        "* **Konsep**: Mereplikasi model pada setiap perangkat dan menjalankan setiap langkah pelatihan secara bersamaan pada semua replika, menggunakan *mini-batch* yang berbeda untuk setiap replika. [cite_start]Gradien yang dihitung oleh setiap replika kemudian dirata-ratakan dan hasilnya digunakan untuk memperbarui parameter model.\n",
        "\n",
        "#### 7.2.1. Data Parallelism Menggunakan Mirrored Strategy\n",
        "* **Konsep**: Semua parameter model dicerminkan sepenuhnya di semua GPU, dan pembaruan parameter yang sama persis selalu diterapkan pada setiap GPU. Semua replika tetap identik sempurna.\n",
        "* [cite_start]**Efisiensi**: Sangat efisien, terutama pada satu mesin.\n",
        "* **Mekanisme**: Menggunakan algoritma *AllReduce* (misalnya, NVIDIA Collective Communications Library - NCCL) untuk menghitung rata-rata gradien secara efisien dan mendistribusikan hasilnya ke semua GPU.\n",
        "* [cite_start]**Implementasi Keras**: Gunakan `tf.distribute.MirroredStrategy()` dan bungkus pembuatan serta kompilasi model di dalam *scope* strategi ini.\n",
        "\n",
        "#### 7.2.2. Data Parallelism dengan Parameter Terpusat\n",
        "* [cite_start]**Konsep**: Parameter model disimpan di luar perangkat GPU yang melakukan komputasi (disebut *workers*), misalnya pada CPU atau server parameter khusus.\n",
        "\n",
        "* **Pembaruan Sinkron**: *Aggregator* menunggu hingga semua gradien tersedia sebelum menghitung rata-rata gradien dan meneruskannya ke *optimizer*. Semua perangkat harus menunggu yang paling lambat.\n",
        "* **Pembaruan Asinkron**: Setiap replika memperbarui parameter model segera setelah selesai menghitung gradien. Tidak ada agregasi atau sinkronisasi. Replika bekerja secara independen. [cite_start]Ini dapat menyebabkan *stale gradients* (gradien yang sudah ketinggalan zaman) yang dapat memperlambat konvergensi atau menyebabkan divergensi.\n",
        "\n",
        "* **Saturasi Bandwidth**: Baik *mirrored strategy* maupun parameter terpusat melibatkan komunikasi data yang signifikan. [cite_start]Ada titik di mana menambahkan lebih banyak GPU tidak lagi meningkatkan kinerja karena waktu yang dihabiskan untuk memindahkan data masuk dan keluar dari RAM GPU (dan melalui jaringan dalam pengaturan terdistribusi) melebihi peningkatan kecepatan komputasi.\n",
        "    * Lebih parah untuk model *dense* besar.\n",
        "    * [cite_start]Kurang parah untuk model *sparse* besar, di mana gradien sebagian besar nol dan dapat dikomunikasikan secara efisien.\n",
        "    * [cite_start]Solusi: Menggunakan beberapa GPU yang kuat daripada banyak GPU lemah, mengelompokkan GPU pada server yang saling terhubung dengan baik, dan mengurangi presisi *float* (misalnya, dari 32 bit ke 16 bit).\n",
        "\n",
        "### 7.3. Melatih pada Skala Menggunakan Distribution Strategies API\n",
        "TensorFlow menyediakan *Distribution Strategies API* untuk mempermudah pelatihan model berskala besar.\n",
        "\n",
        "* **MultiWorkerMirroredStrategy**: Mirrored data parallelism di banyak server. Model direplikasi di semua server dan perangkat, dan setiap replika mendapatkan *batch* data yang berbeda. Rata-rata gradien dihitung dan dibagikan menggunakan implementasi *AllReduce* terdistribusi (NCCL secara default). Ini adalah strategi yang direkomendasikan secara umum.\n",
        "* **ParameterServerStrategy**: Asinkron data parallelism dengan *parameter server*. [cite_start]Model direplikasi di semua perangkat pada semua *worker*, dan parameter dipecah di seluruh *parameter server*.\n",
        "* **TPUStrategy**: Untuk menggunakan TPU di Google Cloud.\n",
        "\n",
        "## 8. Menjalankan Pekerjaan Pelatihan Besar di Google Cloud AI Platform\n",
        "*Google Cloud AI Platform* memungkinkan *deployment* pekerjaan pelatihan menggunakan kode pelatihan yang sama seperti pada *cluster* TF Anda sendiri, sambil mengelola penyediaan dan konfigurasi VM GPU.\n",
        "\n",
        "* [cite_start]**gcloud command-line tool**: Digunakan untuk memulai pekerjaan pelatihan, menentukan wilayah, *scale tier* (misalnya, `PREMIUM_1` untuk banyak *workers* dan *parameter servers*), versi *runtime*, jalur kode, dll.\n",
        "* **Google Cloud Storage (GCS)**: Digunakan untuk data pelatihan.\n",
        "* [cite_start]**Pemantauan**: Konsol GCP menyediakan grafik pemanfaatan CPU, GPU, dan RAM, serta *log* detail menggunakan *Stackdriver*.\n",
        "\n",
        "### 8.1. Penyetelan Hiperparameter Black Box di AI Platform\n",
        "[cite_start]AI Platform menyediakan layanan penyetelan hiperparameter *Bayesian optimization* yang kuat bernama *Google Vizier*.\n",
        "\n",
        "* [cite_start]**Konfigurasi**: Melalui file konfigurasi YAML, menentukan tujuan (misalnya, `MAXIMIZE`), metrik hiperparameter (misalnya, `accuracy`), jumlah *trial* maksimum, *trial* paralel maksimum, dan parameter yang akan disetel dengan rentang dan *scale type* (misalnya, `UNIT_LINEAR_SCALE`, `UNIT_LOG_SCALE`).\n",
        "* **Komunikasi Metrik**: Kode pelatihan berkomunikasi metrik kembali ke AI Platform dengan menggunakan *callback* `TensorBoard()` (yang menulis *event file* yang dipantau oleh AI Platform).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UFu9be5N7SZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REPRODUCE CODE"
      ],
      "metadata": {
        "id": "AGeYz7NR7Tb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting SavedModels"
      ],
      "metadata": {
        "id": "p7fE6O_VAkut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Asumsikan model sudah dibuat dan dilatih\n",
        "# model = keras.models.Sequential([...])\n",
        "# model.compile([...])\n",
        "# history = model.fit([...])\n",
        "\n",
        "model_version = \"0001\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "# tf.saved_model.save(model, model_path) # Baris ini akan menyimpan model"
      ],
      "metadata": {
        "id": "4dGh61mb7ZZ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode berikut menunjukkan cara memuat kembali SavedModel dan menggunakannya untuk prediksi, serta cara menggunakan saved_model_cli dari terminal."
      ],
      "metadata": {
        "id": "R--kJgMaAmy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat model yang tersimpan\n",
        "# saved_model = tf.saved_model.load(model_path)\n",
        "# y_pred = saved_model(tf.constant(X_new, dtype=tf.float32))\n",
        "\n",
        "# Memuat model sebagai model Keras\n",
        "# model = keras.models.load_model(model_path)\n",
        "# y_pred = model.predict(tf.constant(X_new, dtype=tf.float32))\n",
        "\n",
        "# Menyimpan data tes untuk digunakan dengan CLI\n",
        "import numpy as np\n",
        "# X_new = np.random.rand(10, 28, 28) # Contoh data baru\n",
        "# np.save(\"my_mnist_tests.npy\", X_new)"
      ],
      "metadata": {
        "id": "gyqQetozAo4g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perintah Terminal untuk saved_model_cli:"
      ],
      "metadata": {
        "id": "6U4M2zubAsAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengekspor path ke variabel lingkungan (opsional, untuk kenyamanan)\n",
        "# export ML_PATH=\"$HOME/ml\"\n",
        "# cd $ML_PATH\n",
        "\n",
        "# Menampilkan informasi SavedModel\n",
        "# saved_model_cli show --dir my_mnist_model/0001 --all\n",
        "\n",
        "# Menjalankan prediksi menggunakan CLI\n",
        "# saved_model_cli run --dir my_mnist_model/0001 --tag_set serve \\\n",
        "#  --signature_def serving_default \\\n",
        "#  --inputs flatten_input=my_mnist_tests.npy"
      ],
      "metadata": {
        "id": "iS0kyfOFAsZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing TensorFlow Serving"
      ],
      "metadata": {
        "id": "sCIH6H8uAvQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengunduh image Docker resmi TF Serving\n",
        "# docker pull tensorflow/serving\n",
        "\n",
        "# Menjalankan container Docker untuk TF Serving\n",
        "# docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
        "#  -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
        "#  -e MODEL_NAME=my_mnist_model \\\n",
        "#  tensorflow/serving"
      ],
      "metadata": {
        "id": "tiZ1o8AvAtc_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying TF Serving through the REST API"
      ],
      "metadata": {
        "id": "6OeI9FuJAzvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Asumsikan X_new adalah data input (misalnya, dari data tes MNIST)\n",
        "# X_new = [...]\n",
        "\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status() # raise an exception in case of error\n",
        "response = response.json()\n",
        "\n",
        "y_proba = np.array(response[\"predictions\"])\n",
        "# print(y_proba.round(2))"
      ],
      "metadata": {
        "id": "Y2fa18hwA0dz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying TF Serving through the gRPC API"
      ],
      "metadata": {
        "id": "_AjTNRtNA8Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "# Membuat permintaan gRPC\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "# Asumsikan 'model' adalah model Keras yang sudah dimuat\n",
        "# dan X_new adalah data input\n",
        "# input_name = model.input_names[0]\n",
        "# request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n",
        "\n",
        "# Mengirim permintaan\n",
        "# channel = grpc.insecure_channel('localhost:8500')\n",
        "# predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "# response = predict_service.Predict(request, timeout=10.0)\n",
        "\n",
        "# Mengonversi respons kembali ke tensor\n",
        "# output_name = model.output_names[0]\n",
        "# outputs_proto = response.outputs[output_name]\n",
        "# y_proba = tf.make_ndarray(outputs_proto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "c0R2xW3WA9B3",
        "outputId": "24d54375-3d54-4d14-a196-2d50b83b3fe1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_serving'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-448e8c6d8632>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprediction_service_pb2_grpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Membuat permintaan gRPC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_serving'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "90eb8e38",
        "outputId": "9e5275a2-6804-4b19-c317-4aac9097b9ab"
      },
      "source": [
        "!pip install tensorflow-serving-api grpcio"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-serving-api\n",
            "  Downloading tensorflow_serving_api-2.19.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-serving-api) (4.25.6)\n",
            "Collecting tensorflow<3,>=2.19.0 (from tensorflow-serving-api)\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (1.17.2)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow<3,>=2.19.0->tensorflow-serving-api)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.12.1)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<3,>=2.19.0->tensorflow-serving-api)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<3,>=2.19.0->tensorflow-serving-api) (0.1.2)\n",
            "Downloading tensorflow_serving_api-2.19.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow, tensorflow-serving-api\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow-serving-api-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "0f2abf2a0f5240758cce7b1c8463b2cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying a new model version"
      ],
      "metadata": {
        "id": "ai0K_ADaBajj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumsikan model baru telah dilatih\n",
        "# model = keras.models.Sequential([...])\n",
        "# model.compile([...])\n",
        "# history = model.fit([...])\n",
        "\n",
        "model_version = \"0002\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "# tf.saved_model.save(model, model_path)"
      ],
      "metadata": {
        "id": "lM2eqjivBbds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Prediction Service (on GCP)"
      ],
      "metadata": {
        "id": "3P0Ct4caBcoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import googleapiclient.discovery\n",
        "\n",
        "# Mengatur environment variable untuk autentikasi\n",
        "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\"\n",
        "\n",
        "project_id = \"your-gcp-project-id\" # Ganti dengan project ID Anda\n",
        "model_id = \"my_mnist_model\"\n",
        "model_path = \"projects/{}/models/{}\".format(project_id, model_id)\n",
        "ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()\n",
        "\n",
        "def predict(X):\n",
        "    # Asumsikan 'output_name' sudah didefinisikan sebelumnya\n",
        "    output_name = \"dense_1\" # Contoh dari buku, sesuaikan jika perlu\n",
        "\n",
        "    input_data_json = {\"signature_name\": \"serving_default\",\n",
        "                       \"instances\": X.tolist()}\n",
        "    request = ml_resource.predict(name=model_path, body=input_data_json)\n",
        "    response = request.execute()\n",
        "    if \"error\" in response:\n",
        "        raise RuntimeError(response[\"error\"])\n",
        "    return np.array([pred[output_name] for pred in response[\"predictions\"]])\n",
        "\n",
        "# Y_probas = predict(X_new)\n",
        "# print(np.round(Y_probas, 2))"
      ],
      "metadata": {
        "id": "mwNpDnxCBeVL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying a Model to a Mobile or Embedded Device"
      ],
      "metadata": {
        "id": "q99Nc_7tBgmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan model ke path\n",
        "# saved_model_path = \"my_mnist_model/0001\"\n",
        "\n",
        "# Mengonversi model ke TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"converted_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Menggunakan kuantisasi post-training\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "# tflite_model_quantized = converter.convert()"
      ],
      "metadata": {
        "id": "b7-hJb7fBieJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPUs to Speed Up Computations"
      ],
      "metadata": {
        "id": "a9DdVq84Bjf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.test.is_gpu_available())\n",
        "print(tf.test.gpu_device_name())\n",
        "print(tf.config.experimental.list_physical_devices(device_type='GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE7yzGSrBmAp",
        "outputId": "fefd81f7-5072-4e87-a47a-fa5855e969cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-9d8a43eea128>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "/device:GPU:0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengatur pembatasan memori GPU (Memory Growth):"
      ],
      "metadata": {
        "id": "sH1vZASTBu3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Opsi 1: Mengatur batas memori virtual\n",
        "# for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "#     tf.config.experimental.set_virtual_device_configuration(\n",
        "#         gpu,\n",
        "#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
        "\n",
        "# Opsi 2: Mengizinkan memory growth\n",
        "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "b74mvy19Bvtq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menempatkan operasi pada device tertentu:"
      ],
      "metadata": {
        "id": "6HREd201By3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "    c = tf.Variable(42.0)\n",
        "print(c.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_vgRqMzB0xy",
        "outputId": "77fd3d1d-a154-45f8-a69c-22be7046809a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menggunakan MirroredStrategy (pada satu mesin dengan beberapa GPU):"
      ],
      "metadata": {
        "id": "b8Bk4cXNB4Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "    mirrored_model = keras.models.Sequential([\n",
        "        # ... lapisan-lapisan model Anda\n",
        "    ])\n",
        "    mirrored_model.compile(loss=\"...\", optimizer=\"...\")\n",
        "\n",
        "batch_size = 100 # harus bisa dibagi dengan jumlah replika\n",
        "# history = mirrored_model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Memuat model dalam scope distribusi\n",
        "# with distribution.scope():\n",
        "#    mirrored_model = keras.models.load_model(\"my_mnist_model.h5\")"
      ],
      "metadata": {
        "id": "QXcXe4bnB5PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menggunakan MultiWorkerMirroredStrategy (pada beberapa server):"
      ],
      "metadata": {
        "id": "LNIuWA-5B6ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh TF_CONFIG untuk worker pertama\n",
        "cluster_spec = {\n",
        "    \"worker\": [\n",
        "        \"machine-a.example.com:2222\",\n",
        "        \"machine-b.example.com:2222\"\n",
        "    ],\n",
        "    \"ps\": [\"machine-a.example.com:2221\"] # Parameter Server\n",
        "}\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": cluster_spec,\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "})\n",
        "\n",
        "# Kode training (dijalankan di setiap worker)\n",
        "# distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "# with distribution.scope():\n",
        "#    mirrored_model = keras.models.Sequential([...])\n",
        "#    mirrored_model.compile([...])\n",
        "# batch_size = 100\n",
        "# history = mirrored_model.fit(X_train, y_train, epochs=10)"
      ],
      "metadata": {
        "id": "IsiKtXS0B89E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Large Training Jobs on Google Cloud AI Platform"
      ],
      "metadata": {
        "id": "x69561OfB_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gcloud ai-platform jobs submit training my_job_20190531_164700 \\\n",
        "#  --region asia-southeast1 \\\n",
        "#  --scale-tier PREMIUM_1 \\\n",
        "#  --runtime-version 2.0 \\\n",
        "#  --python-version 3.5 \\\n",
        "#  --package-path /my_project/src/trainer \\\n",
        "#  --module-name trainer.task \\\n",
        "#  --staging-bucket gs://my-staging-bucket \\\n",
        "#  --job-dir gs://my-mnist-model-bucket/trained_model \\\n",
        "#  -- \\\n",
        "#  --my-extra-argument1 foo --my-extra-argument2 bar"
      ],
      "metadata": {
        "id": "AK4G_wFYCAfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}