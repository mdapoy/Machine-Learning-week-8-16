{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laporan Analisis Bab 11: Training Deep Neural Networks\n",
        "\n",
        "## Pendahuluan\n",
        "[cite_start]Bab 11 membahas tantangan-tantangan yang muncul saat melatih Jaringan Saraf Tiruan (DNN) yang sangat dalam (*deep*)[cite: 331]. [cite_start]Bab sebelumnya telah memperkenalkan ANN dangkal dengan beberapa *hidden layer*, namun untuk masalah yang lebih kompleks (misalnya, deteksi objek pada gambar resolusi tinggi), DNN yang jauh lebih dalam diperlukan[cite: 331]. [cite_start]Pelatihan DNN yang dalam menghadapi berbagai kesulitan, termasuk masalah *vanishing/exploding gradients*, kurangnya data pelatihan berlabel, waktu pelatihan yang lambat, dan risiko *overfitting* yang tinggi karena jutaan parameter[cite: 331]. [cite_start]Bab ini menyajikan berbagai teknik dan strategi untuk mengatasi masalah-masalah tersebut[cite: 331].\n",
        "\n",
        "## Ringkasan Isi Bab\n",
        "\n",
        "Bab 11 menguraikan beberapa konsep kunci terkait Pelatihan DNN yang Dalam:\n",
        "\n",
        "1.  **The Vanishing/Exploding Gradients Problems (Masalah Vanishing/Exploding Gradients)**:\n",
        "    * [cite_start]**Vanishing Gradients**: Gradien menjadi semakin kecil saat algoritma bergerak mundur ke lapisan yang lebih rendah selama *backpropagation*[cite: 332]. [cite_start]Ini menyebabkan bobot koneksi di lapisan bawah hampir tidak berubah, sehingga pelatihan gagal konvergen ke solusi yang baik[cite: 332].\n",
        "    * [cite_start]**Exploding Gradients**: Kebalikannya, gradien bisa menjadi sangat besar, menyebabkan pembaruan bobot yang sangat besar dan algoritma menjadi divergen[cite: 332]. [cite_start]Ini lebih sering terjadi pada *recurrent neural networks* (RNNs)[cite: 332].\n",
        "    * [cite_start]**Penyebab**: Terutama disebabkan oleh kombinasi fungsi aktivasi *logistic sigmoid* (yang cenderung saturasi pada input besar, menghasilkan gradien mendekati nol) dan teknik inisialisasi bobot standar (distribusi normal dengan rata-rata 0 dan standar deviasi 1)[cite: 332]. [cite_start]Ini menyebabkan varians output lapisan jauh lebih besar dari varians inputnya, menyebabkan saturasi[cite: 332].\n",
        "\n",
        "2.  **Glorot and He Initialization (Inisialisasi Glorot dan He)**:\n",
        "    * [cite_start]**Solusi**: Untuk mengatasi masalah gradien yang tidak stabil, sinyal perlu mengalir dengan baik ke dua arah (*forward* dan *reverse*) tanpa menghilang atau meledak[cite: 333].\n",
        "    * [cite_start]**Glorot Initialization (Xavier Initialization)**: Menginisialisasi bobot koneksi setiap lapisan secara acak dengan rata-rata 0 dan varians $1 / fan_{avg}$ atau distribusi seragam antara $\\pm r$ dengan $r = \\sqrt{3 / fan_{avg}}$, di mana $fan_{avg} = (fan_{in} + fan_{out}) / 2$[cite: 333]. [cite_start]Ini adalah kompromi yang baik dan sangat mempercepat pelatihan[cite: 334].\n",
        "    * [cite_start]**He Initialization**: Mirip dengan Glorot, tetapi khusus untuk fungsi aktivasi ReLU dan variasinya, menggunakan varians $2 / fan_{in}$[cite: 334].\n",
        "    * [cite_start]**LeCun Initialization**: Mirip dengan Glorot, tetapi menggunakan varians $1 / fan_{in}$, direkomendasikan untuk fungsi aktivasi SELU[cite: 334].\n",
        "    * [cite_start]**Implementasi di Keras**: Dapat diatur melalui parameter `kernel_initializer` saat membuat lapisan `Dense`[cite: 334].\n",
        "\n",
        "3.  **Nonsaturating Activation Functions (Fungsi Aktivasi Non-Saturasi)**:\n",
        "    * [cite_start]**Masalah ReLU**: Meskipun cepat dihitung dan tidak saturasi untuk nilai positif, ReLU menderita masalah \"dying ReLUs\", di mana neuron berhenti mengeluarkan apa pun selain 0 karena bobotnya disetel sedemikian rupa sehingga jumlah tertimbang inputnya selalu negatif[cite: 335].\n",
        "    * [cite_start]**Leaky ReLU**: Mengatasi masalah *dying ReLUs* dengan memiliki sedikit kemiringan ($\\alpha$, biasanya 0.01 atau 0.2) untuk $z < 0$[cite: 335]. [cite_start]Variannya termasuk Randomized Leaky ReLU (RReLU) dan Parametric Leaky ReLU (PReLU)[cite: 335].\n",
        "    * [cite_start]**ELU (Exponential Linear Unit)**: Mengungguli semua varian ReLU dalam eksperimen[cite: 336]. Mengambil nilai negatif untuk $z < 0$ (membantu masalah gradien menghilang), memiliki gradien non-nol untuk $z < 0$ (menghindari *dying neurons*), dan mulus di mana-mana jika $\\alpha = 1$ (mempercepat Gradient Descent). [cite_start]Kerugiannya adalah lebih lambat dihitung[cite: 337].\n",
        "    * [cite_start]**SELU (Scaled ELU)**: Versi skala dari ELU[cite: 337]. [cite_start]Jika DNN terdiri dari tumpukan lapisan *dense* dengan SELU dan inisialisasi LeCun normal serta input distandardisasi, jaringan akan *self-normalize* (output setiap lapisan cenderung mempertahankan rata-rata 0 dan standar deviasi 1), yang memecahkan masalah gradien yang menghilang/meledak[cite: 337].\n",
        "    * **Panduan Praktis**: SELU > ELU > Leaky ReLU > ReLU > tanh > logistic. [cite_start]Namun, ReLU mungkin tetap menjadi pilihan terbaik jika kecepatan adalah prioritas utama karena optimasi hardware[cite: 338].\n",
        "\n",
        "4.  **Batch Normalization (Normalisasi Batch)**:\n",
        "    * [cite_start]**Teknik**: Menambahkan operasi di model, sebelum atau sesudah fungsi aktivasi setiap *hidden layer*, yang melakukan *zero-centering* dan normalisasi setiap input, lalu menskalakan dan menggeser hasilnya menggunakan dua vektor parameter baru (gamma $\\gamma$ untuk scaling dan beta $\\beta$ untuk shifting)[cite: 339].\n",
        "    * **Manfaat**: Sangat mengurangi masalah gradien yang menghilang, membuat jaringan kurang sensitif terhadap inisialisasi bobot, memungkinkan *learning rate* yang lebih besar (mempercepat pembelajaran), dan bertindak sebagai regularizer (mengurangi kebutuhan akan teknik regularisasi lain).\n",
        "    * [cite_start]**Cara Kerja saat Inferensi**: Menggunakan rata-rata bergerak (moving average) dari rata-rata input dan standar deviasi yang diestimasi selama pelatihan[cite: 340].\n",
        "    * [cite_start]**Implementasi di Keras**: Cukup tambahkan lapisan `BatchNormalization`[cite: 341]. [cite_start]Ini menambah parameter non-trainable (rata-rata bergerak) dan trainable ($\\gamma, \\beta$)[cite: 342].\n",
        "    * [cite_start]**Posisi BN Layer**: Ada perdebatan apakah lebih baik sebelum atau sesudah fungsi aktivasi[cite: 343]. [cite_start]BN layer dapat digabungkan dengan lapisan sebelumnya setelah pelatihan untuk menghindari penalti *runtime*[cite: 341].\n",
        "    * **Hyperparameter**: `momentum` (mengontrol rata-rata bergerak) dan `axis` (menentukan sumbu normalisasi).\n",
        "\n",
        "5.  **Gradient Clipping (Pemotongan Gradien)**:\n",
        "    * [cite_start]**Teknik**: Memotong gradien selama *backpropagation* agar tidak melebihi *threshold* tertentu[cite: 345].\n",
        "    * [cite_start]**Tujuan**: Mengatasi masalah *exploding gradients*[cite: 345]. [cite_start]Paling sering digunakan di RNNs di mana Batch Normalization sulit diterapkan[cite: 345].\n",
        "    * [cite_start]**Implementasi di Keras**: Atur `clipvalue` atau `clipnorm` saat membuat *optimizer*[cite: 345]. [cite_start]`clipvalue` memotong setiap komponen gradien, `clipnorm` memotong seluruh gradien jika norma L2-nya melebihi *threshold*[cite: 345].\n",
        "\n",
        "6.  **Reusing Pretrained Layers (Menggunakan Ulang Lapisan yang Sudah Dilatih)**:\n",
        "    * **Transfer Learning**: Menggunakan lapisan bawah dari jaringan saraf yang sudah ada (dilatih untuk tugas serupa) dan menambahkan lapisan output baru untuk tugas yang sedang dikerjakan.\n",
        "    * [cite_start]**Manfaat**: Mempercepat pelatihan secara signifikan dan membutuhkan lebih sedikit data pelatihan[cite: 346].\n",
        "    * **Proses**: Ganti lapisan output model asli, bekukan lapisan yang digunakan kembali (buat bobotnya tidak dapat dilatih) untuk *epochs* awal, lalu *unfreeze* beberapa lapisan teratas dan lanjutkan pelatihan dengan *learning rate* yang lebih rendah untuk *fine-tuning*.\n",
        "    * **Implementasi di Keras**: Muat model asli, buat model baru dengan lapisan-lapisan yang digunakan kembali dan lapisan output baru. Kloning model jika tidak ingin model asli ikut terpengaruh.\n",
        "\n",
        "7.  **Unsupervised Pretraining (Pelatihan Awal Tanpa Pengawasan)**:\n",
        "    * [cite_start]**Tujuan**: Untuk tugas kompleks dengan sedikit data berlabel, tetapi banyak data tidak berlabel[cite: 349].\n",
        "    * [cite_start]**Teknik**: Latih model tanpa pengawasan (misalnya, *autoencoder* atau *generative adversarial network*) menggunakan semua data tidak berlabel, lalu gunakan kembali lapisan bawah model tanpa pengawasan tersebut, tambahkan lapisan output untuk tugas sebenarnya, dan *fine-tune* menggunakan pembelajaran terawasi dengan data berlabel[cite: 349].\n",
        "    * [cite_start]Ini adalah teknik yang mengawali kebangkitan Deep Learning pada tahun 2006[cite: 349].\n",
        "\n",
        "8.  **Pretraining on an Auxiliary Task (Pelatihan Awal pada Tugas Tambahan)**:\n",
        "    * [cite_start]**Tujuan**: Ketika data berlabel untuk tugas utama sedikit, tetapi mudah mendapatkan data berlabel untuk tugas tambahan yang terkait[cite: 350].\n",
        "    * [cite_start]**Teknik**: Latih jaringan saraf pertama pada tugas tambahan tersebut, kemudian gunakan kembali lapisan bawah jaringan itu untuk tugas utama dan *fine-tune*[cite: 350].\n",
        "    * [cite_start]**Self-supervised learning**: Secara otomatis menghasilkan label dari data itu sendiri (contohnya adalah memprediksi kata yang hilang dalam kalimat), yang kemudian digunakan untuk melatih model menggunakan teknik pembelajaran terawasi[cite: 351]. [cite_start]Ini diklasifikasikan sebagai bentuk pembelajaran tanpa pengawasan[cite: 351].\n",
        "\n",
        "9.  **Faster Optimizers (Pengoptimal yang Lebih Cepat)**:\n",
        "    * [cite_start]**Tujuan**: Mempercepat pelatihan DNN yang sangat besar[cite: 351].\n",
        "    * [cite_start]**Momentum Optimization**: Menambahkan \"momentum\" ke Gradient Descent, memperhitungkan gradien sebelumnya untuk mempercepat konvergensi di lembah yang dangkal dan menghindari *local optima*[cite: 351].\n",
        "    * [cite_start]**Nesterov Accelerated Gradient (NAG)**: Varian *momentum optimization* yang mengukur gradien sedikit di depan arah momentum, seringkali lebih cepat[cite: 353].\n",
        "    * **AdaGrad**: Menskalakan gradien ke bawah sepanjang dimensi yang paling curam, menghasilkan *adaptive learning rate*. [cite_start]Namun, seringkali berhenti terlalu cepat pada DNN[cite: 355].\n",
        "    * [cite_start]**RMSProp**: Memperbaiki AdaGrad dengan hanya mengakumulasi gradien dari iterasi terbaru menggunakan peluruhan eksponensial[cite: 355]. [cite_start]Umumnya lebih baik dari AdaGrad[cite: 355].\n",
        "    * [cite_start]**Adam (Adaptive Moment Estimation)**: Menggabungkan ide *momentum optimization* dan RMSProp[cite: 356]. [cite_start]Menjaga rata-rata bergerak gradien dan rata-rata bergerak kuadrat gradien[cite: 356]. [cite_start]Seringkali merupakan pilihan *default* yang bagus karena memerlukan sedikit *tuning* `learning_rate`[cite: 357].\n",
        "    * [cite_start]**AdaMax**: Varian Adam yang menggantikan norma L2 dengan norma L-infinity (maksimum), dapat lebih stabil tergantung dataset[cite: 357].\n",
        "    * [cite_start]**Nadam**: Adam + trik Nesterov, seringkali sedikit lebih cepat dari Adam[cite: 358].\n",
        "    * [cite_start]**Catatan**: *Adaptive optimization methods* terkadang dapat menghasilkan solusi yang *generalize* dengan buruk pada beberapa dataset[cite: 358]. [cite_start]Coba NAG biasa jika ini terjadi[cite: 358].\n",
        "\n",
        "10. **Learning Rate Scheduling (Penjadwalan Learning Rate)**:\n",
        "    * **Tujuan**: Mencapai konvergensi yang lebih cepat dan solusi yang lebih baik daripada *learning rate* konstan.\n",
        "    * **Power Scheduling**: $\\eta(t) = \\eta_0 / (1 + t/s)^c$. [cite_start]*Learning rate* turun dengan cepat, lalu melambat[cite: 360].\n",
        "    * **Exponential Scheduling**: $\\eta(t) = \\eta_0 \\cdot 0.1^{t/s}$. [cite_start]*Learning rate* turun secara eksponensial[cite: 360].\n",
        "    * [cite_start]**Piecewise Constant Scheduling**: Menggunakan *learning rate* konstan untuk sejumlah *epochs*, lalu *learning rate* yang lebih kecil untuk *epochs* berikutnya[cite: 361].\n",
        "    * [cite_start]**Performance Scheduling**: Mengurangi *learning rate* ketika *validation error* berhenti turun[cite: 361].\n",
        "    * [cite_start]**1cycle Scheduling**: Meningkatkan *learning rate* dari rendah ke tinggi di paruh pertama pelatihan, lalu menurunkannya lagi di paruh kedua, sambil membalik momentum[cite: 361]. [cite_start]Seringkali sangat mempercepat pelatihan dan menghasilkan kinerja lebih baik[cite: 361].\n",
        "    * **Implementasi di Keras**: Menggunakan parameter `decay` pada *optimizer* (untuk *power scheduling*), atau `LearningRateScheduler` callback dengan fungsi jadwal kustom, atau `keras.optimizers.schedules` (spesifik `tf.keras`).\n",
        "\n",
        "11. **Avoiding Overfitting Through Regularization (Menghindari Overfitting Melalui Regularisasi)**:\n",
        "    * [cite_start]**Tujuan**: Mengurangi *overfitting* pada model DNN yang memiliki banyak parameter[cite: 364].\n",
        "    * [cite_start]**Early Stopping**: Menghentikan pelatihan segera setelah *validation error* mencapai minimum[cite: 364].\n",
        "    * [cite_start]**Batch Normalization**: Selain mengatasi masalah gradien, juga berfungsi sebagai regularizer[cite: 341].\n",
        "    * [cite_start]**$l_1$ and $l_2$ Regularization**: Menambahkan istilah penalti ke fungsi *loss* untuk menjaga bobot model tetap kecil[cite: 364]. [cite_start]$l_1$ menghasilkan model yang *sparse* (banyak bobot nol), $l_2$ (Ridge) mendorong bobot kecil[cite: 364].\n",
        "        * [cite_start]**Implementasi di Keras**: `kernel_regularizer=keras.regularizers.l1(0.01)` atau `keras.regularizers.l2(0.01)`[cite: 364].\n",
        "    * [cite_start]**Dropout**: Pada setiap langkah pelatihan, setiap neuron (kecuali output) memiliki probabilitas $p$ untuk \"dihilangkan\" sementara[cite: 365]. [cite_start]Ini memaksa neuron untuk tidak bergantung pada neuron tetangga, menciptakan jaringan yang lebih *robust* dan *generalize* lebih baik[cite: 366].\n",
        "        * [cite_start]**Dropout Rate ($p$)**: Biasanya antara 10% dan 50%[cite: 365].\n",
        "        * [cite_start]**Implementasi di Keras**: `keras.layers.Dropout(rate=p)`[cite: 367].\n",
        "    * [cite_start]**Monte Carlo (MC) Dropout**: Meningkatkan kinerja model *dropout* yang sudah dilatih tanpa melatih ulang, dan memberikan ukuran ketidakpastian model yang lebih baik[cite: 368]. [cite_start]Dilakukan dengan membuat banyak prediksi pada waktu inferensi dengan *dropout* diaktifkan (`training=True`) dan merata-ratakan hasilnya[cite: 368].\n",
        "    * [cite_start]**Max-Norm Regularization**: Untuk setiap neuron, membatasi norma L2 dari bobot koneksi masukannya ($\\lVert \\mathbf{w} \\rVert_2 \\le r$, di mana $r$ adalah hyperparameter *max-norm*)[cite: 370]. [cite_start]Tidak menambah istilah *loss*, tetapi menskalakan bobot jika perlu setelah setiap langkah pelatihan[cite: 370]. [cite_start]Membantu mengurangi *overfitting* dan menstabilkan gradien[cite: 371].\n",
        "        * [cite_start]**Implementasi di Keras**: `kernel_constraint=keras.constraints.max_norm(r)`[cite: 371].\n",
        "\n",
        "12. **Summary and Practical Guidelines (Ringkasan dan Panduan Praktis)**:\n",
        "    * [cite_start]**Default DNN Configuration**: Tabel 11-3 menyajikan konfigurasi DNN default yang bekerja baik di sebagian besar kasus: He initialization, ELU activation, Batch Norm (jika dalam), Early stopping (+L2 reg.), Momentum optimization (atau RMSProp/Nadam), 1cycle scheduling[cite: 371].\n",
        "    * [cite_start]**Self-Normalizing Net Configuration**: Tabel 11-4 untuk jaringan *self-normalizing*: LeCun initialization, SELU activation, tanpa normalisasi eksplisit (karena *self-normalization*), Alpha Dropout (jika perlu), Momentum optimization (atau RMSProp/Nadam), 1cycle scheduling[cite: 372].\n",
        "    * [cite_start]**Penting**: Normalisasi fitur input, penggunaan ulang model *pretrained*, *unsupervised pretraining*, atau *pretraining* pada tugas tambahan[cite: 372].\n",
        "    * [cite_start]**Kasus Khusus**: Untuk model *sparse* ($l_1$ regularization, TF Model Optimization Toolkit), model *low-latency* (lebih sedikit lapisan, gabungkan BN, fungsi aktivasi cepat, kuantisasi), atau aplikasi *risk-sensitive* (MC Dropout)[cite: 372].\n",
        "\n",
        "## Analisis dan Relevansi untuk Mahasiswa\n",
        "\n",
        "Bab 11 adalah panduan esensial untuk melatih Jaringan Saraf Tiruan yang dalam secara efektif. Dengan cakupan yang luas mulai dari inisialisasi bobot dan fungsi aktivasi hingga berbagai teknik optimasi dan regularisasi, bab ini membekali mahasiswa dengan pengetahuan dan alat yang diperlukan untuk membangun dan menyetel model DNN berkinerja tinggi, serta mengatasi masalah-masalah umum yang mungkin terjadi selama pelatihan. Pemahaman mendalam dari bab ini adalah kunci untuk menjadi praktisi Deep Learning yang kompeten.\n",
        "\n",
        "* **Mengatasi Tantangan Nyata**: Mahasiswa akan dihadapkan pada masalah *vanishing/exploding gradients* dan *overfitting* dalam proyek DNN nyata. [cite_start]Bab ini memberikan *toolkit* komprehensif untuk mendiagnosis dan mengatasi masalah-masalah ini[cite: 331].\n",
        "* [cite_start]**Fondasi Arsitektur Modern**: Teknik-teknik seperti Batch Normalization dan fungsi aktivasi non-saturasi adalah blok bangunan penting dalam arsitektur DNN modern[cite: 339, 335]. Memahaminya bukan hanya untuk perbaikan model, tetapi juga untuk memahami mengapa model-model saat ini didesain seperti itu.\n",
        "* **Optimalisasi Pelatihan**: Berbagai *optimizer* canggih (Momentum, Adam, Nadam, dll.) dan strategi penjadwalan *learning rate* (1cycle scheduling) memberikan mahasiswa alat untuk mempercepat pelatihan dan mencapai konvergensi yang lebih baik. Ini sangat krusial untuk efisiensi komputasi.\n",
        "* **Regularisasi Komprehensif**: Selain *early stopping*, bab ini memperkenalkan *L1/L2 regularization*, *dropout*, *MC Dropout*, dan *max-norm regularization*. Mahasiswa belajar tentang berbagai cara untuk memerangi *overfitting*, yang merupakan salah satu tantangan terbesar dalam DNN.\n",
        "* **Transfer Learning dan Pretraining**: Konsep *transfer learning*, *unsupervised pretraining*, dan *pretraining* pada tugas tambahan adalah teknik tingkat lanjut yang memungkinkan mahasiswa melatih model yang efektif bahkan dengan data berlabel terbatas. Ini adalah keahlian yang sangat dicari di industri.\n",
        "* **Panduan Praktis**: Adanya bagian ringkasan dan panduan praktis sangat membantu mahasiswa dalam menyaring informasi yang banyak dan menerapkan rekomendasi terbaik secara langsung ke proyek mereka.\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab 11 adalah panduan esensial untuk melatih Jaringan Saraf Tiruan yang dalam secara efektif. Dengan cakupan yang luas mulai dari inisialisasi bobot dan fungsi aktivasi hingga berbagai teknik optimasi dan regularisasi, bab ini membekali mahasiswa dengan pengetahuan dan alat yang diperlukan untuk membangun dan menyetel model DNN berkinerja tinggi, serta mengatasi masalah-masalah umum yang mungkin terjadi selama pelatihan. Pemahaman mendalam dari bab ini adalah kunci untuk menjadi praktisi Deep Learning yang kompeten.\n"
      ],
      "metadata": {
        "id": "QAZ2QBwLkq6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REPRODUCE CODE"
      ],
      "metadata": {
        "id": "xKgBt_N_kvb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from functools import partial # Untuk RegularizedDense\n",
        "\n",
        "# Mengatur seed untuk reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- BAGIAN 0: Persiapan Data (dari Chapter 10) ---\n",
        "print(\"--- Persiapan Data Fashion MNIST ---\")\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255.0 # Skala test set juga\n",
        "\n",
        "# Skalakan X_train untuk SELU (mean 0, std 1)\n",
        "# Ini penting untuk self-normalizing networks (SELU)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n",
        "X_valid_scaled = scaler.transform(X_valid.reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n",
        "X_test_scaled = scaler.transform(X_test.reshape(-1, 28 * 28)).reshape(-1, 28, 28)\n",
        "\n",
        "print(\"Data Fashion MNIST siap.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 1: Glorot dan He Initialization ---\n",
        "print(\"\\n--- Bagian 1: Glorot dan He Initialization ---\")\n",
        "\n",
        "# Contoh penggunaan He initialization dengan relu\n",
        "# Default Keras adalah Glorot uniform\n",
        "model_he_init = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"), # He Normal\n",
        "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_he_init.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan He Initialization dibuat.\")\n",
        "model_he_init.summary()\n",
        "\n",
        "# Contoh penggunaan VarianceScaling untuk kustomisasi lebih lanjut (misal He uniform fan_avg)\n",
        "# Ini adalah contoh, tidak digunakan dalam model utama di bawah\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "print(\"\\nInisialisasi kustom VarianceScaling (He uniform fan_avg) dibuat.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 2: Nonsaturating Activation Functions (ELU, Leaky ReLU, SELU) ---\n",
        "print(\"\\n--- Bagian 2: Nonsaturating Activation Functions ---\")\n",
        "\n",
        "# Model dengan ELU activation\n",
        "model_elu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_elu.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan ELU activation dibuat.\")\n",
        "model_elu.summary()\n",
        "\n",
        "# Model dengan Leaky ReLU activation\n",
        "model_leaky_relu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2), # Tambahkan LeakyReLU layer\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_leaky_relu.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Leaky ReLU activation dibuat.\")\n",
        "model_leaky_relu.summary()\n",
        "\n",
        "# Model dengan SELU activation (membutuhkan LeCun Normal Initialization)\n",
        "# Pastikan X_train_scaled dan X_valid_scaled digunakan\n",
        "model_selu = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_selu.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan SELU activation (LeCun Normal) dibuat.\")\n",
        "model_selu.summary()\n",
        "\n",
        "# Latih model ELU sebagai contoh (pelatihan bisa lama)\n",
        "# print(\"\\nMelatih model ELU (demo singkat)...\")\n",
        "# history_elu = model_elu.fit(X_train, y_train, epochs=5,\n",
        "#                             validation_data=(X_valid, y_valid))\n",
        "# print(\"Pelatihan model ELU selesai.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 3: Batch Normalization ---\n",
        "print(\"\\n--- Bagian 3: Batch Normalization ---\")\n",
        "\n",
        "# Model dengan Batch Normalization (BN) setelah setiap layer Dense, dan sebagai layer pertama\n",
        "model_bn_after = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(), # BN sebagai layer pertama\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_bn_after.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Batch Normalization (setelah aktivasi/dense) dibuat.\")\n",
        "model_bn_after.summary()\n",
        "\n",
        "# Model dengan Batch Normalization sebelum aktivasi\n",
        "# Dense layer tanpa bias (use_bias=False)\n",
        "model_bn_before = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), # Tanpa bias\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"), # Aktivasi setelah BN\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_bn_before.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Batch Normalization (sebelum aktivasi) dibuat.\")\n",
        "model_bn_before.summary()\n",
        "\n",
        "# Periksa parameter trainable dari layer BN pertama (hanya gamma dan beta)\n",
        "# gamma dan beta adalah trainable, moving_mean dan moving_variance tidak\n",
        "first_bn_layer = model_bn_after.layers[1]\n",
        "print(f\"\\nParameter layer BN pertama: {[(var.name, var.trainable) for var in first_bn_layer.variables]}\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 4: Gradient Clipping ---\n",
        "print(\"\\n--- Bagian 4: Gradient Clipping ---\")\n",
        "\n",
        "# Optimizer dengan clipvalue\n",
        "optimizer_clip_value = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model_clip_value = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_clip_value.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_clip_value)\n",
        "print(\"Model dengan optimizer clipvalue=1.0 dibuat.\")\n",
        "\n",
        "# Optimizer dengan clipnorm\n",
        "optimizer_clip_norm = keras.optimizers.SGD(clipnorm=1.0)\n",
        "model_clip_norm = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_clip_norm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_clip_norm)\n",
        "print(\"Model dengan optimizer clipnorm=1.0 dibuat.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 5: Reusing Pretrained Layers (Transfer Learning) ---\n",
        "print(\"\\n--- Bagian 5: Reusing Pretrained Layers (Transfer Learning) ---\")\n",
        "\n",
        "# Ini adalah contoh konseptual dari buku.\n",
        "# Model A yang sudah dilatih (disimulasikan di sini)\n",
        "# Model A dilatih pada 8 kelas Fashion MNIST (misal tanpa sandal dan shirt)\n",
        "# Kita akan membuat dummy model_A\n",
        "model_A = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(8, activation=\"softmax\") # 8 kelas\n",
        "])\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "# Latih model_A secara singkat untuk bobot\n",
        "# model_A.fit(X_train[:1000], y_train[:1000] % 8, epochs=1) # Data dan label dummy\n",
        "# model_A.save(\"my_model_A.h5\") # Simpan model_A\n",
        "\n",
        "# Memuat model A dan membuat model B_on_A\n",
        "# model_A_loaded = keras.models.load_model(\"my_model_A.h5\") # Asumsi my_model_A.h5 ada\n",
        "# model_B_on_A = keras.models.Sequential(model_A_loaded.layers[:-1]) # Reused layers\n",
        "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # Output layer baru untuk task B (binary classification)\n",
        "\n",
        "# Atau, kloning model A (jika tidak ingin berbagi layer)\n",
        "# model_A_clone = keras.models.clone_model(model_A_loaded)\n",
        "# model_A_clone.set_weights(model_A_loaded.get_weights())\n",
        "# model_B_on_A_cloned = keras.models.Sequential(model_A_clone.layers[:-1])\n",
        "# model_B_on_A_cloned.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "print(\"Konsep transfer learning dijelaskan.\")\n",
        "print(\"Kode lengkap membutuhkan dataset dan model A yang spesifik.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 6: Faster Optimizers (Momentum, NAG, Adam, RMSprop) ---\n",
        "print(\"\\n--- Bagian 6: Faster Optimizers ---\")\n",
        "\n",
        "# Momentum Optimization\n",
        "optimizer_momentum = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "model_momentum = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_momentum.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_momentum)\n",
        "print(\"Model dengan Momentum Optimizer dibuat.\")\n",
        "\n",
        "# Nesterov Accelerated Gradient (NAG)\n",
        "optimizer_nag = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model_nag = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_nag.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_nag)\n",
        "print(\"Model dengan NAG Optimizer dibuat.\")\n",
        "\n",
        "# RMSprop\n",
        "optimizer_rmsprop = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "model_rmsprop = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_rmsprop.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_rmsprop)\n",
        "print(\"Model dengan RMSprop Optimizer dibuat.\")\n",
        "\n",
        "# Adam\n",
        "optimizer_adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "model_adam = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_adam.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_adam)\n",
        "print(\"Model dengan Adam Optimizer dibuat.\")\n",
        "\n",
        "# Latih model Adam sebagai contoh (pelatihan bisa lama)\n",
        "# print(\"\\nMelatih model Adam (demo singkat)...\")\n",
        "# history_adam = model_adam.fit(X_train, y_train, epochs=5,\n",
        "#                               validation_data=(X_valid, y_valid))\n",
        "# print(\"Pelatihan model Adam selesai.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 7: Learning Rate Scheduling ---\n",
        "print(\"\\n--- Bagian 7: Learning Rate Scheduling ---\")\n",
        "\n",
        "# Power Scheduling (melalui decay di optimizer)\n",
        "optimizer_power_decay = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4) # decay = 1/s\n",
        "model_power_decay = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_power_decay.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_power_decay)\n",
        "print(\"Model dengan Power Scheduling (decay) dibuat.\")\n",
        "\n",
        "# Exponential Scheduling (melalui LearningRateScheduler callback)\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20) # learning_rate = 0.01, turun 10x setiap 20 epoch\n",
        "\n",
        "lr_scheduler_exp = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "model_exp_decay = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_exp_decay.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\") # Optimizer dengan LR awal\n",
        "print(\"Model dengan Exponential Scheduling dibuat.\")\n",
        "# history_exp_decay = model_exp_decay.fit(X_train, y_train, epochs=5,\n",
        "#                                         validation_data=(X_valid, y_valid),\n",
        "#                                         callbacks=[lr_scheduler_exp])\n",
        "\n",
        "\n",
        "# Piecewise Constant Scheduling (melalui LearningRateScheduler callback)\n",
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 15:\n",
        "        return 0.005\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "lr_scheduler_piecewise = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
        "model_piecewise = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_piecewise.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Piecewise Constant Scheduling dibuat.\")\n",
        "# history_piecewise = model_piecewise.fit(X_train, y_train, epochs=20,\n",
        "#                                         validation_data=(X_valid, y_valid),\n",
        "#                                         callbacks=[lr_scheduler_piecewise])\n",
        "\n",
        "# ReduceLROnPlateau callback (Performance Scheduling)\n",
        "lr_scheduler_plateau = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "model_plateau = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_plateau.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "print(\"Model dengan ReduceLROnPlateau (Performance Scheduling) dibuat.\")\n",
        "# history_plateau = model_plateau.fit(X_train, y_train, epochs=20,\n",
        "#                                     validation_data=(X_valid, y_valid),\n",
        "#                                     callbacks=[lr_scheduler_plateau])\n",
        "\n",
        "# ExponentialDecay dari keras.optimizers.schedules (TF Keras-specific)\n",
        "s = 20 * len(X_train) // 32 # Langkah dalam 20 epoch, batch size 32\n",
        "learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=s,\n",
        "    decay_rate=0.1\n",
        ")\n",
        "optimizer_schedule = keras.optimizers.SGD(learning_rate=learning_rate_schedule)\n",
        "model_schedule = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_schedule.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer_schedule)\n",
        "print(\"Model dengan ExponentialDecay (schedules API) dibuat.\")\n",
        "\n",
        "\n",
        "# --- BAGIAN 8: Avoiding Overfitting Through Regularization ---\n",
        "print(\"\\n--- Bagian 8: Regularization ---\")\n",
        "\n",
        "# L1 dan L2 Regularization\n",
        "# Menggunakan functools.partial untuk mengurangi pengulangan kode\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                         activation=\"elu\",\n",
        "                         kernel_initializer=\"he_normal\",\n",
        "                         kernel_regularizer=keras.regularizers.l2(0.01)) # L2 regularization\n",
        "\n",
        "model_l2_reg = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_uniform\") # Output layer biasanya glorot\n",
        "])\n",
        "model_l2_reg.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan L2 Regularization dibuat.\")\n",
        "model_l2_reg.summary()\n",
        "\n",
        "\n",
        "# Dropout Regularization\n",
        "model_dropout = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout setelah Flatten\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout setelah hidden layer 1\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2), # Dropout setelah hidden layer 2\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_dropout.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Dropout dibuat.\")\n",
        "model_dropout.summary()\n",
        "\n",
        "# Monte Carlo (MC) Dropout (konseptual)\n",
        "# Untuk model_dropout yang sudah dilatih\n",
        "# print(\"\\nMonte Carlo Dropout (konseptual):\")\n",
        "# # Ini harusnya dilakukan setelah model_dropout dilatih\n",
        "# # Misalnya, setelah: history_dropout = model_dropout.fit(X_train, y_train, epochs=10)\n",
        "# # Jika Anda ingin menjalankan ini, latih model_dropout terlebih dahulu\n",
        "\n",
        "# # Membuat 100 prediksi dengan dropout aktif\n",
        "# if 'model_dropout' in locals() and hasattr(model_dropout, 'history'): # Check if model_dropout was trained\n",
        "#     y_probas_mc = np.stack([model_dropout(X_test_scaled, training=True) for sample in range(100)])\n",
        "#     y_proba_mc = y_probas_mc.mean(axis=0)\n",
        "#     y_std_mc = y_probas_mc.std(axis=0)\n",
        "#     print(f\"Probabilitas prediksi MC Dropout untuk X_test_scaled[0] (rata-rata): {y_proba_mc[0].round(2)}\")\n",
        "#     print(f\"Standar deviasi MC Dropout untuk X_test_scaled[0]: {y_std_mc[0].round(2)}\")\n",
        "# else:\n",
        "#     print(\"Lewati MC Dropout: model_dropout belum dilatih.\")\n",
        "\n",
        "# Custom MCDropout layer (jika model berisi BatchNormalization atau layer training=True lainnya)\n",
        "class MCDropout(keras.layers.Dropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)\n",
        "print(\"\\nCustom MCDropout layer class dibuat.\")\n",
        "\n",
        "# Max-Norm Regularization\n",
        "model_max_norm = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                       kernel_constraint=keras.constraints.max_norm(1.)), # Max-norm constraint\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                       kernel_constraint=keras.constraints.max_norm(1.)),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model_max_norm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "print(\"Model dengan Max-Norm Regularization dibuat.\")\n",
        "model_max_norm.summary()\n",
        "\n",
        "print(\"\\n--- Semua contoh kode dari Chapter 11 telah direproduksi. ---\")\n",
        "print(\"Catatan: Beberapa bagian memerlukan pelatihan penuh atau file yang disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DfA4k3Xskyxl",
        "outputId": "67893bc5-9de0-4c2f-dce7-c78f079cf497"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Persiapan Data Fashion MNIST ---\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Data Fashion MNIST siap.\n",
            "\n",
            "--- Bagian 1: Glorot dan He Initialization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan He Initialization dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inisialisasi kustom VarianceScaling (He uniform fan_avg) dibuat.\n",
            "\n",
            "--- Bagian 2: Nonsaturating Activation Functions ---\n",
            "Model dengan ELU activation dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan Leaky ReLU activation dibuat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan SELU activation (LeCun Normal) dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bagian 3: Batch Normalization ---\n",
            "Model dengan Batch Normalization (setelah aktivasi/dense) dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan Batch Normalization (sebelum aktivasi) dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m270,946\u001b[0m (1.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,946</span> (1.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,578\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,578</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter layer BN pertama: [('gamma', True), ('beta', True), ('moving_mean', False), ('moving_variance', False)]\n",
            "\n",
            "--- Bagian 4: Gradient Clipping ---\n",
            "Model dengan optimizer clipvalue=1.0 dibuat.\n",
            "Model dengan optimizer clipnorm=1.0 dibuat.\n",
            "\n",
            "--- Bagian 5: Reusing Pretrained Layers (Transfer Learning) ---\n",
            "Konsep transfer learning dijelaskan.\n",
            "Kode lengkap membutuhkan dataset dan model A yang spesifik.\n",
            "\n",
            "--- Bagian 6: Faster Optimizers ---\n",
            "Model dengan Momentum Optimizer dibuat.\n",
            "Model dengan NAG Optimizer dibuat.\n",
            "Model dengan RMSprop Optimizer dibuat.\n",
            "Model dengan Adam Optimizer dibuat.\n",
            "\n",
            "--- Bagian 7: Learning Rate Scheduling ---\n",
            "Model dengan Power Scheduling (decay) dibuat.\n",
            "Model dengan Exponential Scheduling dibuat.\n",
            "Model dengan Piecewise Constant Scheduling dibuat.\n",
            "Model dengan ReduceLROnPlateau (Performance Scheduling) dibuat.\n",
            "Model dengan ExponentialDecay (schedules API) dibuat.\n",
            "\n",
            "--- Bagian 8: Regularization ---\n",
            "Model dengan L2 Regularization dibuat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_18 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dengan Dropout dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_19\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_19\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_19 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom MCDropout layer class dibuat.\n",
            "Model dengan Max-Norm Regularization dibuat.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_20 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Semua contoh kode dari Chapter 11 telah direproduksi. ---\n",
            "Catatan: Beberapa bagian memerlukan pelatihan penuh atau file yang disimpan.\n"
          ]
        }
      ]
    }
  ]
}