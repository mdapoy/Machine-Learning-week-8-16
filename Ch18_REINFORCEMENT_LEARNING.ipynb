{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOi1p/jq9sp87ZPmTK8NEgE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdapoy/Machine-Learning-week-8-16/blob/main/Ch18_REINFORCEMENT_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAPORAN ANALISIS BUKU\n",
        "**Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aurélien Géron)**\n",
        "\n",
        "**BAB 18: REINFORCEMENT LEARNING**\n",
        "\n",
        "## 1. Pendahuluan\n",
        "[cite_start]Bab 18 memperkenalkan *Reinforcement Learning* (RL), salah satu bidang *Machine Learning* yang paling menarik dan tertua, telah ada sejak tahun 1950-an. [cite_start]RL telah menghasilkan banyak aplikasi menarik, terutama dalam game (misalnya, TD-Gammon) dan kontrol mesin. [cite_start]Namun, sebuah revolusi terjadi pada tahun 2013 ketika peneliti dari *DeepMind* mendemonstrasikan sistem yang dapat belajar memainkan hampir semua game Atari dari awal, hanya menggunakan piksel mentah sebagai input dan tanpa pengetahuan sebelumnya tentang aturan game. [cite_start]Ini memuncak pada kemenangan sistem mereka, *AlphaGo*, melawan Lee Sedol dan Ke Jie, juara dunia game Go. [cite_start]*DeepMind* diakuisisi oleh Google pada tahun 2014.\n",
        "\n",
        "## 2. Belajar Mengoptimalkan Hadiah (Learning to Optimize Rewards)\n",
        "[cite_start]Dalam *Reinforcement Learning*, agen perangkat lunak membuat observasi dan mengambil tindakan dalam suatu lingkungan, dan sebagai balasannya menerima hadiah (rewards). [cite_start]Tujuannya adalah belajar bertindak sedemikian rupa untuk memaksimalkan hadiah yang diharapkan dari waktu ke waktu. [cite_start]Hadiah positif dapat dianggap sebagai kesenangan, dan hadiah negatif sebagai rasa sakit.\n",
        "\n",
        "* [cite_start]**Agen**: Program perangkat lunak yang membuat observasi dan mengambil tindakan.\n",
        "* [cite_start]**Lingkungan**: Dunia tempat agen berinteraksi.\n",
        "* [cite_start]**Hadiah (Rewards)**: Umpan balik positif atau negatif yang diterima agen dari lingkungan.\n",
        "\n",
        "Beberapa contoh aplikasi RL meliputi:\n",
        "* [cite_start]Kontrol robot melalui sensor dan motor.\n",
        "* [cite_start]Bermain game seperti Ms. Pac-Man atau Go.\n",
        "* [cite_start]Smart thermostat yang belajar mengantisipasi kebutuhan manusia.\n",
        "* [cite_start]Pengambilan keputusan di pasar saham.\n",
        "\n",
        "[cite_start]Terkadang, tidak ada hadiah positif sama sekali; agen mungkin hanya menerima hadiah negatif, sehingga tujuannya adalah menyelesaikan tugas secepat mungkin. [cite_start]RL juga dapat diterapkan pada mobil otonom, sistem rekomendasi, penempatan iklan web, atau mengontrol fokus perhatian sistem klasifikasi gambar.\n",
        "\n",
        "## 3. Pencarian Kebijakan (Policy Search)\n",
        "[cite_start]*Kebijakan* (policy) adalah algoritma yang digunakan agen perangkat lunak untuk menentukan tindakannya. [cite_start]Kebijakan bisa berupa jaringan saraf yang mengambil observasi sebagai input dan menghasilkan tindakan yang harus diambil.\n",
        "\n",
        "* [cite_start]**Kebijakan Stokastik**: Kebijakan yang melibatkan unsur keacakan, misalnya robot pembersih vakum yang bergerak maju dengan probabilitas tertentu atau berputar secara acak.\n",
        "* **Pendekatan Pembelajaran untuk Kebijakan**:\n",
        "    * [cite_start]**Brute Force**: Mencoba banyak nilai parameter yang berbeda dan memilih kombinasi terbaik.\n",
        "    * [cite_start]**Algoritma Genetik**: Membuat generasi kebijakan secara acak, memilih yang terbaik, dan membiarkan mereka menghasilkan keturunan dengan variasi acak.\n",
        "    * [cite_start]**Teknik Optimasi (Policy Gradients - PG)**: Mengevaluasi gradien hadiah terhadap parameter kebijakan, kemudian menyesuaikan parameter untuk mendapatkan hadiah yang lebih tinggi.\n",
        "\n",
        "## 4. Pengantar OpenAI Gym\n",
        "[cite_start]OpenAI Gym adalah toolkit yang menyediakan berbagai lingkungan simulasi (game Atari, game papan, simulasi fisik 2D dan 3D) untuk melatih agen dan mengembangkan algoritma RL.\n",
        "\n",
        "* [cite_start]**Lingkungan CartPole**: Simulasi 2D di mana sebuah kereta dapat dipercepat ke kiri atau kanan untuk menyeimbangkan tiang yang diletakkan di atasnya.\n",
        "    * [cite_start]**Observasi**: Array NumPy 1D berisi empat float yang mewakili posisi horizontal kereta, kecepatannya, sudut tiang, dan kecepatan sudutnya.\n",
        "    * [cite_start]**Tindakan**: Integer 0 (mempercepat ke kiri) atau 1 (mempercepat ke kanan).\n",
        "    * [cite_start]**Metode `reset()`**: Menginisialisasi lingkungan dan mengembalikan observasi pertama.\n",
        "    * [cite_start]**Metode `render()`**: Menampilkan lingkungan.\n",
        "    * [cite_start]**Metode `step(action)`**: Mengeksekusi tindakan yang diberikan dan mengembalikan observasi baru, hadiah, status selesai (`done`), dan informasi tambahan (`info`).\n",
        "* [cite_start]**Pentingnya Lingkungan Simulasi**: Pelatihan dalam lingkungan nyata lambat dan mahal, sehingga simulasi seringkali diperlukan untuk pelatihan awal (*bootstrap training*).\n",
        "\n",
        "## 5. Kebijakan Jaringan Saraf (Neural Network Policies)\n",
        "[cite_start]Kebijakan dapat diimplementasikan sebagai jaringan saraf yang mengambil observasi sebagai input dan menghasilkan tindakan yang akan dieksekusi. [cite_start]Lebih tepatnya, ia memperkirakan probabilitas untuk setiap tindakan, dan kemudian tindakan dipilih secara acak berdasarkan probabilitas ini.\n",
        "\n",
        "* [cite_start]**Alasan Pemilihan Tindakan Acak**: Memungkinkan agen untuk menemukan keseimbangan antara menjelajahi tindakan baru dan mengeksploitasi tindakan yang sudah diketahui berhasil.\n",
        "* [cite_start]**Struktur Jaringan Saraf Sederhana untuk CartPole**: Jaringan Sequential dengan 4 input (ukuran ruang observasi), beberapa unit tersembunyi, dan satu neuron output dengan fungsi aktivasi sigmoid untuk menghasilkan probabilitas tindakan (misalnya, probabilitas bergerak ke kiri).\n",
        "\n",
        "## 6. Mengevaluasi Tindakan: Masalah Penugasan Kredit (Credit Assignment Problem)\n",
        "* [cite_start]**Masalah Penugasan Kredit**: Dalam RL, agen hanya mendapatkan panduan melalui hadiah, yang biasanya jarang dan tertunda. [cite_start]Sulit bagi agen untuk mengetahui tindakan mana yang berkontribusi pada hadiah positif atau negatif.\n",
        "* [cite_start]**Solusi**: Mengevaluasi tindakan berdasarkan jumlah semua hadiah yang datang setelahnya, biasanya menerapkan faktor diskon ($\\gamma$) pada setiap langkah. [cite_start]Jumlah hadiah yang didiskon ini disebut *return* tindakan.\n",
        "    * [cite_start]**Faktor Diskon ($\\gamma$)**: Menentukan seberapa besar nilai hadiah di masa depan dibandingkan dengan hadiah langsung (biasanya 0.9 hingga 0.99).\n",
        "    * [cite_start]**Action Advantage**: Mengestimasi seberapa baik atau buruk suatu tindakan dibandingkan dengan tindakan lain, rata-rata. [cite_start]Ini dihitung dengan menormalkan semua *action returns* (mengurangi rata-rata dan membagi dengan standar deviasi).\n",
        "\n",
        "## 7. Gradien Kebijakan (Policy Gradients - PG)\n",
        "Algoritma PG mengoptimalkan parameter kebijakan dengan mengikuti gradien menuju hadiah yang lebih tinggi.\n",
        "\n",
        "* **Algoritma REINFORCE (varian umum)**:\n",
        "    1.  [cite_start]Biarkan kebijakan jaringan saraf memainkan game beberapa kali, dan pada setiap langkah, hitung gradien yang akan membuat tindakan yang dipilih lebih mungkin terjadi (tetapi jangan langsung diterapkan).\n",
        "    2.  [cite_start]Setelah beberapa episode, hitung *advantage* setiap tindakan.\n",
        "    3.  Jika *advantage* tindakan positif, terapkan gradien yang dihitung sebelumnya untuk membuat tindakan lebih mungkin dipilih di masa depan. [cite_start]Jika *advantage* negatif, terapkan gradien yang berlawanan untuk membuat tindakan tersebut sedikit kurang mungkin terjadi. [cite_start]Solusinya adalah mengalikan setiap vektor gradien dengan *advantage* tindakan yang sesuai.\n",
        "    4.  [cite_start]Hitung rata-rata semua vektor gradien yang dihasilkan, dan gunakan untuk melakukan langkah *Gradient Descent*.\n",
        "* **Implementasi dengan tf.keras**: Melibatkan fungsi `play_one_step` untuk bermain satu langkah (menghitung loss dan gradien untuk tindakan yang diambil), `play_multiple_episodes` untuk bermain beberapa episode (mengumpulkan hadiah dan gradien), `discount_rewards` untuk menghitung hadiah didiskon, dan `discount_and_normalize_rewards` untuk menghitung *action advantages*.\n",
        "* [cite_start]**Keterbatasan**: Algoritma ini sangat tidak efisien dalam sampel (*sample inefficient*), membutuhkan eksplorasi game yang sangat lama untuk mencapai kemajuan signifikan. [cite_start]Namun, ini adalah dasar untuk algoritma yang lebih kuat seperti algoritma *Actor-Critic*.\n",
        "\n",
        "## 8. Proses Keputusan Markov (Markov Decision Processes - MDP)\n",
        "[cite_start]MDP menyerupai *Markov chains* tetapi dengan twist: pada setiap langkah, agen dapat memilih satu dari beberapa tindakan yang mungkin, dan probabilitas transisi bergantung pada tindakan yang dipilih. [cite_start]Selain itu, beberapa transisi keadaan mengembalikan hadiah (positif atau negatif), dan tujuan agen adalah menemukan kebijakan yang akan memaksimalkan hadiah dari waktu ke waktu.\n",
        "\n",
        "* [cite_start]**Nilai Keadaan Optimal ($V^*(s)$)**: Jumlah semua hadiah di masa depan yang didiskon yang diharapkan agen dapatkan rata-rata setelah mencapai keadaan $s$, dengan asumsi bertindak secara optimal.\n",
        "* **Persamaan Optimalitas Bellman (Bellman Optimality Equation)**: $V^*(s) = \\max_a \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\cdot V^*(s')]$. Persamaan rekursif ini menyatakan bahwa nilai optimal keadaan saat ini sama dengan hadiah yang akan didapatkan rata-rata setelah mengambil satu tindakan optimal, ditambah nilai optimal yang diharapkan dari semua keadaan berikutnya yang dapat dicapai tindakan ini.\n",
        "* [cite_start]**Algoritma Iterasi Nilai (Value Iteration algorithm)**: $V^{k+1}(s) = \\max_a \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\cdot V^k(s')]$. [cite_start]Algoritma ini menjamin konvergensi ke nilai keadaan optimal.\n",
        "* **Nilai Aksi-Keadaan Optimal (Optimal State-Action Values - Q-Values)**: $Q^*(s, a)$ adalah jumlah hadiah di masa depan yang didiskon yang diharapkan agen dapatkan rata-rata setelah mencapai keadaan $s$ dan memilih tindakan $a$, sebelum melihat hasil tindakan ini, dengan asumsi bertindak secara optimal setelah tindakan tersebut.\n",
        "* **Algoritma Iterasi Q-Value (Q-Value Iteration algorithm)**: $Q^{k+1}(s, a) = \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\cdot \\max_{a'} Q^k(s', a')]$.\n",
        "* [cite_start]**Kebijakan Optimal**: Setelah memiliki Q-Values optimal, kebijakan optimal ($\\pi^*(s)$) adalah memilih tindakan dengan Q-Value tertinggi untuk keadaan tersebut: $\\pi^*(s) = \\operatorname{argmax}_a Q^*(s, a)$.\n",
        "\n",
        "## 9. Pembelajaran Perbedaan Temporal (Temporal Difference Learning - TD Learning)\n",
        "TD Learning mirip dengan algoritma Iterasi Nilai, tetapi disesuaikan dengan fakta bahwa agen hanya memiliki pengetahuan parsial tentang MDP (tidak tahu probabilitas transisi $T(s, a, s')$ atau hadiah $R(s, a, s')$ di awal). Agen menggunakan kebijakan eksplorasi untuk menjelajahi MDP, dan seiring kemajuan, algoritma TD Learning memperbarui perkiraan nilai keadaan berdasarkan transisi dan hadiah yang benar-benar diamati.\n",
        "\n",
        "* [cite_start]**Rumus TD Learning**: $V^{k+1}(s) \\leftarrow (1-\\alpha)V^k(s) + \\alpha[r + \\gamma \\cdot V^k(s')]$.\n",
        "* [cite_start]**TD Target**: $r + \\gamma \\cdot V^k(s')$.\n",
        "* **TD Error**: $r + \\gamma \\cdot V^k(s') - V^k(s)$.\n",
        "\n",
        "## 10. Q-Learning\n",
        "Algoritma Q-Learning adalah adaptasi dari algoritma Iterasi Q-Value untuk situasi di mana probabilitas transisi dan hadiah awalnya tidak diketahui. Q-Learning bekerja dengan mengamati agen bermain dan secara bertahap meningkatkan perkiraan Q-Values-nya.\n",
        "\n",
        "* [cite_start]**Rumus Q-Learning**: $Q(s, a) \\leftarrow (1-\\alpha)Q(s, a) + \\alpha[r + \\gamma \\cdot \\max_{a'} Q(s', a')]$.\n",
        "* **Kebijakan Optimal**: Setelah memiliki perkiraan Q-Value yang akurat, kebijakan optimal adalah memilih tindakan dengan Q-Value tertinggi (kebijakan *greedy*).\n",
        "* [cite_start]**Algoritma Off-policy**: Q-Learning adalah algoritma *off-policy* karena kebijakan yang dilatih tidak harus sama dengan kebijakan yang dieksekusi (agen dapat bertindak secara acak).\n",
        "* [cite_start]**Keterbatasan**: Tidak skala dengan baik untuk MDPs besar dengan banyak keadaan dan tindakan.\n",
        "\n",
        "### 10.1. Kebijakan Eksplorasi (Exploration Policies)\n",
        "* **Kebijakan $\\epsilon$-greedy**: Pada setiap langkah, agen bertindak secara acak dengan probabilitas $\\epsilon$, atau secara *greedy* dengan probabilitas $1-\\epsilon$ (memilih tindakan dengan Q-Value tertinggi). Biasanya, $\\epsilon$ dimulai dengan nilai tinggi (misalnya, 1.0) dan secara bertahap dikurangi (misalnya, hingga 0.05).\n",
        "* **Eksplorasi Berbasis Rasa Ingin Tahu**: Mendorong agen untuk mencoba tindakan yang belum banyak dicoba sebelumnya, dengan menambahkan bonus ke perkiraan Q-Value.\n",
        "\n",
        "### 10.2. Approximate Q-Learning dan Deep Q-Learning\n",
        "* **Approximate Q-Learning**: Mengatasi masalah skalabilitas Q-Learning untuk MDP besar dengan mencari fungsi $Q_\\theta(s, a)$ yang mengaproksimasi Q-Value pasangan keadaan-tindakan menggunakan jumlah parameter yang dapat dikelola.\n",
        "* [cite_start]**Deep Q-Network (DQN)**: Jaringan saraf dalam yang digunakan untuk mengestimasi Q-Values.\n",
        "* [cite_start]**Deep Q-Learning**: Menggunakan DQN untuk Approximate Q-Learning. [cite_start]Tujuannya adalah meminimalkan *squared error* antara Q-Value yang diestimasi dan Q-Value target.\n",
        "\n",
        "### 10.3. Mengimplementasikan Deep Q-Learning\n",
        "Implementasi DQN melibatkan:\n",
        "* [cite_start]Membangun jaringan saraf yang mengambil keadaan dan menghasilkan Q-Value untuk setiap tindakan yang mungkin.\n",
        "* Menggunakan kebijakan $\\epsilon$-greedy untuk memilih tindakan.\n",
        "* [cite_start]Menyimpan pengalaman dalam *replay buffer* (atau *replay memory*) untuk mengurangi korelasi antara pengalaman dalam satu batch pelatihan.\n",
        "* Melatih DQN dengan melakukan satu langkah *Gradient Descent* pada batch pengalaman yang diambil dari *replay buffer*.\n",
        "\n",
        "* **Masalah Pelatihan**:\n",
        "    * [cite_start]**Catastrophic Forgetting**: Agen melupakan apa yang telah dipelajari di satu bagian lingkungan ketika mempelajari bagian lain. [cite_start]Ini dapat dikurangi dengan meningkatkan ukuran *replay buffer* dan mengurangi *learning rate*.\n",
        "    * [cite_start]**Ketidakstabilan Pelatihan**: Pelatihan seringkali tidak stabil dan sangat sensitif terhadap pilihan hiperparameter dan *random seed*.\n",
        "\n",
        "### 10.4. Varian Deep Q-Learning\n",
        "* **Fixed Q-Value Targets**: Menggunakan dua DQN: satu *online model* (belajar dan menggerakkan agen) dan satu *target model* (hanya untuk mendefinisikan target). *Target model* diperbarui lebih jarang, menstabilkan target Q-Value. Ini adalah kontribusi utama *DeepMind* pada tahun 2013.\n",
        "* [cite_start]**Double DQN (DDQN)**: Mengatasi *overestimation* Q-Value oleh *target network* dengan menggunakan *online model* untuk memilih tindakan terbaik untuk keadaan berikutnya dan *target model* hanya untuk mengestimasi Q-Value untuk tindakan terbaik tersebut.\n",
        "* [cite_start]**Prioritized Experience Replay (PER)**: Mengambil sampel pengalaman dari *replay buffer* berdasarkan \"kepentingan\" mereka, yang diukur dengan besarnya *TD error*. [cite_start]Pengalaman penting lebih sering diambil sampelnya, dan bobot pelatihan disesuaikan untuk mengkompensasi bias ini.\n",
        "* [cite_start]**Dueling DQN (DDQN)**: Memisahkan estimasi Q-Value menjadi nilai keadaan ($V(s)$) dan keuntungan setiap tindakan ($A(s, a)$). [cite_start]Model mengestimasi keduanya secara terpisah dan menggabungkannya.\n",
        "\n",
        "## 11. Pustaka TF-Agents\n",
        "[cite_start]TF-Agents adalah pustaka *Reinforcement Learning* berbasis TensorFlow, dikembangkan oleh Google dan dirilis pada tahun 2018.\n",
        "\n",
        "* **Fitur**: Menyediakan berbagai lingkungan (*wrapper* untuk OpenAI Gym, PyBullet, DM Control, Unity ML-Agents), mengimplementasikan banyak algoritma RL (REINFORCE, DQN, DDQN), dan komponen RL (buffer replay, metrik).\n",
        "* [cite_start]**Lingkungan TF-Agents**: Mirip dengan OpenAI Gym, tetapi metode `reset()` dan `step()` mengembalikan objek `TimeStep`.\n",
        "* **Environment Wrappers dan Atari Preprocessing**: TF-Agents menyediakan berbagai *wrapper* lingkungan (misalnya, `ActionRepeat`, `TimeLimit`). [cite_start]`AtariPreprocessing` adalah *wrapper* khusus untuk game Atari, yang menerapkan *grayscale* dan *downsampling*, *max pooling* dua *frame* terakhir, *frame skipping*, dan *end on life lost*.\n",
        "* **TFPyEnvironment**: Memungkinkan lingkungan dapat digunakan dalam grafik TensorFlow.\n",
        "\n",
        "### 11.1. Arsitektur Pelatihan\n",
        "Program pelatihan TF-Agents biasanya terbagi menjadi dua bagian yang berjalan paralel:\n",
        "* [cite_start]**Driver**: Mengeksplorasi lingkungan menggunakan *collect policy* untuk memilih tindakan, mengumpulkan *trajectories* (pengalaman), dan mengirimkannya ke *observer* yang menyimpannya di *replay buffer*.\n",
        "* [cite_start]**Agent**: Mengambil *batch trajectories* dari *replay buffer* dan melatih beberapa jaringan, yang kemudian digunakan oleh *collect policy*.\n",
        "\n",
        "### 11.2. Membuat Deep Q-Network\n",
        "[cite_start]*TF-Agents* menyediakan *QNetwork* (`tf_agents.networks.q_network.QNetwork`) yang mengambil observasi sebagai input dan menghasilkan satu Q-Value per tindakan. [cite_start]Jaringan ini dapat mencakup *preprocessing layers*, *convolutional layers*, dan *dense layers*.\n",
        "\n",
        "### 11.3. Membuat Agen DQN\n",
        "Pustaka *TF-Agents* mengimplementasikan berbagai jenis agen, termasuk `DqnAgent` (`tf_agents.agents.dqn.dqn_agent.DqnAgent`). [cite_start]Agen DQN dikonfigurasi dengan spesifikasi waktu-langkah dan tindakan, *QNetwork*, *optimizer*, periode pembaruan target, fungsi *loss*, faktor diskon, penghitung langkah pelatihan, dan fungsi $\\epsilon$-greedy.\n",
        "\n",
        "### 11.4. Membuat Replay Buffer dan Observer yang Sesuai\n",
        "[cite_start]*TF-Agents* menyediakan berbagai implementasi *replay buffer*, seperti `TFUniformReplayBuffer` (`tf_agents.replay_buffers.tf_uniform_replay_buffer`) yang menyediakan implementasi *replay buffer* berkinerja tinggi dengan sampling seragam. [cite_start]*Observer* adalah fungsi yang mengambil argumen *trajectory*.\n",
        "\n",
        "### 11.5. Membuat Metrik Pelatihan\n",
        "*TF-Agents* mengimplementasikan beberapa metrik RL dalam paket `tf_agents.metrics`, termasuk `NumberOfEpisodes`, `EnvironmentSteps`, `AverageReturnMetric`, dan `AverageEpisodeLengthMetric`.\n",
        "\n",
        "### 11.6. Membuat Collect Driver\n",
        "[cite_start]*Driver* adalah objek yang menjelajahi lingkungan menggunakan kebijakan tertentu, mengumpulkan pengalaman, dan menyiarkannya ke *observer*. [cite_start]Ada dua kelas *driver* utama: `DynamicStepDriver` (mengumpulkan pengalaman untuk sejumlah langkah tertentu) dan `DynamicEpisodeDriver` (mengumpulkan pengalaman untuk sejumlah episode tertentu).\n",
        "\n",
        "### 11.7. Membuat Dataset\n",
        "Untuk mengambil sampel *batch trajectories* dari *replay buffer*, metode `get_next()` dapat dipanggil, atau `as_dataset()` untuk memanfaatkan Data API.\n",
        "\n",
        "### 11.8. Membuat Loop Pelatihan\n",
        "[cite_start]Loop pelatihan melibatkan pemanggilan *driver* untuk menjalankan *collect policy* dan mengumpulkan pengalaman, lalu mengambil satu *batch trajectories* dari dataset, dan meneruskannya ke metode `train()` agen.\n",
        "\n",
        "## 12. Tinjauan Beberapa Algoritma RL Populer\n",
        "* **Algoritma Actor-Critic**: Menggabungkan Gradien Kebijakan dengan Jaringan Q-Deep.\n",
        "* [cite_start]**Asynchronous Advantage Actor-Critic (A3C)**: Beberapa agen belajar secara paralel di lingkungan yang berbeda, memperbarui jaringan master secara asinkron.\n",
        "* [cite_start]**Advantage Actor-Critic (A2C)**: Varian A3C yang menghilangkan asinkronisitas, dengan pembaruan model sinkron.\n",
        "* **Soft Actor-Critic (SAC)**: Belajar memaksimalkan hadiah dan *entropy* tindakan untuk mendorong eksplorasi.\n",
        "* [cite_start]**Proximal Policy Optimization (PPO)**: Berbasis A2C, membatasi fungsi *loss* untuk menghindari pembaruan bobot yang terlalu besar dan ketidakstabilan.\n",
        "* [cite_start]**Eksplorasi Berbasis Rasa Ingin Tahu**: Mengabaikan hadiah lingkungan dan membuat agen penasaran untuk menjelajahi lingkungan, mencari hasil yang tidak sesuai dengan prediksinya.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Uug4PHVw6_2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REPRODUCE CODE"
      ],
      "metadata": {
        "id": "PKxG2k3u7FlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to OpenAI Gym"
      ],
      "metadata": {
        "id": "jRh5AxM172lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi (dijalankan di terminal)\n",
        "# python3 -m pip install -U gym\n",
        "\n",
        "import gym\n",
        "\n",
        "# Membuat environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "print(obs)\n",
        "\n",
        "# Merender environment\n",
        "# env.render() # Kode ini akan membuka jendela untuk menampilkan environment\n",
        "\n",
        "# Menanyakan action space yang tersedia\n",
        "print(env.action_space)\n",
        "\n",
        "# Melakukan sebuah action\n",
        "action = 1 # accelerate right\n",
        "obs, reward, done, info = env.step(action)\n",
        "print(obs)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)\n",
        "\n",
        "# Menjalankan policy sederhana\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n",
        "\n",
        "# Menganalisis hasil\n",
        "import numpy as np\n",
        "print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals))\n",
        "\n",
        "# Menutup environment setelah selesai\n",
        "env.close()"
      ],
      "metadata": {
        "id": "YntBmQfw7IP8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "01341f8a",
        "outputId": "03dc0087-0f6b-4657-bbda-ce2d608f9126"
      },
      "source": [
        "%pip install -U gym"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827730 sha256=98be7a4b11566f239b9ac23e0dcf2c69b10911cdbb262eabf43d48b016743f6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "f2467b8a11cf4c35ae823951f0b9c220"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Policies"
      ],
      "metadata": {
        "id": "FKVQplCt89zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "n_inputs = 4 # == env.observation_space.shape[0]\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ToR7qJR8_L5",
        "outputId": "591389c0-6a55-44cf-87b8-2d456c0128e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradients"
      ],
      "metadata": {
        "id": "ZKPAVXq89AIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads"
      ],
      "metadata": {
        "id": "P_sFPE5j9B2W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fungsi untuk menghitung discounted rewards"
      ],
      "metadata": {
        "id": "ReAdpINh9HY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "metadata": {
        "id": "qsgasFYs9MoT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING LOOP"
      ],
      "metadata": {
        "id": "A_3wX_cD9O2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "RGlhKCtU9QOA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Processes"
      ],
      "metadata": {
        "id": "lnKODNt89dvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi MDP\n",
        "transition_probabilities = [ # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]]\n",
        "rewards = [ # shape=[s, a, s']\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
        "\n",
        "# Inisialisasi Q-Values\n",
        "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state, actions] = 0.0 # for all possible actions\n",
        "\n",
        "# Q-Value Iteration\n",
        "gamma = 0.90 # the discount factor\n",
        "for iteration in range(50):\n",
        "    Q_prev = Q_values.copy()\n",
        "    for s in range(3):\n",
        "        for a in possible_actions[s]:\n",
        "            Q_values[s, a] = np.sum([\n",
        "                transition_probabilities[s][a][sp]\n",
        "                * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
        "                for sp in range(3)])\n",
        "\n",
        "# Menemukan kebijakan optimal\n",
        "print(np.argmax(Q_values, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_fGS_RT9fuD",
        "outputId": "f9fd54e5-ea72-45be-b320-31202775b20c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "y_cqHmeR9hpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk satu langkah\n",
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward\n",
        "\n",
        "# Kebijakan eksplorasi\n",
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])\n",
        "\n",
        "# Algoritma Q-Learning\n",
        "alpha0 = 0.05 # initial learning rate\n",
        "decay = 0.005 # learning rate decay\n",
        "gamma = 0.90 # discount factor\n",
        "state = 0 # initial state\n",
        "\n",
        "for iteration in range(10000):\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = np.max(Q_values[next_state])\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= 1 - alpha\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state"
      ],
      "metadata": {
        "id": "vpuUTkeJ9kCu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Deep Q-Learning"
      ],
      "metadata": {
        "id": "c6Km0B3B9lXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "input_shape = [4] # == env.observation_space.shape\n",
        "n_outputs = 2 # == env.action_space.n\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbpi5Mp_9nEp",
        "outputId": "1fe1ba62-1c6c-4631-9224-92609af08deb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  if \"render_modes\" in env_creator.metadata:\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kebijakan ε-greedy, replay buffer, dan fungsi-fungsi helper:\n",
        "\n"
      ],
      "metadata": {
        "id": "mVFgkPt19qly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(2)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "from collections import deque\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "DjXa9c4h9rl7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fungsi untuk satu langkah training:"
      ],
      "metadata": {
        "id": "ottgS1Jm9uBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - dones) * discount_factor * max_next_Q_values)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "CvAf_TB_9u9m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop Utama:"
      ],
      "metadata": {
        "id": "-WWDykIf92DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)"
      ],
      "metadata": {
        "id": "ec7T7d0F93Se"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double DQN:"
      ],
      "metadata": {
        "id": "xw3eG05d98WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Di dalam training_step():\n",
        "def training_step_double_dqn(batch_size): # Nama fungsi diubah untuk kejelasan\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - dones) * discount_factor * next_best_Q_values)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "pl7UD7oE9_H7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dueling DQN:"
      ],
      "metadata": {
        "id": "ok3H95Rq-BsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arsitektur model Dueling DQN\n",
        "K = keras.backend\n",
        "input_states = keras.layers.Input(shape=[4])\n",
        "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "# Use a Lambda layer to perform the operation on KerasTensors\n",
        "advantages = keras.layers.Lambda(lambda x: x - K.max(x, axis=1, keepdims=True))(raw_advantages)\n",
        "Q_values = state_values + advantages\n",
        "model = keras.Model(inputs=[input_states], outputs=[Q_values])"
      ],
      "metadata": {
        "id": "O6n3gZak-DBE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The TF-Agents Library"
      ],
      "metadata": {
        "id": "8Z1i0UmS-Lle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi (di terminal)\n",
        "# python3 -m pip install -U tf-agents\n",
        "# python3 -m pip install -U 'gym[atari]'\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "\n",
        "# Membuat environment TF-Agents\n",
        "env = suite_gym.load(\"Breakout-v4\")\n",
        "\n",
        "# Menggunakan environment\n",
        "env.reset()\n",
        "env.step(1) # Fire\n",
        "\n",
        "# Preprocessing dengan wrapper\n",
        "from tf_agents.environments import suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "\n",
        "max_episode_steps = 27000\n",
        "environment_name = \"BreakoutNoFrameskip-v4\"\n",
        "env = suite_atari.load(\n",
        "    environment_name,\n",
        "    max_episode_steps=max_episode_steps,\n",
        "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
        "\n",
        "# Membuat TFPyEnvironment\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "tf_env = TFPyEnvironment(env)"
      ],
      "metadata": {
        "id": "R1cY9NJG-Mm6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat Komponen-komponen TF-Agents:"
      ],
      "metadata": {
        "id": "IEhbFswn_rv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat DQN\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "preprocessing_layer = keras.layers.Lambda(\n",
        "    lambda obs: tf.cast(obs, np.float32) / 255.)\n",
        "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "fc_layer_params=[512]\n",
        "q_net = QNetwork(\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing_layer,\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "# Membuat DQN Agent\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "train_step = tf.Variable(0)\n",
        "update_period = 4\n",
        "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
        "                                     epsilon=0.00001, centered=True)\n",
        "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0, # initial ε\n",
        "    decay_steps=250000 // update_period,\n",
        "    end_learning_rate=0.01) # final ε\n",
        "agent = DqnAgent(tf_env.time_step_spec(),\n",
        "                 tf_env.action_spec(),\n",
        "                 q_network=q_net,\n",
        "                 optimizer=optimizer,\n",
        "                 target_update_period=2000,\n",
        "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
        "                 gamma=0.99, # discount factor\n",
        "                 train_step_counter=train_step,\n",
        "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
        "agent.initialize()\n",
        "\n",
        "# Membuat Replay Buffer dan Observer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=1000000)\n",
        "replay_buffer_observer = replay_buffer.add_batch\n",
        "\n",
        "# Membuat Training Metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(),\n",
        "]\n",
        "\n",
        "# Membuat Collect Driver\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer_observer] + train_metrics,\n",
        "    num_steps=update_period)\n",
        "\n",
        "# Mengisi Replay Buffer awal\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
        "                                        tf_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=[replay_buffer.add_batch], # ShowProgress dihilangkan untuk keringkasan\n",
        "    num_steps=20000)\n",
        "# final_time_step, final_policy_state = init_driver.run() # Dijalankan sebelum training loop\n",
        "\n",
        "# Membuat Dataset\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3).prefetch(3)"
      ],
      "metadata": {
        "id": "ch4hPQmI_teH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop dengan TF-Agents:"
      ],
      "metadata": {
        "id": "KDNQtvKX_vob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.utils.common import function\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)\n",
        "\n",
        "def train_agent(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
        "    iterator = iter(dataset)\n",
        "    for iteration in range(n_iterations):\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "        train_loss = agent.train(trajectories)\n",
        "        print(\"\\r{} loss:{:.5f}\".format(\n",
        "            iteration, train_loss.loss.numpy()), end=\"\")\n",
        "        # if iteration % 1000 == 0:\n",
        "        #    log_metrics(train_metrics)\n",
        "\n",
        "# train_agent(10000000)"
      ],
      "metadata": {
        "id": "2JG_HgJm_va5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}